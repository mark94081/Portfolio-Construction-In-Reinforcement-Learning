% Encoding: UTF-8

@Article{Aked-Ko-2017,
  author               = {Aked, Michael and Ko, Amie},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Time Diversification Redux},
  url                  = {https://ssrn.com/abstract=3040967},
  abstract             = {Conventional risk measures may not accurately describe the volatility investors actually experience, especially for portfolios servicing their retirement spending needs. Return volatility rises as its calculated holding period nears 1 year and falls as it lengthens to 10 years. Lower volatility at longer holding periods implies that longer-term mean reversion exists. A portfolio achieves the greatest extra-return benefit by rebalancing over the holding period of highest volatility. Time diversification is helpful, up until long-term uncertainty about the value of reinvested cash flows from dividends leads to rising volatility.},
  citeulike-article-id = {14438504},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3040967},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-09-26 22:40:12},
  timestamp            = {2020-02-25 20:57},
}

@Article{Allen-et-al-2016,
  author               = {Allen, David and McAleer, Michael and Powell, Robert and Singh, Abhay},
  date                 = {2016-06},
  journaltitle         = {Journal of Risk and Financial Management},
  title                = {Down-Side Risk Metrics as Portfolio Diversification Strategies across the Global Financial Crisis},
  doi                  = {10.3390/jrfm9020006},
  number               = {2},
  pages                = {6+},
  volume               = {9},
  abstract             = {This paper features an analysis of the effectiveness of a range of portfolio diversification strategies, with a focus on down-side risk metrics, as a portfolio diversification strategy in a European market context. We apply these measures to a set of daily arithmetically-compounded returns, in U.S. dollar terms, on a set of ten market indices representing the major European markets for a nine-year period from the beginning of 2005 to the end of 2013. The sample period, which incorporates the periods of both the Global Financial Crisis (GFC) and the subsequent European Debt Crisis (EDC), is a challenging one for the application of portfolio investment strategies. The analysis is undertaken via the examination of multiple investment strategies and a variety of hold-out periods and backtests. We commence by using four two-year estimation periods and a subsequent one-year investment hold out period, to analyse a naive 1/N diversification strategy and to contrast its effectiveness with Markowitz mean variance analysis with positive weights. Markowitz optimisation is then compared to various down-side investment optimisation strategies. We begin by comparing Markowitz with CVaR, and then proceed to evaluate the relative effectiveness of Markowitz with various draw-down strategies, utilising a series of backtests. Our results suggest that none of the more sophisticated optimisation strategies appear to dominate naive diversification.},
  citeulike-article-id = {14412283},
  citeulike-linkout-0  = {http://dx.doi.org/10.3390/jrfm9020006},
  citeulike-linkout-1  = {http://www.mdpi.com/1911-8074/9/2/6},
  citeulike-linkout-2  = {http://www.mdpi.com/1911-8074/9/2/6/pdf},
  day                  = {21},
  groups               = {Diversification_Measure, Diversified_Invest, Effective_Dim_Diversif, Invest_Diversif},
  posted-at            = {2017-08-10 23:14:01},
  timestamp            = {2020-02-25 20:57},
}

@Article{Baitinger-et-al-2015,
  author               = {Baitinger, Eduard and Kutsarov, Iliya and Maier, Thomas and Storr, Marcus and Wan, Tao},
  date                 = {2015-03},
  journaltitle         = {Credit and Capital Markets - Kredit und Kapital},
  title                = {A Wholistic Approach to Diversification Management: The Diversification Delta Strategy Applied to Non-Normal Return Distributions},
  doi                  = {10.3790/ccm.48.1.89},
  issn                 = {2199-1227},
  number               = {1},
  pages                = {89--119},
  volume               = {48},
  abstract             = {In this paper we study a higher moment diversification measure, the so-called diversification delta (Vermorken et al. (2012)), in a dynamic portfolio optimization context. Particularly, we set up an investment strategy that dynamically maximizes the diversification delta for a given set of assets. Thus, we label the resulting optimized portfolio structure as Maximum Diversification Delta Portfolio (MDDP). Our out-of-sample empirical study reveals that considering crisis-periods, the MDDP is superior to popular investment strategies, such as Minimum-Variance-Portfolio, Risk-Parity-Portfolio and Equally-Weighted-Portfolio, in terms of risk adjusted returns, risk moments and certainty equivalents. However, in line with other diversification measures the MDDP is no longer superior in upward trending markets.},
  citeulike-article-id = {14148591},
  citeulike-linkout-0  = {http://dx.doi.org/10.3790/ccm.48.1.89},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-09-28 20:02:36},
  timestamp            = {2020-02-25 20:57},
}

@Article{Bianchi-et-al-2016a,
  author               = {Bianchi, Robert and Drew, Michael and Walk, Adam},
  date                 = {2016},
  journaltitle         = {Financial Planning Research Journal},
  title                = {The Time Diversification Puzzle: A Survey},
  url                  = {https://www.griffith.edu.au/__data/assets/pdf_file/0025/205729/time-diversification-puzzle-bianchi-drew-walk.pdf},
  abstract             = {Since Samuelson's (1969) theoretical proof that risk and time are unrelated, a half century of debate and controversy has ensued, leaving time diversification as one of the most enduring puzzles of modern finance. The most conspicuous aspect of the debate is the questionable assumptions that underlie much of the analysis. Thus we are left with an unsatisfying debate conducted in a paradigm where terminal wealth is usually a function only of returns, and where time-weighted measures are assumed to adequately evaluate performance. This paper reviews the major streams in the time diversification literature and argues that more realistic analysis using defensible assumptions is likely to lead to better prescriptions for improved retirement investing},
  citeulike-article-id = {14514122},
  groups               = {Diversification_Measure, Invest_Diversif},
  posted-at            = {2018-01-09 16:42:45},
  timestamp            = {2020-02-25 20:57},
}

@TechReport{Carli-et-al-2014,
  author      = {Tiffanie Carli and Romain Deguest and Lionel Martellini},
  date        = {2014},
  institution = {EDHEC-Risk Institute},
  title       = {Improved Risk Reporting with Factor-Based Diversification Measures},
  url         = {https://risk.edhec.edu/publications/improved-risk-reporting-factor-based-diversification-measures},
  abstract    = {This paper analyses various measures of portfolio diversification, and explores the implication in terms of advanced risk reporting techniques. We use the minimal linear torsion approach (Meucci et al. (2013)) to turn correlated constituents into uncorrelated factors, and focus on the effective number of (uncorrelated) bets (ENB), the entropy of the distribution of risk factor contribution to portfolio risk, as a meaningful measure of the degree of diversification in a portfolio.

In an attempt to assess whether a relationship exists between the degree of diversification of a portfolio and its performance in various market conditions, we empirically analyse the diversification of various equity indices and pension fund policy portfolios. We find strong evidence of a significantly positive time-series and cross-sectional relationship between the ENB risk diversification measure and performance in bear markets.

This relationship, however, is highly linear, and the top performing portfolios in severe bear markets are typically portfolios concentrated in safe assets, as opposed to well-diversified portfolios. We also find statistical and economic evidence that this diversification measure has predictive power for equity market returns, a predictive power which becomes substantial over long holding period.

Overall our results suggest that the ENB measure could be a useful addition to the list of risk indicators reported for equity and policy portfolios.},
  groups      = {Diversification_Measure, Effective_Dim_Diversif, Invest_Diversif},
  owner       = {Anne},
  timestamp   = {2020-02-25 20:57},
}

@Article{Carmichael-et-al-2015,
  author               = {Carmichael, Benoit and Koumou, Gilles and Moran, Kevin},
  date                 = {2015-05},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Unifying Portfolio Diversification Measures Using Rao's Quadratic Entropy},
  url                  = {https://ssrn.com/abstract=2610814},
  abstract             = {This paper extends the use of Rao (1982b)'s Quadratic Entropy (RQE) to modern portfolio theory. It argues that the RQE of a portfolio is a valid, flexible and unifying approach to measuring portfolio diversification. The paper demonstrates that portfolio's RQE can encompass most existing measures, such as the portfolio variance, the diversification ratio, the normalized portfolio variance, the diversification return or excess growth rates, the Gini-Simpson indices, the return gaps, Markowitz's utility function and Bouchaud's general free utility. The paper also shows that assets selected under RQE can protect portfolios from mass destruction (systemic risk) and an empirical illustration suggests that this protection is substantial.},
  citeulike-article-id = {13997244},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2610814},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2610814code361712.pdf?abstractid=2610814 and mirid=1},
  day                  = {26},
  groups               = {Diversification_Measure, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-04-04 22:58:50},
  timestamp            = {2020-02-25 20:57},
}

@Article{Cesarone-Colucci-2018,
  author         = {Cesarone, Francesco and Colucci, Stefano},
  date           = {2018-02},
  journaltitle   = {Journal of the Operational Research Society},
  title          = {Minimum risk versus capital and risk diversification strategies for portfolio construction},
  doi            = {10.1057/s41274-017-0216-5},
  issn           = {0160-5682},
  number         = {2},
  pages          = {183--200},
  volume         = {69},
  abstract       = {In this paper, we propose an extensive empirical analysis on three categories of portfolio selection models with very different objectives: minimization of risk, maximization of capital diversification, and uniform distribution of risk allocation. The latter approach, also called Risk Parity or Equal Risk Contribution (ERC), is a recent strategy for asset allocation that aims at equally sharing the risk among all the assets of the selected portfolio. The risk measure commonly used to select ERC portfolios is volatility. We propose here new developments of the ERC approach using Conditional Value-at-Risk (CVaR) as a risk measure. Furthermore, under appropriate conditions, we also provide an approach to find a CVaR ERC portfolio as a solution of a convex optimization problem. We investigate how these classes of portfolio models (Minimum-Risk, Capital-Diversification, and Risk-Diversification) work on seven investment universes, each with different sources of risk, including equities, bonds, and mixed assets. Then, we highlight some strengths and weaknesses of all portfolio strategies in terms of various performance measures.},
  f1000-projects = {QuantInvest},
  groups         = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 20:57},
}

@Article{Cesarone-et-al-2018,
  author         = {Cesarone, Francesco and Moretti, Jacopo and Tardella, Fabio},
  date           = {2018-02-07},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Why Small Portfolios Are Preferable and How to Choose Them},
  url            = {https://ssrn.com/abstract=3154353},
  abstract       = {One of the fundamental principles in portfolio selection models is minimization of risk through diversification of the investment. However, this principle does not necessarily translate into a request for investing in all the assets of the investment universe. Indeed, following a line of research started by Evans and Archer almost 50 years ago, we provide here further evidence that small portfolios are sufficient to achieve almost optimal in-sample risk reduction with respect to variance and to some other popular risk measures, and very good out-of-sample performances. While leading to similar results, our approach is significantly different from the classical one pioneered by Evans and Archer. Indeed, we describe models for choosing the portfolio of a prescribed size with the smallest possible risk, as opposed to the random portfolio choice investigated in most of the previous works. We find that the smallest risk portfolios generally require no more than 15 assets. Furthermore, it is almost always possible to find portfolios that are just 1\% more risky than the smallest risk portfolios and contain no more than 10 assets. The preference for small optimal portfolios is also justified by recent theoretical results on the estimation errors for the parameters required by portfolio selection models. Our empirical analysis is based on some new and on some publicly available benchmark data sets often used in the literature.},
  day            = {7},
  f1000-projects = {QuantInvest},
  groups         = {Diversification_Measure},
  timestamp      = {2020-02-25 20:57},
}

@Article{Cesarone-Moretti-2016,
  author         = {Cesarone, Francesco and Moretti, Jacopo},
  date           = {2016},
  journaltitle   = {Economics Bulletin},
  title          = {Optimally chosen small portfolios are better than large ones},
  number         = {4},
  translator     = {Tardella, Fabio},
  volume         = {36},
  abstract       = {One of the fundamental principles in portfolio selection models is minimization of risk through diversification of the investment. However, this principle does not necessarily translate into a request for investing in all the assets of the investment universe. Indeed, following a line of research started by Evans and Archer almost fifty years ago, we provide here further evidence that small portfolios are sufficient to achieve almost optimal in-sample risk reduction with respect to variance and to some other popular risk measures, and very good out-of-sample performances. While leading to similar results, our approach is significantly different from the classical one pioneered by Evans and Archer. Indeed, we describe models for choosing the portfolio of a prescribed size with the smallest possible risk, as opposed to the random portfolio choice investigated in most of the previous works. We find that the smallest risk portfolios generally require no more than 15 assets. Furthermore, it is almost always possible to find portfolios that are just 1\% more risky than the smallest risk portfolios and contain no more than 10 assets. Furthermore, the optimal small portfolios generally show a better performance than the optimal large ones. Our empirical analysis is based on some new and on some publicly available benchmark data sets often used in the literature.},
  f1000-projects = {QuantInvest},
  groups         = {Diversification_Measure},
  timestamp      = {2020-02-25 20:57},
}

@Article{Chollete-et-al-2012,
  author       = {Chollete, L. and de la Pena, V. and Lu, C.},
  date         = {2012},
  journaltitle = {Journal of Banking and Finance},
  title        = {International diversification: An extreme value approach},
  number       = {3},
  pages        = {871--885},
  url          = {https://www.sciencedirect.com/science/article/pii/S0378426611002767},
  volume       = {36},
  abstract     = {International diversification has costs and benefits, depending on the degree of asset dependence. We study international diversification with two dependence measures: correlations and extreme dependence. We discover that dependence has typically increased over time, and document mixed evidence on heavy tails in individual countries.

Moreover, we uncover three additional findings related to dependence. First, the timing of downside risk differs depending on the region. Surprisingly, recent Latin American returns exhibit little downside risk. Second, Latin America exhibits a great deal of correlation complexity. Third, according to the empirical results, correlation does not vary with returns, but extreme dependence does vary monotonically with regional returns.

Our results are consistent with a tradeoff between international diversification and systemic risk. They also suggest international limits to diversification, and that international investors demand some compensation for joint downside risk during extreme events.},
  groups       = {Diversification_Measure, Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 20:57},
}

@Article{Ayres-Nalebuff-2013,
  author               = {Ayres, Ian and Nalebuff, Barry},
  date                 = {2013-01},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Diversification Across Time},
  doi                  = {10.3905/jpm.2013.39.2.073},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {73--86},
  volume               = {39},
  abstract             = {Young people who buy stock on margin and reduce their equity exposure as they age can reduce lifetime portfolio risk. For example, an initially leveraged portfolio produces the same mean accumulation as a constant 74 percent stock allocation with a 21 percent smaller standard deviation. Since the means are equal, the reduced volatility doesn't depend on the equity premium.

A leveraged life-cycle strategy also lets investors come closer to their utility-maximizing equity allocation. Monte Carlo simulations show that the gains continue even with equity premiums well below historical levels.},
  citeulike-article-id = {13972068},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2013.39.2.073},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-08 06:43:35},
  timestamp            = {2020-02-25 21:00},
}

@PhdThesis{Bektic-2018,
  author         = {Bektic, Demir},
  date           = {2018},
  institution    = {Technical University of Darmstadt},
  title          = {Factor-based Portfolio Management with Corporate Bonds},
  type           = {phdthesis},
  url            = {https://tuprints.ulb.tu-darmstadt.de/7272/},
  abstract       = {Over the past 50 years financial asset pricing theories have evolved from simple single-factor models to more complex multi-factor models. Initially, Sharpe (1964) Capital Asset Pricing Model (CAPM) postulated that security markets can be described by a single factor (market beta). The basic premise of the model is that market participants require a risk premium for investing in high-beta assets that are typically considered more risky than low-beta assets. However, in the aftermath of the 2008 global financial crisis, two major trends emerged in the investment industry that laid the groundwork for the rise of factor-based investment strategies: 1) Investors started to evaluate and implement portfolio diversification in terms of underlying systematic risk factors given the failure of active management to provide adequate downside protection. 2) Investors demanded cost-effective, transparent and systematic alternative investment vehicles that could capture most or at least parts of active managers excess return. As a consequence, factor-based investing has grown in popularity and rapidly attracted academics, asset managers and institutional investors.},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Multifactor_Invest},
  timestamp      = {2020-02-25 21:00},
}

@Article{Benichou-et-al-2017,
  author               = {Benichou, Raphael and Lemperiere, Yves and Serie, Emmanuel and Kockelkoren, Julien and Seager, Philip and Bouchaud, Jean P. and Potters, Marc},
  date                 = {2017-06},
  journaltitle         = {Journal of Investment Strategies},
  title                = {Agnostic risk parity: taming known and unknown unknowns},
  doi                  = {10.21314/jois.2017.083},
  issn                 = {2047-1238},
  number               = {3},
  pages                = {1--12},
  volume               = {6},
  abstract             = {Markowitz' celebrated optimal portfolio theory generally fails to deliver out-of-sample diversification. In this note, we propose a new portfolio construction strategy based on symmetry arguments only, leading to "Eigenrisk Parity"portfolios that achieve equal realized risk on all the principal components of the covariance matrix. This holds true for any other definition of uncorrelated factors. We then specialize our general formula to the most agnostic case where the indicators of future returns are assumed to be uncorrelated and of equal variance. This "Agnostic Risk Parity"(AGP) portfolio minimizes unknown-unknown risks generated by over-optimistic hedging of the different bets. AGP is shown to fare quite well when applied to standard technical strategies such as trend following.},
  citeulike-article-id = {14386377},
  citeulike-linkout-0  = {http://dx.doi.org/10.21314/jois.2017.083},
  groups               = {Risk_Budgeting, Diversified_Invest, Invest_Risk},
  posted-at            = {2017-07-02 23:52:46},
  timestamp            = {2020-02-25 21:00},
}

@Article{Berger-et-al-2013a,
  author               = {Berger, Dave and Pukthuanthong, Kuntara and Yang, Jimmy J.},
  date                 = {2013-07-31},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Is the Diversification Benefit of Frontier Markets Realizable by Mean-Variance Investors? The Evidence of Investable Funds},
  doi                  = {10.3905/jpm.2013.39.4.036},
  issn                 = {0095-4918},
  number               = {4},
  pages                = {36--48},
  volume               = {39},
  abstract             = {The authors investigate whether the diversification benefits of frontier markets are realizable. They focus on investable frontier exchange-traded funds (ETFs) and their corresponding indices. Their analysis ncludes directly measuring the economic benefits of frontier-market diversification, as well as considering frontier-market trading dynamics. Evidence indicates that frontier markets offer diversification benefits through risk-reducing potential. The authors find that frontier market volatility tends to be largely idiosyncratic, which supports the risk-reducing role of frontier markets. Their comparison of funds and indices indicates that, to the extent that frontier-market indices offer hypothetical benefits, traders can obtain these benefits by using investable funds.},
  citeulike-article-id = {14504915},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2013.39.4.036},
  day                  = {31},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-12-18 21:50:53},
  timestamp            = {2020-02-25 21:00},
}

@Article{Bernardi-et-al-2018,
  author               = {Bernardi, Simone and Leippold, Markus and Lohre, Harald},
  date                 = {2018-01},
  journaltitle         = {European Financial Management},
  title                = {Maximum diversification strategies along commodity risk factors},
  doi                  = {10.1111/eufm.12122},
  issn                 = {1354-7798},
  number               = {1},
  pages                = {53--78},
  volume               = {24},
  abstract             = {Pursuing risk-based allocation across a universe of commodity assets, we find diversified risk parity (DRP) strategies to provide convincing results. DRP strives for maximum diversification along uncorrelated risk sources. A straightforward way to derive uncorrelated risk sources relies on principal components analysis (PCA). While the ensuing statistical factors can be associated with commodity sector bets, the corresponding DRP strategy entails excessive turnover because of the instability of the PCA factors. We suggest an alternative design of the DRP strategy relative to common commodity risk factors that implicitly allows for a uniform exposure to commodity risk premia.},
  citeulike-article-id = {14521503},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/eufm.12122},
  groups               = {Diversified_Invest, Invest_Risk, Invest_Cmdty, Invest_Diversif},
  posted-at            = {2018-01-22 17:16:36},
  timestamp            = {2020-02-25 21:00},
}

@Article{Binstock-et-al-2017,
  author               = {Binstock, Jay and Kose, Engin and Mazzoleni, Michele},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Diversification Strikes Again: Evidence from Global Equity Factors},
  url                  = {https://ssrn.com/abstract=3036423},
  abstract             = {The benefits of country diversification are well established. This article shows that the same benefits extend to equity factors, such as value, size, momentum, investment, and profitability. Specifically, country factor portfolios reflect both common variation, which we define as the global factor, and local variation. On average, a US investor could enjoy a 30 percent reduction in portfolio volatility by investing globally. We also document three other properties of equity factors. Like major asset classes, greater market integration is associated with greater factor co-movement, and factor portfolios of different countries tend to be more correlated during bear stock markets. However, unlike asset classes, the correlations of factor portfolios across countries have not been increasing over the last two decades, making global equity factors a particularly desirable addition to a portfolio.},
  citeulike-article-id = {14433817},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3036423},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-09-18 20:32:50},
  timestamp            = {2020-02-25 21:00},
}

@Article{Bock-2018,
  author         = {Bock, Johannes},
  date           = {2018-11-20},
  journaltitle   = {arXiv Electronic Journal},
  title          = {An updated review of (sub-)optimal diversification models},
  url            = {https://arxiv.org/abs/1811.08255},
  abstract       = {In the past decade many researchers have proposed new optimal portfolio selection strategies to show that sophisticated diversification can outperform the naive 1/N strategy in out-of-sample benchmarks. Providing an updated review of these models since DeMiguel et al. (2009b), I test sixteen strategies across six empirical datasets to see if indeed progress has been made. However, I find that none of the recently suggested strategies consistently outperforms the 1/N or minimum-variance approach in terms of Sharpe ratio, certainty-equivalent return or turnover. This suggests that simple diversification rules are not in fact inefficient, and gains promised by optimal portfolio choice remain unattainable out-of-sample due to large estimation errors in expected returns. Therefore, further research effort should be devoted to both improving estimation of expected returns, and possibly exploring diversification rules that do not require the estimation of expected returns directly, but also use other available information about the stock characteristics.},
  day            = {20},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:00},
}

@Article{Boigner-Gadzinski-2015,
  author               = {Boigner, Philip and Gadzinski, Gregory},
  date                 = {2015-03},
  journaltitle         = {Journal of Asset Management},
  title                = {Diversification with risk factors and investable hedge fund indices},
  doi                  = {10.1057/jam.2015.10},
  issn                 = {1470-8272},
  number               = {2},
  pages                = {101--116},
  volume               = {16},
  abstract             = {This article complements existing studies on dynamic portfolio construction by implementing a wide spectrum of optimization methodologies on four types of investments: traditional asset classes, risk factors and premia, and hedge fund investable indices. Portfolios are constructed using four different objectives with weights re-allocated every month during the period 2002-2013.

We show evidence that including hedge fund investable indices in a traditional portfolio is effective in mitigating volatility and drawdown. Risk factors portfolios do not benefit from the inclusion of hedge funds though. Moreover, we provide guidance on which allocation techniques should be used given the nature of the assets in one's portfolio.},
  citeulike-article-id = {13967863},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2015.10},
  groups               = {Diversified_Invest, Hedge_Funds, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-05 12:59:13},
  timestamp            = {2020-02-25 21:00},
}

@Article{Bouchey-et-al-2017,
  author         = {Bouchey, Paul and Li, Tianchuan and Nemtchinov, Vassilii},
  date           = {2017-08-31},
  journaltitle   = {The Journal of Investing},
  title          = {Systematic Diversification Using Beta},
  url            = {https://joi.pm-research.com/content/26/3/144},
  abstract       = {Higher-risk investments deserve higher expected returns to compensate for the extra risk, or so theory tells us. Historically, this has not always been the case for U.S. and other developed-market stocks. This anomaly, which is now well established by academics, has started to gain traction with investors. This is demonstrated by the large flows into the numerous low-beta and low-volatility strategies that were established in the wake of the Global Financial Crisis. The authors examine the beta anomaly in the academic literature and provide an empirical analysis for stocks in the U.S., developed, and emerging markets. Their primary finding is that beta is not a strong predictor of expected returns, but it is useful when used to help reduce risk in a portfolio. The authors present results for an investment strategy that filters out the highest-beta stocks while controlling for concentration risks by country and sector. The study finds mixed results for beta as an anomaly: Low beta outperforms for international markets, underperforms in emerging markets, and is flat in the U.S. markets. None of these differences appears to be statistically significant. The authors find beta is very useful, however, as a tool for controlling risk, especially in the context of strategies that diversify across countries and sectors.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-02-25 21:00},
}

@Article{Carl-2017,
  author               = {Carl, Ulrich},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {The Power of Equity Factor Diversification},
  url                  = {https://ssrn.com/abstract=2915443},
  abstract             = {This paper analyses the diversification properties of country equity factors across six equity factors and twenty developed markets from 1991 to 2015. The factors considered are the market excess return, size, value, momentum, low beta and quality. I find substantial diversification benefits along the country dimension as well as the factor dimension. In a portfolio setting, country diversification significantly reduces the volatility compared to single country investing for each of the six equity factors. Factor diversification works in each of the twenty markets by means of reducing the portfolio volatility.},
  citeulike-article-id = {14511760},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2915443},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2018-01-03 21:20:24},
  timestamp            = {2020-02-25 21:00},
}

@Article{Cesarone-et-al-2014,
  author               = {Cesarone, Francesco and Moretti, Jacopo and Tardella, Fabio},
  date                 = {2014},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Does Greater Diversification Really Improve Performance in Portfolio Selection?},
  doi                  = {10.2139/ssrn.2473630},
  issn                 = {1556-5068},
  abstract             = {One of the fundamental principles in portfolio selection models is minimization of risk through diversification of the investment. This seems to require that in a given working universe, or market, the investment should be spread among all (or almost all) the available assets. Indeed, this is what some classical investment strategies, like Equally-Weighted portfolios, or more recent and refined ones, like Risk Parity, actually recommend.

The purpose of this work consists in giving some empirical evidence of the fact that diversifying through the use of larger portfolios is not the best way to achieve an improvement in out-of-sample performance. More precisely, we investigate the role of the restriction on the number of assets in a portfolio (a cardinality constraint) on the in-sample and out-of-sample outcomes of the Equally-Weighted approach and of some well-known portfolio selection models that minimize risk through the use of Variance, Semi-Mean Absolute Deviation, and Conditional Value-at-Risk.

Our empirical analysis is based on some new and on some publicly available benchmark data sets often used in the literature.},
  citeulike-article-id = {14320281},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.2473630},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-03-26 19:16:17},
  timestamp            = {2020-02-25 21:00},
}

@Article{Chambers-Zdanowicz-2014,
  author               = {Chambers, Donald R. and Zdanowicz, John S.},
  date                 = {2014-07},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {The Limitations of Diversification Return},
  doi                  = {10.3905/jpm.2014.40.4.065},
  issn                 = {0095-4918},
  number               = {4},
  pages                = {65--76},
  volume               = {40},
  abstract             = {Diversification return is the amount by which the geometric mean return (i.e., average compounded return) of a portfolio exceeds the weighted average of the geometric means of the portfolio's constituent assets. Diversification return has been touted as a source of added return, even if markets are informationally efficient. Portfolio rebalancing has been advocated as a valuable source of diversification return.

The authors demonstrate that diversification return is not a source of increased expected value. However, portfolio rebalancing can be an effective mean-reverting strategy. Any enhanced expected value from rebalancing emanates from mean-reversion, rather than from diversification or variance reduction.},
  citeulike-article-id = {13972189},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2014.40.4.065},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-08 09:05:15},
  timestamp            = {2020-02-25 21:00},
}

@Article{Choueifaty-Coignard-2008,
  author       = {Choueifaty, Y. and Coignard, Y.},
  date         = {2008},
  journaltitle = {The Journal of Portfolio Management},
  title        = {Toward maximum diversification},
  number       = {1},
  pages        = {40--51},
  url          = {https://jpm.pm-research.com/content/35/1/40},
  volume       = {35},
  abstract     = {Along with the ongoing effort to build market cap-independent portfolios, the authors explore the properties of diversification as a driver of portfolio construction. They introduce a measure of the diversification of a portfolio that they term the diversification ratio. The measure is then employed to build a risk-efficient portfolio, or the Most- Diversified Portfolio. The theoretical properties of the resulting portfolios are discussed and compared to other popular methodologies, such as market-cap weights, equal weights, and minimum variance. The empirical results confirm that these popular methodologies are dominated by risk-efficient portfolios in many aspects. The implication is that in the long run, actively managed portfolios that maximize diversification are strong candidates for achieving consistently better results than commonly used passive index tracking methodologies. The message is clear- investors and their trustees cannot afford to ignore the benefits of maximal diversification.},
  groups       = {Diversified_Invest, Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:00},
}

@Article{Christophe-2017,
  author               = {Christophe, Stephen E.},
  date                 = {2017-04},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Time to Stay Home?Global Diversification during the Past 25 Years},
  doi                  = {10.3905/jwm.2017.2017.1.053},
  issn                 = {1534-7524},
  abstract             = {Over the past 25 years, as financial markets have become increasingly integrated, the role of foreign equities in a well-diversified portfolio has become increasingly uncertain. We present evidence that a globally diversified portfolio underperforms, on average, a U.S.-only allocation. Investors should not be overly optimistic about the potential benefits of international portfolio diversification.},
  citeulike-article-id = {14338557},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2017.2017.1.053},
  day                  = {07},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-04-17 11:21:53},
  timestamp            = {2020-02-25 21:00},
}

@Article{Chua-et-al-2009,
  author       = {Chua, D. B. and Kritzman, M. and Page, S.},
  date         = {2009},
  journaltitle = {The Journal of Portfolio Management},
  title        = {The Myth of Diversification},
  number       = {1},
  pages        = {26--35},
  url          = {https://jpm.pm-research.com/content/36/1/26},
  volume       = {36},
  abstract     = {Perhaps the most universally accepted precept of prudent investing is to diversify, yet this precept grossly oversimplifies the challenge of portfolio construction. Correlations, as typically measured over the full sample of returns, often belie an asset's diversification properties in market environments when diversification is most needed. Moreover, upside diversification is undesirable. The authors first describe the mathematics of conditional correlations assuming returns are normally distributed. Then they present empirical results across a wide variety of assets, which reveal that, unlike the theoretical conditional correlations, empirical correlations are significantly asymmetric. Finally, the authors show that a portfolio construction technique called full-scale optimization produces portfolios in which the component assets exhibit relatively lower correlations on the downside and higher correlations on the upside than mean-variance optimization portfolios.},
  groups       = {PortfOptim_FullScale, Diversified_Invest, Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:00},
}

@Article{Clarke-et-al-2013,
  author       = {Roger Clarke and Harindra de Silva and Steven Thorley},
  date         = {2013},
  journaltitle = {The Journal of Portfolio Management},
  title        = {Risk Parity, Maximum Diversification, and Minimum Variance: An Analytic Perspective},
  url          = {https://jpm.pm-research.com/content/39/3/39},
  abstract     = {Analytic solutions to risk parity, maximum diversification, and minimum variance portfolios provide useful perspectives about their construction and composition. Individual asset weights depend on both systematic and idiosyncratic risk in all three risk-based portfolios, but systematic risk eliminates many investable assets in long-only, constrained, maximum-diversification, and minimum-variance portfolios. On the other hand, risk-parity portfolios include all investable assets, and idiosyncratic risk has little effect on weight magnitude.

The algebraic forms for optimal asset weights derived in this article yield generalizable properties of risk-based portfolios, in contrast to empirical simulations that employ a specific set of historical returns, proprietary risk models, and multiple constraints. These analytic solutions reveal precisely how various kinds of predicted risk affect the relative magnitude of security weights in each type of risk-based portfolio construction.},
  groups       = {Risk_Budgeting, Diversified_Invest, Invest_Risk, Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:00},
}

@Article{Bergin-Pyun-2016,
  author               = {Bergin, Paul R. and Pyun, Ju H.},
  date                 = {2016-04},
  journaltitle         = {Journal of International Money and Finance},
  title                = {International portfolio diversification and multilateral effects of correlations},
  doi                  = {10.1016/j.jimonfin.2015.12.012},
  issn                 = {0261-5606},
  pages                = {52--71},
  volume               = {62},
  abstract             = {Bilateral asset holdings depend on the correlation with all other countries. Higher stock return correlations lower bilateral equity asset holdings. Multilateral effects of correlations bias estimates. Not only are investors biased toward home assets, but when they do invest abroad, they appear to favor countries with returns more correlated with home assets. Often attributed to a preference for familiarity, this 'correlation puzzle' further reduces effective diversification. We use a multi-country general equilibrium model of portfolio choice to study how bilateral equity holdings are affected by return correlations among alternative destination and source countries. From the theoretical model, we develop an empirical approach to estimate a gravity equation for equity holdings that incorporates the overall covariance structure in a theoretically rigorous yet tractable manner. Estimation using this approach resolves the correlation puzzle, and finds that international investors do seek the diversification benefits of low cross-country correlations, as theory would predict.},
  citeulike-article-id = {14339368},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jimonfin.2015.12.012},
  groups               = {Effective_Dim_Diversif, Invest_Diversif},
  posted-at            = {2017-04-19 06:32:43},
  timestamp            = {2020-02-25 21:00},
}

@InCollection{DeSilva-et-al-2017,
  author               = {{De Silva}, Harindra and McMurran, Gregory M. and Miller, Megan N.},
  booktitle            = {Factor Investing},
  date                 = {2017},
  title                = {Diversification and the Volatility Risk Premium},
  doi                  = {10.1016/b978-1-78548-201-4.50014-3},
  isbn                 = {9781785482014},
  pages                = {365--387},
  publisher            = {Elsevier},
  abstract             = {The volatility risk premium (VRP) found in options has paid off persistently across different assets, different asset classes and over time. A consistent short volatility position using options or volatility swaps has produced attractive risk-adjusted returns because of exposure to VRP. In this chapter, we have extended the study of the VRP to include not only equity indices but also commodities, government bonds and currencies. Using volatility swap returns as a measure of the payoff to the VRP, we see that the returns to a short volatility position are correlated to the volatility of the underlying instrument and to other VRPs in the same asset class. We also find that the returns are relatively uncorrelated to the VRPs of other asset classes and to the traditional equity factors represented by pure factor portfolios (PFPs). Finally, we show that the multiasset class VRP portfolio studied in this chapter has very competitive risk-adjusted returns},
  citeulike-article-id = {14499078},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/b978-1-78548-201-4.50014-3},
  groups               = {Diversified_Invest, RiskPremia_FixedIncome, RiskPremia_Other, Invest_Diversif, [nbkcbu3:]},
  posted-at            = {2017-12-08 00:25:33},
  timestamp            = {2020-02-25 21:06},
}

@Article{Dickson-2016a,
  author               = {Dickson, Mike},
  date                 = {2016-05},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Naive Diversification Isn't So Naive After All},
  url                  = {https://ssrn.com/abstract=2713501},
  abstract             = {I conduct a horse-race of 15 portfolio construction techniques over 8 empirical datasets comprised of individual stocks. I also conduct a robust Monte Carlo analysis that confirms that recent extensions of mean-variance optimization due to Kirby and Ostdiek (2012) are successful in curbing estimation risk and turnover. Despite these facts, my results indicate that no strategy consistently outperforms naive diversification in terms of mean excess return, Sharpe ratio, and turnover. I introduce a statistic, the time series average of the cross-sectional mean absolute deviation of risk and return, to explain why I observe these results. Data limitations and dataset characteristics contribute the most to the performance of a candidate strategy. I also propose several extensions to active timing strategies and include new characteristics in a parametric portfolio choice framework. Naive diversification continues to prevail, suggesting practical optimization techniques are inferior to naive diversification when forming portfolios of individual stocks.},
  citeulike-article-id = {14134601},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2713501},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2713501code2352812.pdf?abstractid=2713501 and mirid=1},
  day                  = {19},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-09-11 12:16:21},
  timestamp            = {2020-02-25 21:06},
}

@Article{Diyarbakrloglu-Satman-2013,
  author               = {Diyarbakrloglu, Erkin and Satman, Mehmet H.},
  date                 = {2014-01},
  journaltitle         = {Journal of Asset Management},
  title                = {The Maximum Diversification Index},
  doi                  = {10.1057/jam.2013.28},
  issn                 = {1470-8272},
  number               = {6},
  pages                = {400--409},
  volume               = {14},
  abstract             = {We propose a new method to assess the risk diversification potential of a given investment set, using only the information content of the covariance matrix of returns. Namely, we extend Rudin and Morgan's (2006) work to numerically solve for the 'Maximum Diversification Index' by means of a genetic algorithm.

Using stock returns data from the S and P-500 index, we show that the MDI can be efficiently implemented to delimit a large set of investable assets by eliminating those subjects that do not improve the diversification characteristics of the underlying portfolio pool. Indeed, a subset of the S and P-500 stocks obtained using the MDI procedure preserves the mean-variance properties of the initial dataset as shown by the ex-post efficient frontiers.},
  citeulike-article-id = {13968912},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2013.28},
  day                  = {16},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-06 06:31:14},
  timestamp            = {2020-02-25 21:06},
}

@Article{duPlessis-vanRensburg-2017,
  author               = {{du Plessis}, Hannes and {van Rensburg}, Paul},
  date                 = {2017-06},
  journaltitle         = {Investment Analysts Journal},
  title                = {Diversification and the realised volatility of equity portfolios},
  doi                  = {10.1080/10293523.2017.1335367},
  pages                = {1--22},
  abstract             = {In Markowitz's (1952) portfolio theory, a reduction in volatility for a given level of expected return is implied as being equivalent to an increase in diversification. The recent development of risk-based portfolio construction methods, which emphasise diversification separately from volatility reduction, challenges this equivalence. Using a point-in-time database of liquid equities listed on the Johannesburg Stock Exchange between 1998 and 2016, a numerical simulation technique is employed to study the behaviour of a range of diversification measures as a portfolio-level attribute and assess and compare their usefulness in estimating out-of-sample portfolio volatility. The empirical performance of maximum diversification portfolios based on each measure is then investigated. It is found that a portfolio?s diversification level is a significant predictor of future portfolio risk beyond that of historic volatility, and that the empirical performance of maximum diversification portfolios, attractive in all cases, depends critically on the definition of diversification applied.},
  citeulike-article-id = {14388871},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/10293523.2017.1335367},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/10293523.2017.1335367},
  day                  = {26},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-07-06 13:37:32},
  publisher            = {Routledge},
  timestamp            = {2020-02-25 21:06},
}

@TechReport{Durante-et-al-2015a,
  author               = {Durante, Fabrizio and Foscolo, Enrico and Pappada, Roberta and Wang, Hao},
  date                 = {2015},
  institution          = {DEAMS Research Paper Series},
  title                = {A portfolio diversification strategy via tail dependence measures},
  url                  = {https://www.openstarts.units.it/handle/10077/11865},
  abstract             = {We provide a two-stage portfolio selection procedure in order to increase the diversification benefits in a bear market. By exploiting tail dependence-based risky measures a first-step cluster analysis is carried out for discerning between assets with the same performance during risky scenarios. Then a mean-variance efficient frontier is computed by fixing a number of assets per portfolio and by selecting only one item from each cluster. Empirical calculations on the EURO STOXX 50 prove that investing on selected index components in trouble periods may improve the risk-averse investor portfolio performance.},
  citeulike-article-id = {14150078},
  citeulike-linkout-0  = {https://www.openstarts.units.it/dspace/bitstream/10077/11865/1/DEAMSRP20153DuranteFoscoloPappadaWang.pdf},
  groups               = {Networks and investment management, Diversification_Measure, Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:48:29},
  timestamp            = {2020-02-25 21:06},
}

@Article{Facchinato-Pola-2014,
  author       = {Simone Facchinato and Gianni Pola},
  date         = {2014},
  journaltitle = {SSRN Electronic Journal},
  title        = {Managing uncertainty with diversification across macroeconomic scenarios (DAMS): from asset segmentation to portfolio},
  url          = {http://research-center.amundi.com/page/Publications/Discussion-Paper/2014/Managing-uncertainty-with-Diversification-Across-Macroeconomic-Scenarios-DAMS-from-asset-segmentation-to-portfolio},
  abstract     = {Recent history has provided an excellent laboratory to test the robustness of investment processes. Despite claims of diversification, most balanced portfolios and pension funds were concentrated on equity risk and, consequently, key investment decisions ultimately consisted in a single binary bet: buy or sell equity. This led to pro-cyclical returns and generated a broad debate on the effectiveness of active management in generating performance in difficult market conditions.

In 2011 AMUNDI Italy decided to revise the asset allocation process starting with a reinterpretation of portfolio diversification in terms of Diversification Across Macroeconomic Scenarios (DAMS). The main ambitions of DAMS are: (i) to explain complex patterns of large investment universes in terms of a limited number of factors and (ii) to catch up the market risk premium without being exposed to specific macroeconomic dynamics and asset idiosyncratic risk. In a previous study we illustrated the DAMS principle and implications in terms of asset segmentation.

The aim of this paper is to move towards a new framework for multi-asset portfolio management, what we call DAMS second generation. DAMS first generation is enriched with new concepts and tools that enable us (i) to infer market expectations on relevant macroeconomic factors (growth and inflation) and global risk premium, and (ii) to properly manage portfolios via strategic and tactical asset allocation.},
  groups       = {Diversified_Invest, Scenario_Market, Scenario_Portfolio, Invest_Diversif},
  howpublished = {Available at http://research-center.amundi.com/page/Publications/Discussion-Paper/Managing-uncertainty-with-Diversification-Across-Macroeconomic-Scenarios-DAMS-from-asset-segmentation-to-portfolio},
  organization = {Amundi},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:06},
}

@Article{Flint-et-al-2016c,
  author               = {Flint, Emlyn J. and Chikurunhe, Florence and Seymour, Anthony J.},
  date                 = {2016},
  journaltitle         = {SSRN Electronic Journal},
  title                = {The Cost of a Free Lunch: Dabbling in Diversification},
  url                  = {https://ssrn.com/abstract=2767436},
  abstract             = {It's often said that diversification is the only 'free lunch' available to investors; meaning that a properly diversified portfolio reduces total risk without necessarily sacrificing expected return. However, achieving true diversification is easier said than done, especially when we don't fully know what we mean when we're talking about diversification. While the qualitative purpose of diversification is well-known, a satisfactory quantitative definition of portfolio diversification is not. In this report, we summarise a wide range of diversification measures, focussing our efforts on those most commonly used in practice. We categorise each measure based on which portfolio aspect it focusses on: cardinality, weights, returns, risk or higher moments. We then apply these measures to a range of South African equity indices, thus giving a diagnostic review of historical local equity diversification and, perhaps more importantly, providing a description of the investable opportunity set available to fund managers in this space. Finally, we introduce the idea of diversification profiles. These regime-dependent profiles give a much richer description of portfolio diversification than their single-value counterparts and also allow one to manage diversification proactively based on one's view of future market conditions.},
  citeulike-article-id = {14186293},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2767436},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-11-17 22:18:46},
  timestamp            = {2020-02-25 21:06},
}

@Article{Flores-et-al-2017,
  author               = {Flores, Yuri Salazar and Bianchi, Robert J. and Drew, Michael E. and Truck, Stefan},
  date                 = {2017-07},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {The Diversification Delta: A Different Perspective},
  doi                  = {10.3905/jpm.2017.43.4.112},
  issn                 = {0095-4918},
  number               = {4},
  pages                = {112--124},
  volume               = {43},
  abstract             = {In a 2012 article published in The Journal of Portfolio Management, Vermorken, Medda, and Schroder introduce a new measure of diversification, the Diversification Delta (DD), based on the entropy of the portfolio return distribution. Entropy as a measure of uncertainty has been used successfully in several frameworks and takes into account the entire statistical distribution, rather than just the first two moments. In this article, the authors highlight some drawbacks of the DD measure and go on to propose an alternative measure based on exponential entropy that overcomes the identified shortcomings. The authors present the properties of this new measure and propose it as an alternative for portfolio optimization that incorporates higher moments of asset returns, such as skewness and excess kurtosis.},
  citeulike-article-id = {14400399},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2017.43.4.112},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-07-27 09:47:48},
  timestamp            = {2020-02-25 21:06},
}

@Article{Fragkiskos-2014,
  author       = {Fragkiskos, A.},
  date         = {2014},
  journaltitle = {SSRN Electronic Journal},
  title        = {What is portfolio diversification?},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2331475},
  abstract     = {There is considerable controversy concerning what exactly portfolio diversification is and under what circumstances is it beneficial to investors, particularly in the wake of the most recent financial crisis in 2008. This paper gathers the various approaches on portfolio diversification throughout history, placing a higher emphasis on recent developments. The goal of this paper is not to provide an exhaustive list of diversification strategies, but rather to highlight the most commonly used ones, provide the motivation behind each approach, and show how they compare with real data.},
  groups       = {Diversified_Invest, Invest_Diversif},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2331475},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:06},
}

@Article{Geczy-2016,
  author               = {Geczy, Christopher},
  date                 = {2016-11},
  journaltitle         = {The Journal of Private Equity},
  title                = {The New Diversification: Open Your Eyes to Alternatives},
  doi                  = {10.3905/jpe.2016.20.1.072},
  issn                 = {1096-5572},
  number               = {1},
  pages                = {72--81},
  volume               = {20},
  abstract             = {During the 2008 financial crisis, many portfolios considered widely diversified failed to fulfill their expected function of protecting against large drawdowns. Historically, correlations among various types of stocks and bonds have usually increased during financial shocks, but the diversification shortcomings of standard portfolio allocations still surprised investors. Six years later, managers have a more sophisticated understanding of portfolio drawdown risk and how to mitigate it through diversification. In this article, the author advocates a focus on the risk exposures within a portfolio and inclusion of risk diversifiers-often sourced through so-called alternatives-to design portfolios more resistant to volatility spikes and major shocks.},
  citeulike-article-id = {14217774},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpe.2016.20.1.072},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-11-30 19:39:57},
  timestamp            = {2020-02-25 21:06},
}

@Article{Hardle-et-al-2018,
  author               = {Hardle, Wolfgang K. and Lee, David K. and Nasekin, Sergey and Petukhina, Alla},
  date                 = {2018},
  journaltitle         = {Journal of Asset Management},
  title                = {Tail Event Driven ASset allocation: evidence from equity and mutual funds' markets},
  doi                  = {10.1057/s41260-017-0060-9},
  pages                = {1--15},
  abstract             = {The correlation structure across assets and opposite tail movements are essential to the asset allocation problem, since they determine the level of risk in a position. Correlation alone is not informative on the distributional details of the assets. Recently introduced TEDAS-Tail Event Driven ASset allocation approach determines the dependence between assets at different tail measures. TEDAS uses adaptive Lasso-based quantile regression in order to determine an active set of negative coefficients. Based on these active risk factors, an adjustment for intertemporal correlation is made. In this research, authors aim to develop TEDAS, by introducing three TEDAS modifications differing in allocation weights' determination: a Cornish-Fisher Value-at-Risk minimization, Markowitz diversification rule or naive equal weighting. TEDAS strategies significantly outperform other widely used allocation approaches on two asset markets: German equity and Global mutual funds.},
  citeulike-article-id = {14479426},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/s41260-017-0060-9},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1057/s41260-017-0060-9},
  groups               = {Diversified_Invest},
  posted-at            = {2017-11-20 20:16:48},
  publisher            = {Palgrave Macmillan UK},
  timestamp            = {2020-02-25 21:06},
}

@Article{Heinze-2016,
  author               = {Heinze, Thomas},
  date                 = {2016},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Markowitz {3.0}: Including Diversification Targets in Portfolio Optimization via Diversification Functions},
  doi                  = {10.2139/ssrn.2805368},
  issn                 = {1556-5068},
  abstract             = {Given Markowitz's mean-risk model, maximization of diversification is established as an additional investment target next to return maximization and risk minimization. This widens the opportunity to transfer market views into the model by additional diversification parameters and should therefore lead to an improved mapping of economic reality. The main focus is on the introduction of diversification functions which make diversification quantifiable and which are used as third objective in the optimization. Thus, the resulting efficient frontier extends to a three dimensional surface which includes the original efficient frontier according to Markowitz. Starting with the original Markowitz model through improvements in stochastic modelling in terms of risk measures, copulas, fat tails, etc., leaving the pure return/risk context can be interpreted as a third model generation.},
  citeulike-article-id = {14332615},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.2805368},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-04-06 01:00:31},
  timestamp            = {2020-02-25 21:06},
}

@Article{Hjalmarsson-2011,
  author               = {Hjalmarsson, Erik},
  date                 = {2011-11},
  journaltitle         = {The Journal of Investing},
  title                = {Portfolio Diversification Across Characteristics},
  doi                  = {10.3905/joi.2011.20.4.084},
  issn                 = {1068-0896},
  number               = {4},
  pages                = {84--88},
  volume               = {20},
  abstract             = {This article studies long short portfolio strategies formed on seven different stock characteristics representing various measures of past returns, value, and size. Each individual characteristic results in a profitable portfolio strategy, but these single-characteristic strategies are dominated by a diversified strategy that places equal weight on each of the single-characteristic strategies. The benefits of diversifying across characteristic-based long short strategies are substantial and can be attributed to the mostly low, and sometimes substantially negative, correlation between the returns on the single-characteristic strategies.},
  citeulike-article-id = {13970992},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/joi.2011.20.4.084},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-07 00:25:40},
  timestamp            = {2020-02-25 21:06},
}

@Article{Homescu-2014b,
  author       = {C. Homescu},
  date         = {2014},
  journaltitle = {SSRN Electronic Journal},
  title        = {Many risks, one (optimal) portfolio},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2473776},
  abstract     = {This study investigates how to obtain a portfolio which would provide above average returns while remaining robust to most risk exposures. Emphasis is placed on risk management, given our perspective (shared by many other practitioners), that retaining above average portfolio performance in current market environment depends strongly on having an effective risk management process.

We rely on a comprehensive survey of the literature to describe stylized facts of market returns and main categories of asset allocation methodologies, including Modern Portfolio Theory, Black-Litterman model, factor-based and risk-based strategies. Furthermore, we present both criticisms and defenses of strategies, together with potential issues identified by practitioners and corresponding solutions (if they do exist).

We outline recent enhancements to various types of portfolio strategies, and analyze how to incorporate (in the asset allocation framework) constraints, regularization, personal views, stylized features of empirical market data, and forward information given by financial options market data.

More prominence is given to strategies (risk parity, risk factors, factor investing, smart beta, dynamic, etc.) that were shown to deliver better portfolio performance in terms of returns, diversification, risk, etc. We also discuss a wide ranging collection of performance measures proposed in the literature for quantifying portfolio return, risk and diversification, identify which such measures are most popular with practitioners, and which corresponding strategies have best results (as shown in the literature).

Since a major topic of this study is managing risks, we provide details on the types of risk that portfolios may be exposed to, on approaches and strategies to handle such exposures, with highlighting of tail risk management. Portfolio insurance is also discussed.

We also describe practical aspects needed for a successful portfolio management, including robust estimation of covariances, correlations and model parameters, numerical optimization methods, key questions and issues identified by practitioners, Monte Carlo simulation, comprehensive testing framework, stress testing, available software implementations (usually in R), etc.

To summarize, the study analyzes all ingredients that are required, in our opinion, to deliver portfolios with above average performances and resilient to most risks, and concentrates on the strategies which have emerged as frontrunners in the last few years, both in the literature and in the market.},
  groups       = {Mean_Variance, Black_Litterman, Risk_Budgeting, PortfOptim_Robust, Invest_Risk, Factor_Types, Factor_Selection, Factor_Test, Invest_SmartBeta, DAA, Diversified_Invest, OBPI},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2473776},
  owner        = {cristi},
  timestamp    = {2020-02-25 21:06},
}

@Article{Hwang-et-al-2018,
  author               = {Hwang, Inchang and Xu, Simon and In, Francis},
  date                 = {2018-02},
  journaltitle         = {European Journal of Operational Research},
  title                = {Naive versus optimal diversification: Tail risk and performance},
  doi                  = {10.1016/j.ejor.2017.07.066},
  issn                 = {0377-2217},
  number               = {1},
  pages                = {372--388},
  volume               = {265},
  abstract             = {It is well documented in portfolio optimization that naive diversification outperforms optimal mean-variance diversification because the latter is subject to severe estimation error. Our study provides an alternative explanation for the outperformance of naive diversification by examining the tail risk of naive diversification relative to optimal mean-variance diversification. We utilize a rolling-sample approach and compare the out-of-sample performance and tail risk of various optimal strategies to that of the naive diversification strategy. Using portfolios consisting of individual stocks, we show that for portfolios containing relatively small number of stocks, naive diversification outperforms optimal mean-variance diversification and is less exposed to tail risk. However, for relatively large number of stocks in the portfolio, naive diversification maintains its superior performance but increases tail risk and results in more concave portfolio returns. These results imply that the outperformance of naive diversification acts as compensation for the increase in tail risk and concavity.},
  citeulike-article-id = {14500767},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2017.07.066},
  groups               = {Diversified_Invest, Invest_Risk, Invest_TailRisk, Invest_Diversif},
  posted-at            = {2017-12-11 09:23:28},
  timestamp            = {2020-02-25 21:06},
}

@Article{Ilmanen-Kizer-2012,
  author       = {Ilmanen, Antti and Jared Kizer},
  date         = {2012},
  journaltitle = {The Journal of Portfolio Management},
  title        = {The Death of Diversification Has Been Greatly Exaggerated},
  pages        = {15--27},
  url          = {https://www.aqr.com/Insights/Research/Journal-Article/The-Death-of-Diversification-Has-Been-Greatly-Exaggerated},
  volume       = {38},
  abstract     = {Asset-class correlations generally tend to rise during crises. That certainly was true in the 2007-2009 financial crisis, and since then correlations have generally remained elevated as markets switch between binary risk-on/risk-off environments. However, we believe it would be wrong to interpret these developments as conclusive evidence of the death of diversification.

First, academics (Asness, Israelov and Liew [2011]) have stressed that while diversification often fails in short-term panics - especially one as systemic as the 2007-2009 crisis - it does effectively reduce downside risks over longer horizons. Second, high-quality bonds have fairly consistently provided positive returns during stressful market environments. Third, in this article, we argue and show that factor diversification has been more effective than asset-class diversification in general and, in particular, during crises. The last two arguments challenge the concentration in equity risk found in most institutional portfolios, which is also a central argument in favor of more risk-balanced, so-called risk parity, portfolios.

Traditional asset-class diversification involves allocating nominal dollars to various asset classes and their subsets. Several large institutions have begun to explore an alternative perspective of factor allocation, asking: What are the most important factors driving our portfolio returns? This perspective involves at least two changes. First, focus is shifted from dollar allocations to risk allocations. This change often reveals the dominant role of the most volatile asset classes and the portfolio's dependence on equity market direction. Second, portfolio analysis is extended beyond asset classes to dynamic strategy styles or to underlying risk factors. Fundamental factors such as growth, inflation and liquidity are naturally interesting, but they are inherently hard to measure. Most investors prefer investable factors and therefore use market-based proxies - equities for growth, Treasuries for deflation and commodities for inflation.},
  groups       = {Diversified_Invest, Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:06},
}

@Article{Iraya-Wafula-2018,
  author         = {Iraya, Cyrus and Wafula, Fernandes Juma},
  date           = {2018-02-28},
  journaltitle   = {European Scientific Journal},
  title          = {Does portfolio diversification affect performance of balanced mutual funds in kenya?},
  doi            = {10.19044/esj.2018.v14n4p158},
  issn           = {1857-7881},
  number         = {4},
  volume         = {14},
  abstract       = {Literature provides conflicting results on the effect of diversification on performance of mutual funds with some studies showing a positive relationship (Markowitz, 1952; Muriithi, 2005; Kagunga, 2010), others negative (Chang AND Elyasiani, 2008; Fiegenbaum and Thomas, 1998) and still others showing that there is no relationship between the two variables (Loeb, 1950). It is with this background that this study sought to establish the effect of diversification on performance of mutual funds in Kenya. The study took a descriptive research design approach on weekly performance of a sample of 7 balanced mutual funds for the year 2013.The study used secondary data sources available at the Capital Market Authority offices and from each mutual funds. The portfolio return was determined by computing the changes in prices of the balanced fund as traded at the Nairobi Securities Exchange (NSE) while diversification was determined from the level of Unsystematic Risk in the Performance. The study used the Ordinary Least Squares (OLS) multiple linear regression equation. Control variables of the size and age of the fund were introduced in the regression model. The results indicated the existence of a positive relationship between the Unsystematic Risk and Performance of balanced mutual funds with a beta coefficient of 0.069 (t=4.971, p 0.5. This implies that the lower the diversification the higher the performance of mutual funds.},
  day            = {28},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:06},
}

@Article{Kanuri-et-al-2018,
  author               = {Kanuri, Srinidhi and Malhotra, Davinder and Malm, James},
  date                 = {2018-01-23},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Evaluating the Performance and Diversification Benefits of Emerging-Market Exchange-Traded Funds},
  doi                  = {10.3905/jwm.2018.20.4.085},
  issn                 = {1534-7524},
  number               = {4},
  pages                = {85--90},
  url                  = {https://jwm.pm-research.com/content/20/4/85},
  volume               = {20},
  abstract             = {This study evaluates the performance and diversification benefits for U.S. investors of emerging-market exchange-traded funds (ETFs) since their inception in January 2003 through June 2015 by comparing their absolute and risk-adjusted performance with the iShares Core SandP 500 ETF (IVV). The authors find that the emerging-market ETF portfolio has very low correlations with IVV during the period of the study. Emerging-market ETF portfolios delivered better absolute performance (returns and wealth) but also had much higher risk (standard deviation of returns). However, the risk-adjusted performance (Sharpe and Omega ratios) of IVV was better than that of the emerging-market ETF portfolio. They also look at the effect of adding some emerging-market ETFs to IVV during the period of study. The authors find that adding some emerging-market ETFs to IVV leads to higher absolute returns, better risk-adjusted performance (Sharpe and Omega ratios), higher cumulative returns, and increased wealth for U.S. investors. Results are statistically significant at 1\% in all cases. Therefore, U.S. investors should add some emerging-market ETFs to their domestic allocation based on their risk tolerance for better performance (absolute and risk-adjusted).},
  citeulike-article-id = {14525342},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2018.20.4.085},
  day                  = {23},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2018-01-28 16:35:16},
  timestamp            = {2020-02-25 21:10},
}

@Article{KaradedeBouras-Laopodis-2015,
  author               = {Karadede-Bouras, Markella and Laopodis, Nikiforos T.},
  date                 = {2015-07},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Dynamics among Traditional and Alternative Assets: Implications for Diversification and Risk},
  doi                  = {10.3905/jwm.2015.18.2.013},
  issn                 = {1534-7524},
  number               = {2},
  pages                = {13--34},
  volume               = {18},
  abstract             = {The authors examine the dynamic correlations and implications of various fi nancial asset classes, such as equities, bonds, ETFs, commodities, and real estate, in the United States from 1990 to 2013 and find that the correlations have varied across time.

They detect no evidence of contagion but rather of herding behavior among these assets. The variation was more pronounced during market declines, but differed in extent across economic expansions and contractions. Finally, shocks from one asset class to another were not persistent, meaning that the assets were able to absorb the shocks and quickly return to normalcy.

The implications for portfolio decisions are clear: Even well-diversified portfolios must be updated to reflect changing economic and financial environments, and past asset behavior does not imply similar future behavior.},
  citeulike-article-id = {13968108},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2015.18.2.013},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-05 18:07:15},
  timestamp            = {2020-02-25 21:10},
}

@Article{Kind-Poonia-2014,
  author               = {Kind, Christoph and Poonia, Muddit},
  date                 = {2014-03},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Diversification Management of a Multi-Asset Portfolio},
  url                  = {https://ssrn.com/abstract=2410153},
  abstract             = {It is a well-known fact in finance that classical mean-variance optimization often leads to highly concentrated portfolios. Giving equal weights to all portfolio assets will instead allow for maximum nominal diversification. More sophisticated ways of nominal diversification are the maximum diversification approach proposed by Choueifaty and Coignard (2008) or the equal weighting of total risk contributions known as risk parity . Instead of looking for nominal diversification, investors may prefer a diversification of the risk factors that drive portfolio returns. In recent papers, risk factors have been modelled by principal components following Partovi and Caputo (2004). Meucci et al. (2013) show that principal components may not be the best way to model risk factors and propose minimum torsion bets instead. The present paper discusses different ways of managing diversification and backtests these strategies in a multi-asset portfolio.},
  citeulike-article-id = {13997420},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2410153},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2410153code1500322.pdf?abstractid=2410153 and mirid=1},
  day                  = {18},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-04-05 04:54:23},
  timestamp            = {2020-02-25 21:10},
}

@Article{Kupiec-2016,
  author               = {Kupiec, Paul},
  date                 = {2016},
  journaltitle         = {Journal of Investment Management},
  title                = {Portfolio Diversification In Concentrated Bond And Loan Portfolios},
  number               = {2},
  url                  = {https://www.joim.com/portfolio-diversification-in-concentrated-bond-and-loan-portfolios/},
  volume               = {14},
  abstract             = {I develop an algorithm to approximate the loss rate distribution for fixed income portfolios with obligor concentrations. The approximation requires no advanced mathematics or statistics, only the summation of large exposures and the evaluation of binomial probabilities. The approximation is model-independent and can be used after removing default dependence using any risk modeling approach. It is especially useful for capital calculations given its inherent accuracy in the upper tail of the cumulative portfolio loss rate distribution. The approximation provides a simple way to calculate the capital benefits of risk mitigation or the capital needed when a marginal credit is added to a concentrated portfolio},
  citeulike-article-id = {14486905},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-12-01 22:20:00},
  timestamp            = {2020-02-25 21:10},
}

@Article{Lassance-et-al-2018,
  author         = {Lassance, Nathan and DeMiguel, Victor and Vrins, Frederic Daniel},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Optimal portfolio diversification via independent component analysis},
  doi            = {10.2139/ssrn.3285156},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3285156},
  abstract       = {A popular approach for enhancing diversification in portfolio selection is to rely on the factor-risk-parity portfolio, which is often defined as the portfolio whose return variance is spread equally among the principal components (PCs) of asset returns. Although PCs are useful for dimensionality reduction, they are arbitrary because any rotation of the PC basis yields an equally uncorrelated basis. This is problematic because we theoretically demonstrate that any portfolio is the factor-risk-parity portfolio corresponding to a specific uncorrelated basis. To overcome this problem, we rely on the factor-risk-parity portfolio based on the independent components (ICs), which are the rotation of the PCs that are maximally independent, and thus, account for higher-order moments. We propose a shrinkage portfolio that is obtained by combining the minimum-variance portfolio and the IC-risk-parity portfolio. We also show how to exploit the near independence of the ICs to parsimoniously estimate the factor-risk-parity portfolio with respect to Value-at-Risk. Finally, we empirically demonstrate that shrinkage portfolios based on the IC basis outperform those based on the PC basis, as well as the minimum-variance portfolio.},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:10},
}

@PhdThesis{Lee-2016a,
  author               = {Lee, Yongjae},
  date                 = {2016},
  institution          = {KAIST},
  title                = {Demystifying Diversification Strategies by using Portfolio Optimization Techniques},
  url                  = {https://www.researchgate.net/publication/309563859_Demystifying_Diversification_Strategies_by_using_Portfolio_Optimization_Techniques},
  abstract             = {In this dissertation, we study a series of diversification strategies in order to improve the understanding of the diversification of investments that was mathematically established by Harry Markowitz. Even though Modern Portfolio Theory considers a trade-off between generating high returns and lowering risks, investment processes inspired by the concept of diversification are generally only addressed with their diversification benefits. Therefore, we analyze the diversification strategies by using portfolio optimization techniques that primarily originated from the mean-variance framework, which considers returns as well as risks with equal importance, in order to more fully understand the quantifiable consequences of various diversification strategies. First, we investigate passive investing and performance benchmarking through analyzing the two most popular equity benchmark portfolios: the cap-weighted portfolio, and the equally-weighted portfolio. As conventional portfolio performance evaluations occur relative to benchmarks, the performance evaluation of the benchmark itself has never been a trivial issue. Thus, an alternative methodology for portfolio performance evaluation that can be conducted without peer information is proposed, and we find little or no evidence of either benchmark portfolio performing better than the average portfolio. In terms of performance benchmarking, however, equally-weighted portfolios exhibit more desirable properties than cap-weighted portfolios. Second, we examine the quantitative properties of asset allocation and asset classification. We derive and compare the closed form expressions for the portfolio performances of asset allocation and direct security selection, and we find that the majority of investors can benefit from employing asset allocation. Furthermore, our analysis indicates that the design of asset classes is a critical factor in determining the portfolio performance of employing asset allocation. Hence, we further test the two most widely used within-stock asset classification schemes, i.e. style and industry classifications, and find that the asset designs should not focus on diversification benefits only.

Third, we discuss the viability of robo-advising, which was recently developed during the ongoing expansion of financial technology (FinTech). Robo-advising attempts to lower the entry barrier to financial advising through utilizing automated but personalized algorithms, in order to attract investors with smaller accounts, who are ineligible to receive traditional financial advising services. We investigate the relationship between portfolio size and risk in order to examine the viability of roboadvisers in providing diversification benefits with limited portfolio sizes. The results indicate that a substantial investment is not necessary to gain diversification benefits.},
  citeulike-article-id = {14525362},
  groups               = {Diversified_Invest, AssetAlloc_vs_SecSelect, FinTech_WealthTech, Invest_Diversif},
  posted-at            = {2018-01-28 17:17:54},
  timestamp            = {2020-02-25 21:10},
}

@Article{Linder-2018,
  author         = {Linder, John},
  date           = {2018-05-31},
  journaltitle   = {The Journal of Investing},
  title          = {Rebalancing-Diversification Return: The Opportunity Cost of Illiquid Investments},
  doi            = {10.3905/joi.2018.27.2.057},
  issn           = {1068-0896},
  number         = {2},
  pages          = {57--65},
  volume         = {27},
  abstract       = {Institutional investors expect a return premium for illiquidity when an investment is private and cannot be sold easily in an established liquid market. For investors that have a very long investment horizon some might argue a permanent (or perpetual) investment portfolio in private markets with any such level of expected return premium might seem to be dominant to a markets only construct. However, how much should this premium be? The author hypothesizes that the illiquidity premium observed is directly related to the risk-equivalent liquid markets diversification-rebalancing returns forgone in pursuing illiquid investments. The author posits the excess return to illiquidity available to the long-term, non-liquidity constrained investor, is an investor-specific opportunity cost of illiquidity, and by logical extension, he proposes, an (efficient) market cost to illiquidity hypothesis. Finally, he examines the private equity industry benchmarking convention for performance evaluation-cap stocks + 300 bps this paradigm.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Rebalancing, Invest_Liquidity, Invest_Diversif},
  timestamp      = {2020-02-25 21:10},
}

@Article{Lohre-et-al-2012a,
  author               = {Lohre, Harald and Neugebauer, Ulrich and Zimmer, Carsten},
  date                 = {2012},
  journaltitle         = {The Journal of Investing},
  title                = {Diversified Risk Parity Strategies for Equity Portfolio Selection},
  doi                  = {10.3905/joi.2012.21.3.111},
  number               = {3},
  volume               = {21},
  abstract             = {This article investigates a new way of equity portfolio selection that provides maximum diversification along the uncorrelated risk sources inherent in the SandP 500.This diversified risk parity strategy is distinct from prevailing risk-based portfolio construction paradigms. Especially, the strategy is characterized by a concentrated allocation that actively adjusts to changes in the underlying risk structure. In addition, x-raying the risk and diversification characteristics of traditional risk-based strategies like 1/N, minimum-variance, risk parity, or the most-diversified portfolio, the authors find the diversified risk parity strategy to be superior. Although most of these alternatives crucially pick up risk-based pricing anomalies like the low-volatility anomaly, the diversified risk parity strategy more effectively exploits systematic factor tilts.},
  citeulike-article-id = {14322318},
  citeulike-linkout-0  = {http://www.iijournals.com/doi/abs/10.3905/joi.2012.21.3.111},
  groups               = {Risk_Budgeting, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-03-29 09:07:28},
  timestamp            = {2020-02-25 21:10},
}

@Article{Lozano-2013,
  author       = {Lozano, Martin},
  date         = {2013},
  journaltitle = {SSRN Electronic Journal},
  title        = {Diversification: A Bittersweet Story},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2373738},
  abstract     = {We all have heard or even follow the proverb: Don't put all your eggs in one basket . Clever advice indeed, though it is mute about how we should distribute the eggs. Ideally, the best allocation advice is the one which minimize the number of broken eggs.

In Finance we do something similar, although we deal with wealth and assets instead of eggs and baskets. In particular, portfolio theory suggests diversification strategies aimed to reduce the overall portfolio risk by combining several assets like real state, stocks, bonds, commodities, and foreign currency, instead of concentrating in only one. In sum, we try to generate informed financial decisions within an uncertain environment.},
  groups       = {Diversified_Invest, Invest_Diversif},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2373738},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:10},
}

@Article{Martellini-Milhau-2017,
  author         = {Martellini, Lionel and Milhau, Vincent},
  date           = {2017-12-31},
  journaltitle   = {The Journal of Portfolio Management},
  title          = {Proverbial Baskets Are Uncorrelated Risk Factors! A Factor-Based Framework for Measuring and Managing Diversification in Multi-Asset Investment Solutions},
  doi            = {10.3905/jpm.2018.44.2.008},
  url            = {https://jpm.pm-research.com/content/44/2/8},
  abstract       = {Multi-asset investment solutions have become increasingly popular among sophisticated institutional investors focusing on efficient harvesting of risk premia across and within asset classes. One key challenge in the construction of diversified multi-asset portfolio strategies is that even a seemingly well-balanced allocation to many asset classes can eventually translate into a portfolio with a very concentrated set of underlying risk exposures. The authors suggest using a factor-based framework to more effectively measure and manage diversification in multi-asset portfolios.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Risk, Effective_Dim_Diversif, MultiFactor_Invest, Invest_Diversif},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-02-25 21:10},
}

@Article{Meucci-et-al-2014,
  author       = {Meucci, A. and Santangelo, A. and Deguest, R.},
  date         = {2014},
  journaltitle = {SSRN Electronic Journal},
  title        = {Measuring portfolio diversification based on optimized uncorrelated factors},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2276632},
  abstract     = {We measure diversification in terms of the Effective Number of Minimum-Torsion Bets , namely a set of uncorrelated factors, optimized to closely track the factors used to allocate the portfolio. This way we introduce a novel notion of absolute risk contributions , which generalizes the marginal contributions to risk in traditional risk parity. We discuss the advantages of the Minimum-Torsion Bets over the traditional approach to diversification based on marginal contributions to risk.

We present a case study in the SandP 500.},
  groups       = {Diversified_Invest, Effective_Dim_Diversif, PortfOptim_Factor},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2276632},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:10},
}

@Article{Meucci-et-al-2015a,
  author         = {Meucci, Attilio and Santangelo, Alberto and Deguest, Romain},
  date           = {2015},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Risk Budgeting and Diversification Based on Optimized Uncorrelated Factors},
  doi            = {10.2139/ssrn.2276632},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=2276632},
  abstract       = {We measure the contributions to risk of a set of factors, strategies, or investments, based on "Minimum-Torsion Bets", namely a set of uncorrelated factors, optimized to closely track the factors used to allocate the portfolio. We then introduce a novel definition of contributions to risk, which generalizes the "marginal contributions to risk", traditionally used in banks for risk budgeting and in asset management to build risk parity strategies. The Minimum-Torsion Bets allow us to also introduce a natural diversification score, the Effective Number of Minimum-Torsion Bets, which we use to measure and manage diversification. We discuss the advantages of the Minimum-Torsion Bets over the traditional approach to diversification based on marginal contributions to risk. We present two case studies, a security-based investment in the stocks of the SandP 500, and a factor-based investment in the five Fama-French factors.},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Effective_Dim_Diversif, PortfOptim_Factor},
  timestamp      = {2020-02-25 21:10},
}

@Article{Miebs-2012,
  author       = {Miebs, Felix},
  date         = {2012},
  journaltitle = {SSRN Electronic Journal},
  title        = {Diversifying Diversification Strategies: Model Averaging in Portfolio Optimization},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2011969},
  abstract     = {The literature on portfolio optimization in the presence of parameter uncertainty has suggested several approaches to mitigate the impact of estimation error on portfolio performance. However, empirical evidence finds no single approach that can achieve a consistently higher risk-adjusted performance than 1/N. In this paper, I propose three averaging rules that synthesize the established approaches in order to mitigate the impact of estimation error on portfolio performance.

The evaluation of the proposed averaging rules on empirical and simulated datasets shows that each rule achieves a consistently higher risk-adjusted performance than 1/N, while all individual portfolio strategies considered in the averaging exercise do not. I find that the observed performance gains are economically and statistically significant. The performance gains are attributable to persistent diversification effects between the portfolio strategies under consideration, as well as to empirical characteristics in portfolio returns that are exploited by one of the averaging rules.},
  groups       = {Diversified_Invest, Invest_Diversif},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2011969},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:10},
}

@Article{Murtazashvili-Vozlyublennaia-2013,
  author               = {Murtazashvili, Irina and Vozlyublennaia, Nadia},
  date                 = {2013-06},
  journaltitle         = {Journal of Financial Research},
  title                = {Diversification strategies: do limited data constrain investors?},
  doi                  = {10.1111/j.1475-6803.2013.12008.x},
  issn                 = {0270-2592},
  number               = {2},
  pages                = {215--232},
  volume               = {36},
  abstract             = {We demonstrate that the mean-variance optimal portfolio does not outperform (out of sample) the naive 1/N diversification strategy even if securities are grouped into indexes or broad asset classes. This finding is due to insufficient data on past returns, which limit investors' ability to accurately estimate the means and covariance structure of securities. The resulting high estimation errors eliminate the benefits of using the means and covariance matrix as compared to the naive strategy in portfolio optimization. Using value-weighted indexes, characteristic-sorted portfolios, or portfolios defined by principal components as underlying assets in mean-variance optimization does not help. At the same time, increasing data frequency or adding data on past earnings may in some cases make mean-variance optimization useful.},
  citeulike-article-id = {14514968},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/j.1475-6803.2013.12008.x},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2018-01-10 23:39:21},
  timestamp            = {2020-02-25 21:10},
}

@Article{Nystrup-et-al-2018a,
  author               = {Nystrup, Peter and Hansen, Bo W. and Larsen, Henrik O. and Madsen, Henrik and Lindstrom, Erik},
  date                 = {2018-12-22},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Dynamic Allocation or Diversification: A Regime-Based Approach to Multiple Assets},
  doi                  = {10.3905/jpm.2018.44.2.062},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {62--73},
  volume               = {44},
  abstract             = {This article investigates whether regime-based asset allocation can effectively respond to changes in financial regimes at the portfolio level in an effort to provide better long-term results when compared to a static 60/40 benchmark. The potential benefit from taking large positions in a few assets at a time comes at the cost of reduced diversification. The authors analyze this trade-off in a multi-asset universe with great potential for static diversification. The regime-based approach is centered around a regime-switching model with time-varying parameters that can match financial markets' behavior and a new, more intuitive way of inferring the hidden market regimes. The empirical results show that regime-based asset allocation is profitable, even when compared to a diversified benchmark portfolio. The results are robust because they are based on available market data with no assumptions about forecasting skills.},
  citeulike-article-id = {14510367},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2018.44.2.062},
  day                  = {22},
  groups               = {BenchmarkInvest, Diversified_Invest, Regime_Invest, Invest_Dynamic, Invest_Regime, FrcstQWIM_MedLngTerm, Invest_Diversif},
  posted-at            = {2017-12-30 13:06:32},
  timestamp            = {2020-02-25 21:10},
}

@Article{Kashyap-2017a,
  author               = {Kashyap, Ravi},
  date                 = {2017-12-01},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Combining Dimension Reduction, Distance Measures and Covariance},
  eprint               = {1603.09060},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1603.09060},
  abstract             = {We develop a novel methodology based on the marriage between the Bhattacharyya distance, a measure of similarity across distributions of random variables, and the Johnson Lindenstrauss Lemma, a technique for dimension reduction. The resulting technique is a simple yet powerful tool that allows comparisons between data-sets representing any two distributions. The degree to which different entities, (markets, groups of securities, etc.), have different measures of their corresponding distributions tells us the extent to which they are different, aiding participants looking for diversification or looking for more of the same thing. We demonstrate a relationship between covariance and distance measures based on a generic extension of Stein's Lemma. We consider an asset pricing application and then briefly discuss how this methodology lends itself to numerous marketstructure studies and even applications outside the realm of finance / social sciences by illustrating a biological application.},
  citeulike-article-id = {14510843},
  citeulike-linkout-0  = {http://arxiv.org/abs/1603.09060},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1603.09060},
  day                  = {1},
  groups               = {Diversification_Measure, Dimens_Reduc},
  posted-at            = {2018-01-02 01:13:37},
  timestamp            = {2020-02-25 21:10},
}

@Article{Lee-2011,
  author       = {Lee, W.},
  date         = {2011},
  journaltitle = {The Journal of Portfolio Management},
  title        = {Risk-Based Asset Allocation: A New Answer to an Old Question?},
  doi          = {10.3905/jpm.2011.37.4.011},
  number       = {4},
  pages        = {11--28},
  url          = {https://jpm.pm-research.com/content/37/4/11},
  volume       = {37},
  abstract     = {In recent years, we have witnessed an alarmingly large and growing amount of literature on portfolio construction approaches focused on risks and diversification rather than on estimating expected returns. Numerous simulations applied to different universes have been documented in support of these approaches based on their apparent outperformance versus passive market capitalization-weighted or static fixed-weight portfolios. Many studies attribute the better performance of these risk-based asset allocation approaches to superior diversification.

Given the absence of clearly defined investment objective functions behind these approaches as well as the metrics used by these studies to evaluate ex post performance, Lee puts these approaches into the same context of mean-variance efficiency in an attempt to understand their theoretical underpinnings. In doing so, he hopes to shed some light on what these approaches attempt to achieve and on the characteristics of the investment universe, if indeed these approaches are meant to approximate mean-variance efficiency. Rather than adding to the already large collection of simulation results, Lee uses some simple examples to compare and contrast the portfolio and risk characteristics of these approaches. He also reiterates that any portfolio which deviates from the market capitalization-weighted portfolio is an active portfolio.

He concludes that there is no theory to predict, ex ante, that any of these risk-based approaches should outperform.},
  groups       = {Diversification_Measure, ExAnte_ExPost},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:10},
}

@Article{Mignacca-2018,
  author         = {Mignacca, Domenico},
  date           = {2018-05-03},
  journaltitle   = {SSRN Electronic Journal},
  title          = {A New Measure of Diversification: The M-DiX},
  url            = {https://ssrn.com/abstract=3172722},
  abstract       = {Diversification is a core concept in Asset Management. Yet diversification can mean different things to different people and there no consensus on how it is measured nor is there a broadly accepted metric for reporting of diversification. Sometimes, there is confusion in understanding diversification and how it differs from hedging. We may say that diversification and hedging both have the same objective i.e. reducing the risk of a portfolio, but diversification is obtained using correlated (in absolute value) securities, while hedging is achieved with correlated securities. In this paper we propose a new index to measure the diversification of a portfolio. Specifically, we outline a two-dimensional risk decomposition that we use to calculate our diversification index: the M-DiX.},
  day            = {3},
  f1000-projects = {QuantInvest},
  groups         = {Diversification_Measure, Invest_Diversif},
  timestamp      = {2020-02-25 21:10},
}

@Article{Oyenubi-2016,
  author               = {Oyenubi, Adeola},
  date                 = {2016},
  journaltitle         = {Computational Economics},
  title                = {Diversification Measures and the Optimal Number of Stocks in a Portfolio: An Information Theoretic Explanation},
  doi                  = {10.1007/s10614-016-9600-5},
  pages                = {1--29},
  abstract             = {This paper provides a plausible explanation for why the optimum number of stocks in a portfolio is elusive, and suggests a way to determine this optimal number. Diversification has a lot to do with the number of stocks in a portfolio. Adding stocks to a portfolio increases the level of diversification, and consequently leads to risk reduction up to a certain number of stocks, beyond which additional stocks are of no benefit, in terms of risk reduction. To explain this phenomenon, this paper investigates the relationship between portfolio diversification and concentration using a genetic algorithm. To quantify diversification, we use the portfolio Diversification Index (PDI). In the case of concentration, we introduce a new quantification method. Concentration is quantified as complexity of the correlation matrix. The proposed method quantifies the level of dependency (or redundancy) between stocks in a portfolio. By contrasting the two methods it is shown that the optimal number of stocks that optimizes diversification depends on both number of stocks and average correlation. Our result shows that, for a given universe, there is a set of Pareto optimal portfolios containing a different number of stocks that simultaneously maximizes diversification and minimizes concentration. The choice portfolio among the Pareto set will depend on the preference of the investor. Our result also suggests that an ideal condition for the optimal number of stocks is when variance reduction benefit of diversification is off-set by the variance contribution of complexity.},
  citeulike-article-id = {14398652},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10614-016-9600-5},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10614-016-9600-5},
  groups               = {Diversification_Measure, Invest_Diversif},
  posted-at            = {2017-07-23 16:05:16},
  publisher            = {Springer US},
  timestamp            = {2020-02-25 21:10},
}

@Article{Raffinot-2016,
  author               = {Raffinot, Thomas},
  date                 = {2016-09},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Hierarchical Clustering Based Asset Allocation},
  url                  = {https://ssrn.com/abstract=2840729},
  abstract             = {Building upon the fundamental notion of hierarchy, Lopez de Prado (2016a) introduces a new portfolio diversification technique called "Hierarchical Risk Parity", which uses graph theory and machine learning techniques. Exploiting the same basic idea, a hierarchical clustering based asset allocation method is proposed. Classical and more modern hierarchical clustering methods are tested, such as Simple Linkage or Directed Bubble Hierarchical Tree for example. A simple and efficient capital allocation within and across clusters of assets at multiple hierarchical levels is computed. The out-of-sample performances of hierarchical clustering based portfolios and more traditional risk-based portfolios are evaluated across three disparate datasets. To avoid data snooping, the comparison of profit measures is assessed using the bootstrap based model confidence set procedure (Hansen et al. (2011)). The empirical results indicate that hierarchical clustering based portfolios are robust, truly diversified and achieve statistically better risk-adjusted performances than commonly used portfolio optimization techniques.},
  citeulike-article-id = {14146762},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2840729},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2840729code2270025.pdf?abstractid=2840729 and mirid=1},
  day                  = {20},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest},
  owner                = {cristi},
  posted-at            = {2016-09-26 22:30:25},
  timestamp            = {2020-02-25 21:14},
}

@Article{Ren-et-al-2016,
  author               = {Ren, Fei and Lu, Ya-Nan and Li, Sai-Ping and Jiang, Xiong-Fei and Zhong, Li-Xin and Qiu, Tian},
  date                 = {2016-08},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Dynamic portfolio strategy using clustering approach},
  eprint               = {1608.03058},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1608.03058},
  abstract             = {The problem of portfolio optimization is one of the most important issues in asset management. This paper proposes a new dynamic portfolio strategy based on the time-varying structures of MST networks in Chinese stock markets, where the market condition is further considered when using the optimal portfolios for investment. A portfolio strategy comprises two stages: selecting the portfolios by choosing central and peripheral stocks in the selection horizon using five topological parameters, i.e., degree, betweenness centrality, distance on degree criterion, distance on correlation criterion and distance on distance criterion, then using the portfolios for investment in the investment horizon. The optimal portfolio is chosen by comparing central and peripheral portfolios under different combinations of market conditions in the selection and investment horizons. Market conditions in our paper are identified by the ratios of the number of trading days with rising index or the sum of the amplitudes of the trading days with rising index to the total number of trading days. We find that central portfolios outperform peripheral portfolios when the market is under a drawup condition, or when the market is stable or drawup in the selection horizon and is under a stable condition in the investment horizon. We also find that the peripheral portfolios gain more than central portfolios when the market is stable in the selection horizon and is drawdown in the investment horizon. Empirical tests are carried out based on the optimal portfolio strategy. Among all the possible optimal portfolio strategy based on different parameters to select portfolios and different criteria to identify market conditions, 65dollar; of our optimal portfolio strategies outperform the random strategy for the Shanghai A-Share market and the proportion is 70dollar; for the Shenzhen A-Share market.},
  citeulike-article-id = {14148628},
  citeulike-linkout-0  = {http://arxiv.org/abs/1608.03058},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1608.03058},
  day                  = {10},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest, Invest_Dynamic, Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-09-28 20:56:42},
  timestamp            = {2020-02-25 21:14},
}

@InCollection{Wang-et-al-2017,
  author               = {Wang, Hao and Pappada, Roberta and Durante, Fabrizio and Foscolo, Enrico},
  booktitle            = {Soft Methods for Data Science},
  date                 = {2017},
  title                = {A Portfolio Diversification Strategy via Tail Dependence Clustering},
  doi                  = {10.1007/978-3-319-42972-4\_63},
  editor               = {Ferraro, Maria B. and Giordani, Paolo and Vantaggi, Barbara and Gagolewski, Marek and Angeles Gil, Mara and Grzegorzewski, Przemyslaw and Hryniewicz, Olgierd},
  pages                = {511--518},
  publisher            = {Springer International Publishing},
  series               = {Advances in Intelligent Systems and Computing},
  volume               = {456},
  abstract             = {We provide a two-stage portfolio selection procedure in order to increase the diversification benefits in a bear market. By exploiting tail dependence-based risky measures, a cluster analysis is carried out for discerning between assets with the same performance in risky scenarios. Then, the portfolio composition is determined by fixing a number of assets and by selecting only one item from each cluster. Empirical calculations on the EURO STOXX 50 prove that investing on selected assets in trouble periods may improve the performance of risk-averse investors.},
  citeulike-article-id = {14150080},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-42972-463},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-42972-463},
  groups               = {Networks and investment management, Clustering and network analysis, Diversification_Measure, Diversified_Invest, Network_Invest, Invest_Network, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:49:54},
  timestamp            = {2020-02-25 21:14},
}

@Article{Dose-Cincotti-2005,
  author               = {Dose, Christian and Cincotti, Silvano},
  date                 = {2005-09},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Clustering of financial time series with application to index and enhanced index tracking portfolio},
  doi                  = {10.1016/j.physa.2005.02.078},
  issn                 = {0378-4371},
  number               = {1},
  pages                = {145--151},
  volume               = {355},
  abstract             = {A stochastic-optimization technique based on time series cluster analysis is described for index tracking and enhanced index tracking problems. Our methodology solves the problem in two steps, i.e., by first selecting a subset of stocks and then setting the weight of each stock as a result of an optimization process (asset allocation). Present formulation takes into account constraints on the number of stocks and on the fraction of capital invested in each of them, whilst not including transaction costs. Computational results based on clustering selection are compared to those of random techniques and show the importance of clustering in noise reduction and robust forecasting applications, in particular for enhanced index tracking.},
  citeulike-article-id = {2251197},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2005.02.078},
  citeulike-linkout-1  = {http://www.sciencedirect.com/science/article/B6TVG-4G33NP0-4/2/33fd4c2a5caa0ce508e18151530c9250},
  day                  = {1},
  groups               = {Networks and investment management, Network_Invest, PortfOptim_Network, Invest_Network},
  posted-at            = {2017-03-26 18:22:57},
  timestamp            = {2020-02-25 21:14},
}

@MastersThesis{Fucik-2017,
  author               = {Fucik, Vojtech},
  date                 = {2017},
  institution          = {Charles University},
  title                = {Portfolio Construction Using Hierarchical Clustering},
  url                  = {https://dspace.cuni.cz/handle/20.500.11956/91113?locale-attribute=en},
  abstract             = {The main objective of this thesis is to summarize and mainly interconnect the existing methodology on correlation matrix filtering, graph algorithms utilized in the minimum spanning trees, hierarchical clustering and principal components analysis in order to create quantitative investment strategies. Instead of traditional usage of stocks returns series, factor models residuals are utilized. Residuals are then an ultimate input for all the algorithms to arrive at probability of centrality (PoC) - an impure probability where values near 1 signalize high probability of a stock being central in the network. Several investment strategies are created based on PoC and tested on data from major US stock market indices. It cannot be imperatively argued that peripheralbased strategies are always better than central-based strategies. Both central and peripheral-based strategies share high upside profit potential at the cost of high volatility whereas traditional Markowitz's optimization process yields stable profits with moderate upside potential.},
  citeulike-article-id = {14461303},
  groups               = {Network_Invest, PortfOptim_Network, Invest_Network, Vol_Cluster},
  posted-at            = {2017-10-19 20:42:17},
  timestamp            = {2020-02-25 21:14},
}

@Article{Page-Panariello-2018,
  author         = {Page, Sebastien and Panariello, Robert A.},
  date           = {2018-08},
  journaltitle   = {Financial Analysts Journal},
  title          = {When Diversification Fails},
  doi            = {10.2469/faj.v74.n3.3},
  issn           = {0015-{198X}},
  number         = {3},
  pages          = {19--32},
  volume         = {74},
  abstract       = {One of the most vexing problems in investment management is that diversification seems to disappear when investors need it the most. We surmise that many investors still do not fully appreciate the impact of extreme correlations on portfolio efficiency particular, on exposure to loss. We take an in-depth look at what drives the stock-to-credit, stock-to-hedge fund, stock-to-private asset, stock-to-risk factors, and stock-to-bond correlations during tail events. We introduce a data-augmentation technique to improve the robustness of tail correlation estimates. Finally, we discuss implications for multi-asset investing.},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:15},
}

@Article{Pappas-et-al-2014,
  author       = {Pappas, Scott N. and Bianchi, Robert J. and Drew, Michael E. and Gupta, Rakesh},
  date         = {2012},
  journaltitle = {SSRN Electronic Journal},
  title        = {Risk-Factor Diversification and Portfolio Selection},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2136827},
  abstract     = {Traditionally, investment portfolios have been constructed with a focus on what asset classes to invest in and how much to invest in each. Recent research, however, has shown that focusing on risk-factor allocations, rather than asset class allocations, can result in better risk-adjusted portfolio performance. The existing literature has focused on simple allocation strategies such as equal-weighted and equal-risk-weighted portfolios.

In addition to these simple allocation techniques, this paper compares the performance using mean-variance analysis, and presents evidence that the outperformance of risk-factor diversification may not be as conclusive as has been previously presented in the literature.

While confirming some of the prior findings on risk-factor diversification, the research shows that previous findings may be subject to strong caveats. Specifically, the evidence suggests that the selection of risk-factors, portfolio selection techniques and time-period have a large impact on performance outcomes.},
  groups       = {Diversified_Invest, Invest_Diversif},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2136827},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:15},
}

@MastersThesis{Parmentier-2018,
  author      = {Loic Parmentier},
  date        = {2018},
  institution = {Louvain School of Management},
  title       = {Measures of Portfolio Diversification},
  url         = {https://dial.uclouvain.be/memoire/ucl/en/object/thesis%3A14352/datastream/PDF_01/view},
  abstract    = {Diversification is one the main and most important concept in the financial world. It is often said that diversification is the only free lunch in finance. From a qualitative point of view, the concept of diversification is quite clear: a portfolio is well-diversified if shocks in the individual components do not heavily impact on the overall portfolio. Relatively simple to understand then but profoundly difficult to define. Indeed, there is no broadly accepted precise and quantitative definition of diversification.

Over the years, many different measures of diversification have been developed in the literature, each with its pros and cons. In the framework of this thesis, we have chosen to analyze six of them. Because we wanted to confront the weights concentration criterion with the risk minimization criterion, we decided to select measures that are based on the entropy of the weights and others that are based on the sources of risk. Those six different measures are the Shannon's Entropy, the Diversification Delta, the Diversification Ratio, the MarginalRisk Contributions, the Portfolio Diversification Index and the Effective Number of Bets.},
  groups      = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  timestamp   = {2020-02-25 21:15},
}

@Article{Platanakis-et-al-2017b,
  author               = {Platanakis, Emmanouil and Sakkas, Athanasios and Sutcliffe, Charles},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Harmful Diversification: Evidence from Alternative Investments},
  url                  = {https://ssrn.com/abstract=2911212},
  abstract             = {Alternative assets have become as important as equities and fixed income in the portfolios of major investors, and so their diversification properties are also important. However, adding five alternative assets (real estate, commodities, hedge funds, emerging markets and private equity) to equity and bond portfolios is shown to be harmful for US investors. We use 19 portfolio models, in conjunction with dummy variable regression, to demonstrate this harm over the 1997-2015 period. This finding is robust to different estimation periods, risk aversion levels, and the use of two regimes. Harmful diversification into alternatives is not primarily due to transactions costs or non-normality, but to estimation risk. This is larger for alternative assets, particularly during the credit crisis which accounts for the harmful diversification of real estate, private equity and emerging markets. Diversification into commodities, and to a lesser extent hedge funds, remains harmful even when the credit crisis is excluded.},
  citeulike-article-id = {14510389},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2911212},
  groups               = {Private_Equity, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-12-30 14:25:45},
  timestamp            = {2020-02-25 21:15},
}

@Article{Pola-2016,
  author               = {Pola, Gianni},
  date                 = {2016-04},
  journaltitle         = {Journal of Asset Management},
  title                = {On entropy and portfolio diversification},
  doi                  = {10.1057/jam.2016.10},
  issn                 = {1470-8272},
  abstract             = {Entropy, a term used in Physics to quantify the degree of randomness in a complex system, is shown to be relevant for portfolio diversification. The link between entropy and diversification lies in the notion of uncertainty. We introduce the concept of available diversification in an investment universe and of diversification curves. We build a framework for assembling a fully diversified risk parity-like portfolio with a fundamental-based high-conviction strategy, through a constrained entropy-maximisation process by which a portion of potential portfolio return is swapped for extra diversification. The main results of this study are: mean-variance optimised portfolios are highly concentrated and scarcely related to the asset return assumptions; few basis points of expected returns can be converted into a huge amount of extra diversification that making the portfolio allocation more robust to parameter uncertainty; on a more conceptual ground, we investigate the relationship between portfolio risk and diversification concluding that they should be managed distinctly. The empirical analysis presented in this work shows that entropy is a useful means to alleviate the lack of diversification of portfolios on the efficient frontier.},
  citeulike-article-id = {14030186},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2016.10},
  day                  = {07},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-05-08 22:55:09},
  timestamp            = {2020-02-25 21:15},
}

@Article{Polbennikov-et-al-2010,
  author               = {Polbennikov, Simon and Desclee, Albert and Hyman, Jay},
  date                 = {2010-01},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Horizon Diversification: Reducing Risk in a Portfolio of Active Strategies},
  doi                  = {10.3905/jpm.2010.36.2.026},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {26--38},
  volume               = {36},
  abstract             = {A primary mechanism for controlling portfolio risk is diversification. Diversification is typically addressed by distributing assets among investment sectors and issuers, preferably with low correlations among their returns, a process that can be called asset diversification. The risk reduction from this type of diversification can be less than expected in the midst of a crisis as correlations increase across market segments.

The authors of this article consider a new approach to managing the active risk profile of a portfolio, an approach that uses active strategies rather than asset allocations as its basic building blocks. The authors show that in this framework, risk reduction is achieved by a combination of two distinct mechanisms asset diversification and signal diversification. Combining alpha strategies based on independent signals can help reduce portfolio risk, even when the returns of the underlying assets are correlated.

One way to achieve signal diversification is by combining strategies with various investment horizons or trading frequencies a technique the authors call horizon diversification. Horizon diversification is an intuitive and robust way to decrease risk in a portfolio of active strategies.},
  citeulike-article-id = {13971833},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2010.36.2.026},
  groups               = {Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-08 04:23:53},
  timestamp            = {2020-02-25 21:15},
}

@Article{Qian-2011,
  author               = {Qian, Edward},
  date                 = {2011-02},
  journaltitle         = {The Journal of Investing},
  title                = {Risk Parity and Diversification},
  doi                  = {10.3905/joi.2011.20.1.119},
  issn                 = {1068-0896},
  number               = {1},
  pages                = {119--127},
  volume               = {20},
  abstract             = {Traditional 60/40 asset allocation portfolios are not truly diversified because they have an unbalanced risk allocation to high-risk assets. As a result, their expected risk-adjusted returns are low. Risk parity is a new way to construct asset allocation portfolios based on the principle of risk diversification, achieving both higher risk-adjusted returns and higher total returns than traditional asset allocation approaches. The diversification benefits of risk parity portfolios also include balanced correlations to underlying asset classes and stronger downside protection against severe losses. Risk parity portfolios can also incorporate active views on risk-adjusted returns of different asset classes. All of these features make risk parity an attractive alternative to traditional asset allocation approaches.},
  citeulike-article-id = {13970982},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/joi.2011.20.1.119},
  groups               = {Risk_Budgeting, Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-07 00:18:12},
  timestamp            = {2020-02-25 21:15},
}

@InCollection{Richard-Roncalli-2015,
  author               = {Richard, Jean-Charles and Roncalli, Thierry},
  booktitle            = {Risk-Based and Factor Investing},
  date                 = {2015},
  title                = {Smart Beta: Managing Diversification of Minimum Variance Portfolios},
  doi                  = {10.1016/b978-1-78548-008-9.50002-2},
  isbn                 = {9781785480089},
  pages                = {31--63},
  publisher            = {Elsevier},
  abstract             = {In this chapter, we consider a new framework for understanding risk-based portfolios (global minimum variance (GMV), equally weighted (EW), equal risk contribution (ERC) and most diversified portfolio (MDP)). This framework is similar to the constrained minimum variance model of Jurczenko et al., but with another definition of the diversification constraint. The corresponding optimization problem can then be solved using the cyclical coordinate descent (CCD) algorithm. This allows us to extend the results of Cazalet et al. and to better understand the trade-off relationships between volatility reduction, tracking error and risk diversification. In particular, we show that the smart beta portfolios differ because they implicitly target different levels of volatility reduction.

We also develop new smart beta strategies by managing the level of volatility reduction and show that they present appealing properties compared to the traditional risk-based portfolios.},
  citeulike-article-id = {13978525},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/b978-1-78548-008-9.50002-2},
  groups               = {Invest_SmartBeta, Diversified_Invest},
  owner                = {cristi},
  posted-at            = {2016-03-12 19:04:23},
  timestamp            = {2020-02-25 21:15},
}

@TechReport{Romahi-Santiago-2012,
  author      = {Romahi, Y. and Santiago, K.},
  date        = {2012},
  institution = {JP Morgan Asset Management},
  title       = {Diversification - Is it still the only Free Lunch? Alternative building blocks for risk parity portfolios},
  url         = {https://am.jpmorgan.com/blobcontent/800/973/1383169203651_11_566.pdf},
  abstract    = {Risk parity has recently garnered significant attention, particularly owing to its strong performance to more traditional approaches of asset allocation in the last decade. This paper seeks to shed some light on this framework and outline the main advantages, while highlighting some of the concerns currently at the forefront of the minds of risk parity investors - namely leveraged positions in fixed income assets at this point in the interest rate cycle as well as the increasing correlation among asset classes.

The premise of risk parity as an approach to strategic asset allocation is based on maximal diversification of beta (or risk premia) as it emphasises the balanced contribution of various risk exposures to overall portfolio risk. One should essentially remain agnostic to return forecasts on the basis that volatility is a much more stable estimate than return.

Much has been made recently of the increasing correlation among asset classes and the increasing difficulty of achieving diversification - particularly at times of crisis arising from systemic risk. A number of recent studies have examined the benefits of factor diversification over asset class diversification.

The difference is subtle because when one refers to asset classes one is also referring to compensated risk premia. These themselves are therefore factors. One can think of equities as a growth factor, Treasuries as a deflation factor and commodities as an inflation factor. However, risk premia go much further than these traditional factors, as argued in a previous J.P. Morgan Asset Management white paper on alternative beta [15].

Indeed, when one focuses on the risk premia, there are a much broader and more orthogonal set of factors of which one can take advantage. In addition to those mentioned, for example, we can also include the equity value premium, the size premium, the forward rate bias and the merger arbitrage premium among others as further risk premia.

The literature is clear that factor diversification is generally more appealing to asset class diversification. Ilmanen and Kizer [8] go further and point out that factor diversification has been more effective, particularly during periods of crisis.

In this paper, extending risk parity in this direction can be seen to address the core concerns around traditional risk parity and can offer a very attractive approach to strategic asset allocation.

In order to demonstrate this, data is included from several periods going back to 1927 and shows that 'factor premium' risk parity consistently outperforms and is stronger to 'asset class' risk parity.},
  groups      = {Diversified_Invest, Invest_Diversif},
  owner       = {Anne},
  timestamp   = {2020-02-25 21:15},
}

@Article{Sharma-Vipul-2018,
  author         = {Sharma, Prateek and Vipul, A},
  date           = {2018},
  journaltitle   = {Managerial and Decision Economics},
  title          = {Improving portfolio diversification: Identifying the right baskets for putting your eggs},
  doi            = {10.1002/mde.2939},
  abstract       = {We measure the economic value of diversification for international multiasset investment strategies. This study implements five existing diversification measures and proposes a novel measure of diversification, the unsystematic risk ratio (URR). Only the URR and the effective number of bets measures predict the future risk-adjusted performance. These relations are robust to the choice of investment horizon and degree of relative risk aversion. The diversification benefits are larger for the frontier and emerging markets than for the developed markets, for multiasset strategies than for single asset class strategies, and for the pre-crisis and post-crisis periods than for the financial crisis period.},
  f1000-projects = {QuantInvest},
  groups         = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:15},
}

@Article{Shi-2015,
  author               = {Shi, Xiang},
  date                 = {2015-08},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Marginal Contribution to Risk and Generalized Effective Number of Bets},
  url                  = {https://ssrn.com/abstract=2642408},
  abstract             = {This paper extends Meucci's Effective Number of Bets to general risk measures with heavy-tailed distributions. By diagonalizing the Hessian matrix of a risk measure we are able to extract locally independent marginal contributions to the risk. The Minimal Torsion approach can still be applied to get the local coordinators of the marginal contributions.

We also calculated second derivatives of CVaR. Furthermore, the Hessian of CVaR can be computed efficiently when the underlying distribution belongs to a class of normal mixture distributions.},
  citeulike-article-id = {13926624},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2642408},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2642408code2345648.pdf?abstractid=2642408 and mirid=1},
  day                  = {18},
  groups               = {Diversified_Invest},
  howpublished         = {Available at SSRN: ssrn.com/abstract=2642408},
  owner                = {zkgst0c},
  posted-at            = {2016-02-06 04:51:27},
  timestamp            = {2020-02-25 21:15},
}

@Article{Staines-et-al-2016,
  author               = {Staines, Joe and Li, Wei V. and Romahi, Yazann},
  date                 = {2016-08},
  journaltitle         = {The Journal of Index Investing},
  title                = {Dimensions of Diversification},
  doi                  = {10.3905/jii.2016.7.2.119},
  issn                 = {2154-7238},
  number               = {2},
  pages                = {119--127},
  volume               = {7},
  abstract             = {Within the investment industry, diversification now refers to not only the division of capital among a large number of securities but also the avoidance of risk concentration in any of a number of dimensions. Market-capitalization-weighted indexes often fail this requirement. The authors thus argue that although capitalization weighting makes a suitable benchmark, smart beta can provide a way to build indexes more suitable for investment. The authors present a methodology to measure and hence maximize diversification simultaneously across multiple dimensions. They show the practical value of this measure by using it to backtest equity portfolios. This provides an example of how the properties of assets, rather than historical returns, can be used to systematically construct well-diversified portfolios.},
  citeulike-article-id = {14150143},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jii.2016.7.2.119},
  groups               = {Diversified_Invest, Effective_Dim_Diversif, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-10-02 02:36:41},
  timestamp            = {2020-02-25 21:15},
}

@Article{Stein-2015a,
  author               = {Stein, Michael},
  date                 = {2015-05},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Limits to Diversification: Tail Risks in Real Estate Portfolios},
  url                  = {https://ssrn.com/abstract=2611905},
  abstract             = {This study addresses real estate's riskiness from a distributional viewpoint. Several studies have found real estate returns to be best modeled with stable paretian distributions. Using NCREIF individual property returns this is confirmed, but the first application of stable distributions to real estate portfolio returns provides evidence that diversification effects ultimately reduce the tailedness and surprisingly drive the tail parameter towards normality. The study further contributes to the literature by showing the importance of a complete view, beyond pure tail parameter considerations. Even in the presence of tail parameters being close to normal, the return risk may still be tremendous, and can only be reduced by diversification effects in property portfolios, and only to a certain time-dependent extent. The results have strong implications for risk managers, fund managers and holders of large commercial real estate portfolios alike.},
  citeulike-article-id = {13997240},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2611905},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2691024code1091028.pdf?abstractid=2611905 and mirid=1},
  day                  = {30},
  groups               = {Diversified_Invest, Invest_TailRisk, Invest_RealEstate, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-04-04 22:56:41},
  timestamp            = {2020-02-25 21:15},
}

@InCollection{Takada-Stern-2017,
  author               = {Takada, Hellinton H. and Stern, Julio M.},
  booktitle            = {Publisher Logo Conference Proceedings},
  date                 = {2017},
  title                = {On portfolio risk diversification},
  doi                  = {10.1063/1.4985363},
  location             = {Ghent, Belgium},
  pages                = {070002+},
  url                  = {https://aip.scitation.org/doi/10.1063/1.4985363},
  abstract             = {The first portfolio risk diversification strategy was put into practice by the All Weather fund in 1996. The idea of risk diversification is related to the risk contribution of each available asset class or investment factor to the total portfolio risk. The maximum diversification or the risk parity allocation is achieved when the set of risk contributions is given by a uniform distribution. Meucci (2009) introduced the maximization of the Renyi entropy as part of a leverage constrained optimization problem to achieve such diversified risk contributions when dealing with uncorrelated investment factors. A generalization of the risk parity is the risk budgeting when there is a prior for the distribution of the risk contributions. Our contribution is the generalization of the existent optimization frameworks to be able to solve the risk budgeting problem. In addition, our framework does not possess any leverage constraint.},
  citeulike-article-id = {14520884},
  citeulike-linkout-0  = {http://dx.doi.org/10.1063/1.4985363},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2018-01-22 01:13:17},
  timestamp            = {2020-02-25 21:15},
}

@Article{Theron-vanVuuren-2018,
  author         = {Theron, Ludan and van Vuuren, Gary},
  date           = {2018-01-18},
  journaltitle   = {Cogent Economics \& Finance},
  title          = {The maximum diversification investment strategy: A portfolio performance comparison},
  doi            = {10.1080/23322039.2018.1427533},
  issn           = {2332-2039},
  number         = {1},
  volume         = {6},
  abstract       = {The efficacy of four different portfolio allocation strategies is evaluated according to their absolute returns during different economic conditions over a period of 10 years. A comparison is drawn between the Most Diversified portfolio (MD) and three alternatives; a Minimum Variance portfolio, an Equally-Weighted portfolio and a Tangent (or Maximum Sharpe ratio) portfolio. The aim is to assess portfolio performance using cumulative returns, the Sharpe ratio and the daily volatilities of each portfolio. The four asset allocation methods are governed by multiple constraints. Although previous work has shown that MD portfolios exhibit greater diversification and a higher Sharpe ratio than other investment strategies, this was not found using developed market index data.},
  day            = {18},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:15},
}

@Article{Vermorken-et-al-2012,
  author               = {Vermorken, Maximilian A. and Medda, Francesca R. and Schroder, Thomas},
  date                 = {2012-10},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {The Diversification Delta: A Higher-MomentMeasure for Portfolio Diversification},
  doi                  = {10.3905/jpm.2012.39.1.067},
  issn                 = {0095-4918},
  number               = {1},
  pages                = {67--74},
  volume               = {39},
  abstract             = {The concept of diversification is central in finance and has become even more so since the 2008 financial crisis.

In this article, the authors introduce a new measure for diversification. The measure, referred to as diversification delta, is nonparametric, based on higher moments, easily interpretable due to its mathematical formulation, and incorporates the advantages of the present measures of diversification while extending them.

The measure is applied to infrastructure returns data in order to understand the benefits of diversifying across various infrastructure classes, gaining useful insights for infrastructure fund managers and investors.},
  citeulike-article-id = {13972052},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2012.39.1.067},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-08 06:20:54},
  timestamp            = {2020-02-25 21:15},
}

@Article{Viceira-et-al-2017,
  author               = {Viceira, Luis M. and Wang, Zixuan and Zhou, John},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Global Portfolio Diversification for Long-Horizon Investors},
  url                  = {https://ssrn.com/abstract=2941652},
  abstract             = {This paper conducts a theoretical and empirical investigation of the risks of globally diversified portfolios of stocks and bonds and of optimal intertemporal global portfolio choice for long horizon investors in the presence of permanent cash flow shocks and transitory discount rate shocks to asset values. We show that an upward shift in cross-country one-period return correlations resulting from correlated cash flow shocks increases the risk of global portfolios and reduces investors' willingness to hold risky assets at all horizons. However, a similar upward shift in cross-country one-period return correlations resulting from correlated discount rate shocks has a much more muted effect on long-run portfolio risk and on the willingness to long horizon investors to hold risky assets. Correlated cash flow shocks imply that markets tend to move together at all horizons, thus reducing the scope for global diversification for all investors regardless of their investment horizon. By contrast, correlated discount rate shocks imply that markets tend to move together only transitorily and long-horizon investors can still benefit from global portfolios to diversify long-term cash flow risk. We document a secular increase in the cross-country correlations of stock and government bond returns since the late 1990's. We show that for global equities this increase has been driven primarily by increased cross-country correlations of discount rate shocks, or global capital markets integration, while for bonds it has been driven by both global capital markets integration and increased cross-country correlations of inflation shocks that determine the real cash flows of nominal government bonds. Therefore, despite the significant increase in the short-run correlation of global equity markets, the benefits from global equity portfolio diversification have not declined nearly as much for long-horizon investors as they have for short-horizon investors. By contrast, increased correlation of inflation across markets implies that the benefits of global bond portfolio diversification have declined for long-only bond investors at all horizons. However, it also means that the scope for hedging liabilities using global bonds has increased, benefiting investors with long-dated liabilities. Finally, we show that the well documented negative stock-bond correlation in the U.S. since the late 1990's is a global phenomenon, suggesting that the benefits of stock-bond diversification have increased in all developed markets.},
  citeulike-article-id = {14327821},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2941652},
  groups               = {Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-04-03 19:26:42},
  timestamp            = {2020-02-25 21:15},
}

@Article{Pastor-et-al-2017,
  author               = {Pastor, Lubos and Stambaugh, Robert F. and Taylor, Lucian A.},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Portfolio Liquidity and Diversification: Theory and Evidence},
  url                  = {https://ssrn.com/abstract=3016781},
  abstract             = {A portfolio's liquidity depends not only on the liquidity of its holdings but also on its diversification. We propose simple, theoretically motivated measures of portfolio liquidity and diversification. We also develop an equilibrium model relating portfolio liquidity to fund size, expense ratio, and turnover. As the model predicts, mutual funds with less liquid portfolios have smaller size, higher expense ratios, and lower turnover. The model also yields additional predictions that we verify empirically: larger funds are cheaper, funds that trade less are larger and cheaper, and funds that are too big perform worse. We also find that mutual fund portfolios have become more liquid because both components of diversification, coverage and balance, have trended upward.},
  citeulike-article-id = {14428169},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3016781},
  groups               = {Characteristics and return prediction, Diversification_Measure, Invest_Liquidity, Invest_Diversif},
  posted-at            = {2017-09-09 17:49:42},
  timestamp            = {2020-02-25 21:16},
}

@Article{Rudin-Morgan-2006,
  author               = {Rudin, Alexander M. and Morgan, Jonathan S.},
  date                 = {2006-01},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {A Portfolio Diversification Index},
  doi                  = {10.3905/jpm.2006.611807},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {81--89},
  volume               = {32},
  abstract             = {Despite the importance of diversification in portfolio construction, our current methods of measuring it are inefficient. Construction of a Portfolio Diversification Index (PDI) presents a new way to understand the concept. PDI, which measures the number of unique investments in a portfolio, is useful to assess marginal and cumulative diversification benefits across asset classes and across time. Implementation in hedge fund strategies reveals that various hedge funds offer less diversification than may have been thought, and that there has been reduced diversification in the past several years},
  citeulike-article-id = {14337565},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2006.611807},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-04-15 04:50:36},
  timestamp            = {2020-02-25 21:16},
}

@Article{Yu-2014,
  author               = {Yu, Jing-Rung and Lee, Wen-Yi and Chiou, Wan-Jiun P.},
  date                 = {2014-08},
  journaltitle         = {Applied Mathematics and Computation},
  title                = {Diversified portfolios with different entropy measures},
  doi                  = {10.1016/j.amc.2014.04.006},
  issn                 = {0096-3003},
  pages                = {47--63},
  volume               = {241},
  abstract             = {One of the major issues for Markowitz mean-variance model is the errors in estimations cause "corner solutions"and low diversity in the portfolio. In this paper, we compare the mean-variance efficiency, realized portfolio values, and diversity of the models incorporating different entropy measures by applying multiple criteria method. Differing from previous studies, we evaluate twenty-three portfolio over-time rebalancing strategies with considering short-sales and various transaction costs in asset diversification. Using the data of the most liquid stocks in Taiwan's market, our finding shows that the models with Yager's entropy yield higher performance because they respond to the change in market by reallocating assets more effectively than those with Shannon's entropy and with the minimax disparity model. Furthermore, including entropy in models enhances diversity of the portfolios and makes asset allocation more feasible than the models without incorporating entropy.},
  citeulike-article-id = {14310470},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.amc.2014.04.006},
  groups               = {Diversification_Measure, Diversified_Invest, Invest_Diversif},
  posted-at            = {2017-03-14 02:52:48},
  timestamp            = {2020-02-25 21:16},
}

@Article{Carmichael-et-al-2017,
  author               = {Carmichael, Benoit and Koumou, Gilles B. and Moran, Kevin},
  date                 = {2017-11-24},
  journaltitle         = {Quantitative Finance},
  title                = {Rao's quadratic entropy and maximum diversification indexation},
  doi                  = {10.1080/14697688.2017.1383625},
  pages                = {1--15},
  abstract             = {This paper proposes a new formulation of the maximum diversification indexation strategy based on Rao's Quadratic Entropy. It clarifies the investment problem underlying this diversification strategy, identifies the source of its out-of-sample performance, and suggests new dimensions along which this performance can be improved. We show that these potential improvements are quantitatively important and are robust to portfolio turnover, portfolio risk, estimation window, and covariance matrix estimation.},
  citeulike-article-id = {14486031},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2017.1383625},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2017.1383625},
  day                  = {24},
  groups               = {Invest_Diversif},
  posted-at            = {2017-11-29 23:50:51},
  publisher            = {Routledge},
  timestamp            = {2020-02-25 21:25},
}

@Article{Amenc-et-al-2017,
  author               = {Amenc, Noel and Ducoulombier, Frederic and Esakia, Mikheil and Goltz, Felix and Sivasubramanian, Sivagaminathan},
  date                 = {2017-03},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Accounting for Cross-Factor Interactions in Multifactor Portfolios without Sacrificing Diversification and Risk Control},
  doi                  = {10.3905/jpm.2017.43.5.099},
  issn                 = {0095-4918},
  number               = {5},
  pages                = {99--114},
  volume               = {43},
  citeulike-article-id = {14324691},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2017.43.5.099},
  groups               = {Factor_Types, Multifactor_Invest, Invest_Diversif},
  posted-at            = {2017-03-30 14:36:32},
  timestamp            = {2020-02-25 21:26},
}

@Article{Choueifaty-et-al-2013,
  author       = {Yves Choueifaty and Tristan Froidure and Julien Reynier},
  date         = {2013},
  journaltitle = {The Journal of Investment Strategies},
  title        = {Properties of the most diversified portfolio},
  number       = {2},
  pages        = {119--131},
  url          = {https://www.risk.net/journal-of-investment-strategies/2255764/properties-of-the-most-diversified-portfolio},
  volume       = {1},
  abstract     = {This article expands upon Toward Maximum Diversification by Choueifaty and Coignard [2008]. We present new mathematical properties of the Diversification Ratio and Most Diversified Portfolio (MDP), and investigate the optimality of the MDP in a mean-variance framework. We also introduce a set of Portfolio Invariance Properties, providing the basic rules an unbiased portfolio construction process should respect.

The MDP is then compared in light of these rules to popular methodologies (equal weights, equal risk contribution, minimum variance), and their performance is investigated over the past decade, using the MSCI World as reference universe. We believe that the results obtained in this article show that the MDP is a strong candidate for being the un-diversifiable portfolio, and as such delivers investors with the full benefit of the equity premium.},
  groups       = {Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:29},
}

@Article{Heckel-et-al-2019,
  author       = {Heckel, Thomas and Amghar, Zine and Haik, Isaac and Laplenie, Olivier and de Carvalho, Raul Leote},
  date         = {2019},
  journaltitle = {The Journal of Fixed Income},
  title        = {Factor Investing in Corporate Bond Markets: Enhancing Efficacy Through Diversification and Purification!},
  url          = {https://jfi.pm-research.com/content/early/2019/10/03/jfi.2019.1.074},
  abstract     = {We show that factors from value, quality, low risk, and momentum styles play an important role in explaining the cross-section of corporate bond expected returns for the US and Euro Investment Grade and US BB-B Nonfinancial High Yield universes. We demonstrate the importance of purifying factor data by neutralizing a number of risk biases that are present in the factors: controlling for sectors, option-adjusted spread (OAS), duration, and size biases significantly increase the predictive power of style factors. We propose a new simple approach for efficiently neutralizing the biases from multiple risk variables and demonstrate its superiority relative to stratified sampling and optimization as alternative control methods. We also measure the added value from diversifying the number of factors in each style. Finally, we show that the results are robust in relation to transaction costs and can be used to design strategies that aim at outperforming traditional benchmark indexes.},
  groups       = {Invest_Diversif},
  timestamp    = {2020-02-25 21:31},
}

@Article{Sorensen-et-al-2018,
  author         = {Sorensen, Eric and Barnes, Mark and Alonso, Nick and Qian, Edward},
  date           = {2018-03-31},
  journaltitle   = {The Journal of Portfolio Management},
  title          = {Not all factor exposures are created equal},
  doi            = {10.3905/jpm.2018.44.4.039},
  issn           = {0095-4918},
  number         = {4},
  pages          = {39--45},
  volume         = {44},
  abstract       = {Approaches to quantitative equity investing have evolved markedly. Thirty years ago, the focus was on alpha generation, but with the recent decade acceptance of smart (alternative) beta, the focus is turning to transparent methods of construction for factor investing. In this article, the authors present an approach for evaluating various methods of portfolio construction that lead to the same factor exposures. Four portfolios are of interest: factor weighted, cap weighted, equal weighted, and risk parity weighted. The authors compare these portfolios based on standard performance statistics as well as new metrics of value-added, such as performance participation rates and portfolio sector concentrations. The results indicate that once the desired factor exposure is achieved, it is beneficial to build the portfolio with the most desirable characteristics in terms of diversification.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Invest_SmartBeta},
  timestamp      = {2020-02-25 21:32},
}

@Article{Bardoscia-et-al-2019,
  author         = {Bardoscia, Marco and {d'Arienzo}, Daniele and Marsili, Matteo and Volpati, Valerio},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Lost in Diversification},
  doi            = {10.2139/ssrn.3323440},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3323440},
  abstract       = {As financial instruments grow in complexity more and more information is neglected by risk optimization practices. This brings down a curtain of opacity on the origination of risk, that has been one of the main culprits in the 2007-2008 global financial crisis. We discuss how the loss of transparency may be quantified in bits, using information theoretic concepts. We find that i) financial transformations imply large information losses, ii) portfolios are more information sensitive than individual stocks only if fundamental analysis is sufficiently informative on the co-movement of assets, that iii) securitisation, in the relevant range of parameters, yields assets that are less information sensitive than the original stocks and that iv) when diversification (or securitisation) is at its best (i.e. when assets are uncorrelated) information losses are maximal. We also address the issue of whether pricing schemes can be introduced to deal with information losses. This is relevant for the transmission of incentives to gather information on the risk origination side. Within a simple mean variance scheme, we find that market incentives are not generally sufficient to make information harvesting sustainable.},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  timestamp      = {2020-02-25 21:34},
}

@Article{Bennyhoff-2009,
  author               = {Bennyhoff, Donald G.},
  date                 = {2009-02},
  journaltitle         = {The Journal of Investing},
  title                = {Time Diversification and Horizon-Based Asset Allocations},
  doi                  = {10.3905/joi.2009.18.1.045},
  issn                 = {1068-0896},
  number               = {1},
  pages                = {45--52},
  volume               = {18},
  abstract             = {Time diversification, the concept that investments in stocks are less risky over longer periods than shorter ones, has been the subject of spirited debate for decades. Over the last few years the growing acceptance of life cycle investment products, such as target retirement mutual funds, has renewed interest in this topic. The objective of this article is not to prove or disprove time diversification, but to evaluate whether the concept must be valid for a horizon-based asset allocation framework to be viable and appropriate. Our findings suggest that there is little evidence to support the notion that time moderates the perceived volatility inherent in risky assets. However, we would expect the risk/reward relationships of the past to prevail in the future, and if that is the case, a longer investment horizon may support a willingness and ability to assume the greater uncertainty of equity-centric asset allocations. This may be true particularly for younger investors for whom the allocation to human capital and the risk posed by the erosion of purchasing power by inflation can reasonably be assumed to be greatest.},
  citeulike-article-id = {14322263},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/joi.2009.18.1.045},
  groups               = {Human_Capital, Invest_Diversif},
  posted-at            = {2017-03-29 07:02:30},
  timestamp            = {2020-02-25 21:35},
}

@Article{Dees-et-al-2019,
  author         = {Dees, Bruno Scalzo and Stankovic, Ljubisa and Constantinides, Anthony G. and Mandic, Danilo P.},
  date           = {2019-10-12},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Portfolio Cuts: A Graph-Theoretic Framework to Diversification},
  url            = {https://arxiv.org/abs/1910.05561},
  urldate        = {2019-10-24},
  abstract       = {Investment returns naturally reside on irregular domains, however, standard multivariate portfolio optimization methods are agnostic to data structure. To this end, we investigate ways for domain knowledge to be conveniently incorporated into the analysis, by means of graphs. Next, to relax the assumption of the completeness of graph topology and to equip the graph model with practically relevant physical intuition, we introduce the portfolio cut paradigm. Such a graph-theoretic portfolio partitioning technique is shown to allow the investor to devise robust and tractable asset allocation schemes, by virtue of a rigorous graph framework for considering smaller, computationally feasible, and economically meaningful clusters of assets, based on graph cuts. In turn, this makes it possible to fully utilize the asset returns covariance matrix for constructing the portfolio, even without the requirement for its inversion. The advantages of the proposed framework over traditional methods are demonstrated through numerical simulations based on real-world price data.},
  day            = {12},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-25 21:35},
}

@Article{Sebastian-Gebbie-2019,
  author         = {Sebastian, Ann and Gebbie, Tim},
  date           = {2019-10-12},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Systematic Asset Allocation using Flexible Views for South African Markets},
  url            = {https://arxiv.org/abs/1910.05555},
  urldate        = {2019-10-24},
  abstract       = {We implement a systematic asset allocation model using the Historical Simulation with Flexible Probabilities (HS-FP) framework developed by Meucci. The HS-FP framework is a flexible non-parametric estimation approach that considers future asset class behavior to be conditional on time and market environments, and derives a forward looking distribution that is consistent with this view while remaining close as possible to the prior distribution. The framework derives the forward looking distribution by applying unequal time and state conditioned probabilities to historical observations of asset class returns. This is achieved using relative entropy to find estimates with the least distortion to the prior distribution. Here, we use the HS-FP framework on South African financial market data for asset allocation purposes; by estimating expected returns, correlations and volatilities that are better represented through the measured market cycle. We demonstrated a range of state variables that can be useful towards understanding market environments. Concretely, we compare the out-of-sample performance for a specific configuration of the HS-FP model relative to classic Mean Variance Optimization(MVO) and Equally Weighted (EW) benchmark models. The framework displays low probability of backtest overfitting and the out-of-sample net returns and Sharpe ratio point estimates of the HS-FP model outperforms the benchmark models. However, the results are inconsistent when training windows are varied, the Sharpe ratio is seen to be inflated, and the method does not demonstrate statistically significant out-performance on a gross and net basis.},
  day            = {12},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-25 21:36},
}

@Article{Barkhagen-et-al-2019,
  author         = {Barkhagen, Mathias and Fleming, Brian and Quiles, Sergio Garcia and Gondzio, Jacek and Kalcsics, Joerg and Kroeske, Jens and Sabanis, Sotirios and Staal, Arne},
  date           = {2019-06-03},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Optimising portfolio diversification and dimensionality},
  url            = {https://arxiv.org/abs/1906.00920},
  urldate        = {2019-10-02},
  abstract       = {A new framework for portfolio diversification is introduced which goes beyond the classical mean-variance approach and portfolio allocation strategies such as risk parity. It is based on a novel concept called portfolio dimensionality that connects diversification to the non-Gaussianity of portfolio returns and can typically be defined in terms of the ratio of risk measures which are homogenous functions of equal degree. The latter arises naturally due to our requirement that diversification measures should be leverage invariant. We introduce this new framework and argue the benefits relative to existing measures of diversification in the literature, before addressing the question of optimizing diversification or, equivalently, dimensionality. Maximising portfolio dimensionality leads to highly non-trivial optimization problems with objective functions which are typically non-convex and potentially have multiple local optima. Two complementary global optimization algorithms are thus presented. For problems of moderate size and more akin to asset allocation problems, a deterministic Branch and Bound algorithm is developed, whereas for problems of larger size a stochastic global optimization algorithm based on Gradient Langevin Dynamics is given. We demonstrate analytically and through numerical experiments that the framework reflects the desired properties often discussed in the literature.},
  day            = {3},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  timestamp      = {2020-02-25 21:41},
}

@Article{Louton-Saraoglu-2008,
  author         = {Louton, David and Saraoglu, Hakan},
  date           = {2008-08-31},
  journaltitle   = {The Journal of Investing},
  title          = {How Many Mutual Funds Are Needed to Form a Well- Diversified Asset Allocated Portfolio?},
  doi            = {10.3905/joi.2008.710919},
  issn           = {1068-0896},
  number         = {3},
  pages          = {47--63},
  urldate        = {2019-07-15},
  volume         = {17},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  timestamp      = {2020-02-25 21:43},
}

@Article{Meucci-et-al-2015,
  author       = {Attilio Meucci and Alberto Santangelo and Romain Deguest},
  date         = {2015},
  journaltitle = {Risk Magazine},
  title        = {Risk budgeting and diversification based on optimised uncorrelated factors},
  url          = {https://www.risk.net/risk-management/2433224/risk-budgeting-and-diversification-based-on-optimised-uncorrelated-factors},
  abstract     = {We measure the contributions to risk of a set of factors, strategies, or investments, based on "Minimum-Torsion Bets", namely a set of uncorrelated factors, optimized to closely track the factors used to allocate the portfolio. We then introduce a novel definition of contributions to risk, which generalizes the "marginal contributions to risk", traditionally used in banks for risk budgeting and in asset management to build risk parity strategies.

The Minimum-Torsion Bets allow us to also introduce a natural diversification score, the Effective Number of Minimum-Torsion Bets, which we use to measure and manage diversification.

We discuss the advantages of the Minimum-Torsion Bets over the traditional approach to diversification based on marginal contributions to risk. We present two case studies, a security-based investment in the stocks of the S\&P 500, and a factor-based investment in the five Fama-French factors.},
  groups       = {Risk_Budgeting, Invest_Risk, Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:43},
}

@Article{Jennings-Payne-2016,
  author               = {Jennings, William W. and Payne, Brian C.},
  date                 = {2016-03},
  journaltitle         = {Financial Analysts Journal},
  title                = {Fees Eat Diversification's Lunch},
  doi                  = {10.2469/faj.v72.n2.1},
  issn                 = {0015-198X},
  number               = {2},
  pages                = {31--40},
  volume               = {72},
  abstract             = {Although diversification is often spoken of as the only free lunch in investing, the authors show that it is not free and that it must be considered in light of its costs. They also show that fees on diversifying asset classes are high relative to their risk-adjusted diversification benefit, with the more exotic asset classes carrying higher price tags. Because there is meaningful cross-sectional variation, fees need to be considered when making strategic asset allocation decisions.},
  citeulike-article-id = {14150315},
  citeulike-linkout-0  = {http://dx.doi.org/10.2469/faj.v72.n2.1},
  groups               = {Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-10-02 16:52:21},
  timestamp            = {2020-02-25 21:45},
}

@Article{Kritzman-2015,
  author               = {Kritzman, Mark},
  date                 = {2015-01},
  journaltitle         = {Financial Analysts Journal},
  title                = {What Practitioners Need to Know ... About Time Diversification (corrected)},
  doi                  = {10.2469/faj.v71.n1.4},
  issn                 = {0015-198X},
  number               = {1},
  pages                = {29--34},
  volume               = {71},
  abstract             = {Although an investor may be less likely to lose money over a long horizon than over a short horizon, the magnitude of a potential loss increases with the length of the investment horizon.},
  citeulike-article-id = {14514123},
  citeulike-linkout-0  = {http://dx.doi.org/10.2469/faj.v71.n1.4},
  groups               = {Invest_Diversif},
  posted-at            = {2018-01-09 16:47:05},
  timestamp            = {2020-02-25 21:46},
}

@Article{Chollete-et-al-2011,
  author       = {Chollete, L. and de la Pena, V. and Lu, C.},
  date         = {2011},
  journaltitle = {Journal of Banking and Finance},
  title        = {International Diversification: A Copula Approach},
  number       = {2},
  pages        = {403--417},
  url          = {https://www.sciencedirect.com/science/article/pii/S0378426610003298},
  volume       = {35},
  abstract     = {The viability of international diversification involves balancing benefits and costs. This balance hinges on the degree of asset dependence. In light of theoretical research linking diversification and dependence, we examine international diversification using two measures of dependence: correlations and copulas.

We document several findings.

First, dependence has increased over time.

Second, we find evidence of asymmetric dependence or downside risk in Latin America, but less in the G5. The results indicate very little downside risk in East Asia.

Third, East Asian and Latin American returns exhibit some correlation complexity. Interestingly, the regions with maximal dependence or worst diversification do not command large returns. Our results suggest international limits to diversification. They are also consistent with a possible tradeoff between international diversification and systemic risk.},
  groups       = {Diversification_Measure, Asymm_Dependence},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:47},
}

@Article{Carmichael-et-al-2015a,
  author         = {Carmichael, Benoit and Koumou, Gilles and Moran, Kevin},
  date           = {2015-09},
  journaltitle   = {SSRN Electronic Journal},
  title          = {A New Formulation of Maximum Diversification Indexation Using Rao's Quadratic Entropy},
  url            = {https://ssrn.com/abstract=2923220},
  abstract       = {This paper proposes a new formulation of the Maximum Diversification indexation strategy based on Rao Quadratic Entropy (RQE). It clarifies the investment problem underlying the Most Diversified Portfolio (MDP) formed with this strategy, identifies the source of the MDP out-of-sample performance, and suggests dimensions along which this performance can be improved. We show that these potential improvements are quantitatively important and are robust to portfolio turnover, portfolio risk, estimation window, and covariance matrix estimation.},
  f1000-projects = {QuantInvest},
  groups         = {Diversified_Invest, Invest_Diversif},
  timestamp      = {2020-02-25 21:52},
}

@Conference{Lee-2013,
  author    = {Wai Lee},
  booktitle = {Second Annual Inside Indexing Conference},
  date      = {2013},
  title     = {Risk Based Asset Allocation},
  url       = {https://pdfs.semanticscholar.org/6101/ee13a5ac3a2387441351be7ffd03b6a8a9d1.pdf},
  abstract  = {In recent years, we have witnessed an alarmingly large and growing amount of literature on portfolio construction approaches focused on risks and diversification rather than estimating expected returns. Numerous simulations, applied to different universes, have been documented in support of these approaches based on their apparent outperformance versus passive market-capitalization weighting or static, fixed weight portfolios. Many studies attribute the better performance of these risk-based asset allocation approaches to superior diversification.

Given the absence of clearly defined investment objective functions behind these approaches as well as the metrics used by these studies to evaluate ex-post performance, we put these approaches into the same context of mean-variance efficiency in an attempt to understand their theoretical underpinnings. In doing so, we hope to shed some light on what these approaches attempt to achieve and on the characteristics of the investment universe, if indeed these approaches are meant to approximate meanvariance efficiency.

Rather than adding to the already large collection of simulation results, we use some simple examples to compare and contrast the portfolio and risk characteristics of these approaches. We also reiterate that any portfolio that deviates from the market capitalization-weighted portfolio is an active portfolio.

Finally, we conclude there is no theory to predict, ex-ante, that any of these riskbased approaches should outperform.},
  groups    = {Diversification_Measure},
  owner     = {zkgst0c},
  timestamp = {2020-02-25 21:52},
}

@Article{Cesarone-et-al-2019,
  author         = {Cesarone, Francesco and Scozzari, Andrea and Tardella, Fabio},
  date           = {2019-07-25},
  journaltitle   = {Journal of Global Optimization},
  title          = {An optimization-diversification approach to portfolio selection},
  doi            = {10.1007/s10898-019-00809-7},
  issn           = {0925-5001},
  urldate        = {2019-09-10},
  abstract       = {The classical approaches to optimal portfolio selection call for finding a feasible portfolio that optimizes a risk measure, or a gain measure, or a combination thereof by means of a utility function or of a performance measure. However, the optimization approach tends to amplify the estimation errors on the parameters required by the model, such as expected returns and covariances. For this reason, the Risk Parity model, a novel risk diversification approach to portfolio selection, has been recently theoretically developed and used in practice, mainly for the case of the volatility risk measure. Here we first provide new theoretical results for the Risk Parity approach for general risk measures. Then we propose a novel framework for portfolio selection that combines the diversification and the optimization approaches through the global solution of a hard nonlinear mixed integer or pseudo Boolean problem. For the latter problem we propose an efficient and accurate Multi-Greedy heuristic that extends the classical single-threaded greedy approach to a multiple-threaded setting. Finally, we provide empirical results on real-world data showing that the diversified optimal portfolios are only slightly suboptimal in-sample with respect to optimal portfolios, and generally show improved out-of-sample performance with respect to their purely diversified or purely optimized counterparts.},
  day            = {25},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  timestamp      = {2020-02-25 21:53},
}

@Article{Darnell-2009,
  author       = {Max Darnell},
  date         = {2009},
  journaltitle = {SSRN Electronic Journal},
  title        = {Did Diversification Fail?},
  url          = {https://www.firstquadrant.com/system/files/2009_10_Did_Diversification_Fail_0.pdf},
  abstract     = {Many investors had felt that by spreading their investments across many asset classes - by investing in a wide array of betas - that they would avoid the risk of an across-theboard decline in their investments. They thought that they had avoided the problem associated with putting all their eggs in one basket as the adage advises. When most assets did fall together in largely simultaneous fashion in the midst of the recent credit crisis, investors rated diversification a failure, and cried out in frustration that correlations had all converged on one. Diversification failed this year, 1 was the title of a New York Times article in the business section in November last year. In another, more recent article,2 one of the large university endowments explained that diversification had failed to protect its asset values. This sentiment was, and is, entirely common. If their eggs were all in different baskets, then it would appear that they were somehow all tied together sharing a common fate when their fates were assumed to have been independent of one another instead. There are several aspects of this that are wrong. Diversification didn't fail; the metaphor of eggs in different baskets doesn't accurately capture the purpose of diversification; and those weren't betas that they diversified across},
  groups       = {Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:54},
}

@InCollection{deCarvalho-et-al-2017a,
  author               = {{de Carvalho}, Raul L. and Lu, Xiao and Soupe, Francois and Dugnolle, Patrick},
  booktitle            = {Factor Investing},
  date                 = {2017},
  title                = {Diversify and Purify Factor Premiums in Equity Markets},
  doi                  = {10.1016/b978-1-78548-201-4.50004-0},
  isbn                 = {9781785482014},
  pages                = {73--97},
  publisher            = {Elsevier},
  abstract             = {In this chapter, we consider the question of how to improve the efficacy of strategies designed to capture factor premiums in equity markets and, in particular, from the value, quality, low-risk and momentum factors. We consider a number of portfolio construction approaches designed to capture factor premiums with the appropriate levels of risk controls aiming at increasing information ratios. We show that information ratios can be increased by targeting constant volatility (CV) over time, hedging market beta (HB) and hedging exposures to the size factor, i.e. neutralizing biases in the market capitalization of stocks used in factor strategies. With regard to the neutralization of sector exposures, we find this to be of particular importance for the value and low-risk factors. Finally, we look at the added value of shorting stocks in factor strategies. We find that with few exceptions the contributions to performance from the short leg are inferior to those from the long leg. Thus, long-only strategies can be efficient alternatives to capture these factor premiums. Finally, we find that factor premiums tend to have fatter tails than what could be expected from a Gaussian distribution of returns, but that skewness is not significantly negative in most cases.},
  citeulike-article-id = {14499092},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/b978-1-78548-201-4.50004-0},
  groups               = {Invest_Factor, Invest_Risk, Factor_Types, Invest_Diversif},
  posted-at            = {2017-12-08 00:48:48},
  timestamp            = {2020-02-25 21:55},
}

@Article{DeMiguel-et-al-2009a,
  author       = {DeMiguel, V. and Garlappi, L. and Uppal, R.},
  date         = {2009},
  journaltitle = {Review of Financial Studies},
  title        = {Optimal versus Naive Diversification: how Inefficient is the 1/N Portfolio Strategy},
  doi          = {10.1093/rfs/hhm075},
  pages        = {1915--1953},
  url          = {Optimal versus Naive Diversification: how Inefficient is the 1/N
Portfolio Strategy},
  volume       = {22},
  abstract     = {We evaluate the out-of-sample performance of the sample-based mean-variance model, and its extensions designed to reduce estimation error, relative to the naive 1/N portfolio. Of the 14 models we evaluate across seven empirical datasets, none is consistently better than the 1/N rule in terms of Sharpe ratio, certainty-equivalent return, or turnover, which indicates that, out of sample, the gain from optimal diversification is more than offset by estimation error. Based on parameters calibrated to the US equity market, our analytical results and simulations show that the estimation window needed for the sample-based mean-variance strategy and its extensions to outperform the 1/N benchmark is around 3000 months for a portfolio with 25 assets and about 6000 months for a portfolio with 50 assets. This suggests that there are still many miles to go before the gains promised by optimal portfolio choice can actually be realized out of sample.},
  groups       = {Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 21:55},
}

@Article{Domian-et-al-2007,
  author               = {Domian, Dale L. and Louton, David A. and Racine, Marie D.},
  date                 = {2007-11},
  journaltitle         = {Financial Review},
  title                = {Diversification in Portfolios of Individual Stocks: 100 Stocks Are Not Enough},
  doi                  = {10.1111/j.1540-6288.2007.00183.x},
  issn                 = {0732-8516},
  number               = {4},
  pages                = {557--570},
  volume               = {42},
  abstract             = {We examine returns and ending wealth in portfolios selected from 1,000 large U.S. stocks over a 20-year holding period. Shortfall risk, the possibility of ending wealth being below a target, is a useful metric for long horizon investors and is consistent with the Safety First criterion. Density functions obtained from simulations illustrate that shortfall risk reduction continues as portfolio size is increased, even above 100 stocks. A slightly lower risk can be achieved in small portfolios by diversifying across industries, but a greater reduction is obtained by simply increasing the number of stocks.},
  citeulike-article-id = {1948681},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/j.1540-6288.2007.00183.x},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/bpl/fire/2007/00000042/00000004/art00004},
  day                  = {1},
  groups               = {Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-11-07 04:12:43},
  publisher            = {Blackwell Publishing Inc},
  timestamp            = {2020-02-25 21:56},
}

@Article{Fouquau-et-al-2018,
  author         = {Fouquau, Julien and Kharoubi, Cecile and Spieser, Philippe},
  date           = {2018},
  journaltitle   = {The Journal of Risk},
  title          = {International and temporal diversifications: the best of both worlds?},
  doi            = {10.21314/{JOR}.2018.382},
  issn           = {1465-1211},
  url            = {https://www.risk.net/journal-of-risk/5472731/international-and-temporal-diversifications-the-best-of-both-worlds},
  urldate        = {2019-05-30},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  timestamp      = {2020-02-25 21:57},
}

@Article{Hallerbach-2017,
  author               = {Hallerbach, Winfried G.},
  date                 = {2017-07},
  journaltitle         = {The Journal of Wealth Management},
  title                = {If You Have Said A, You Must Also Say B: Calculating Diversified Asset Returns},
  doi                  = {10.3905/jwm.2017.20.2.076},
  issn                 = {1534-7524},
  number               = {2},
  pages                = {76--81},
  volume               = {20},
  abstract             = {The bottom-up route to portfolio diversification is clear: Combining individual assets into a portfolio will lower portfolio risk (especially when correlations are low). The top-down route of evaluating individual assets from the perspective of the diversified portfolio is widely applied in risk budgeting but is neglected in return attributions. Consequently, many investors evaluate individual assets on the basis of their undiversified returns instead of including the diversification benefits they offer. This perspective biases the evaluation of high-volatility/low-correlation assets in the portfolio. In this note, we highlight the importance of evaluating diversified returns and show how we can calculate these returns. We illustrate the method for a U.S. asset portfolio.},
  citeulike-article-id = {14402574},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2017.20.2.076},
  groups               = {Invest_Diversif},
  posted-at            = {2017-07-30 07:46:31},
  timestamp            = {2020-02-25 21:58},
}

@Article{Humphrey-et-al-2015,
  author               = {Humphrey, Jacquelyn E. and Benson, Karen L. and Low, Rand K. Y. and Lee, Wei-Lun},
  date                 = {2015-11},
  journaltitle         = {Pacific-Basin Finance Journal},
  title                = {Is diversification always optimal?},
  doi                  = {10.1016/j.pacfin.2015.09.003},
  issn                 = {0927-538X},
  pages                = {521--532},
  volume               = {35},
  abstract             = {Should retirement savers diversify across many funds or consolidate into one fund? We examine Australian retirement savings. Theoretically, diversification across funds is the optimal strategy. With real-world short-selling constraints, investment in a single fund is optimal. Finance theory and recent literature suggest that investors should diversify their retirement savings across a number of funds. However, the Australian government encourages investors to consolidate retirement savings into just one fund. Using a number of optimization techniques, we investigate which of these two actions would result in the best outcome for investors in terms of risk and return. We find that in the majority of cases investors would be better off not diversifying their holdings; mainly because superannuation funds cannot be short sold. Consolidation therefore does appear to be the optimal strategy for the average superannuation investor.},
  citeulike-article-id = {14160270},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.pacfin.2015.09.003},
  groups               = {Invest_Diversif},
  owner                = {zkgst0c},
  posted-at            = {2016-10-12 20:57:58},
  timestamp            = {2020-02-25 21:59},
}

@Article{Jacobs-et-al-2014,
  author               = {Jacobs, Heiko and M uller, Sebastian and Weber, Martin},
  date                 = {2014-06},
  journaltitle         = {Journal of Financial Markets},
  title                = {How should individual investors diversify? An empirical evaluation of alternative asset allocation policies},
  doi                  = {10.1016/j.finmar.2013.07.004},
  issn                 = {1386-4181},
  pages                = {62--85},
  volume               = {19},
  abstract             = {For global equity diversification, prominent Markowitz extensions do not outperform several heuristic weighting schemes (1/N heuristic, market value-weighting and GDP-weighting). Comparing the different heuristic stock weighting schemes, the value-weighted heuristic performs worse than the GDP-weighted global stock portfolio. Diversification gains in the asset allocation context are mainly driven by a well-balanced allocation over different asset classes. Consistent with global equity diversification, Markowitz-based optimization methods do not add significant value when allocating across different asset classes. This paper evaluates numerous diversification strategies as a possible remedy against widespread costly investment mistakes of individual investors. Our results reveal that a very broad range of simple heuristic allocation schemes offers similar diversification gains as well-established or recently developed portfolio optimization approaches. This holds true for both international diversification in the stock market and diversification over different asset classes. We thus suggest easy-to-implement allocation guidelines for individual investors.},
  citeulike-article-id = {13987903},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.finmar.2013.07.004},
  groups               = {Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-03-25 08:36:39},
  timestamp            = {2020-02-25 21:59},
}

@Article{McKay-et-al-2017,
  author               = {McKay, Shawn and Shapiro, Robert and Thomas, Ric},
  date                 = {2017-11},
  journaltitle         = {Financial Analysts Journal},
  title                = {What Free Lunch? The Costs of Overdiversification},
  doi                  = {10.2469/faj.v74.n1.2},
  issn                 = {0015-198X},
  pages                = {1--15},
  abstract             = {Institutional investors, charged with outperforming a policy benchmark, often allocate to external active managers in order to hit their return objective. The challenge is to do so without overdiversifying the plan. Hiring too many managers can significantly reduce active risk, leaving the plan with high fees and limited ability to outperform a policy benchmark. We review the number of external investment strategies held by the largest US public and corporate pension funds. Our analysis shows that most large pension funds are overdiversified, allowing us to suggest a simpler framework for moving forward.},
  citeulike-article-id = {14485296},
  citeulike-linkout-0  = {http://dx.doi.org/10.2469/faj.v74.n1.2},
  day                  = {20},
  groups               = {BenchmarkInvest, Invest_Diversif},
  posted-at            = {2017-11-28 18:26:03},
  timestamp            = {2020-02-25 22:00},
}

@Article{Page-Taborsky-2011,
  author       = {Sebastien Page and Mark A. Taborsky},
  date         = {2011},
  journaltitle = {The Journal of Portfolio Management},
  title        = {The myth of diversification: risk factors versus asset classes},
  number       = {4},
  pages        = {1--2},
  url          = {https://jpm.pm-research.com/content/36/1/26},
  volume       = {37},
  abstract     = {In our New Normal world, regime shifts in economic conditions will continue to cause significant challenges for risk management and portfolio construction. On average, correlations across risk factors are lower than correlations across asset classes, and risk factor correlations tend to be more robust to regime shifts. Risk factors provide a flexible language with which investors may express their forward-looking economic views, adapt to regime shifts and diversify their portfolios accordingly.},
  groups       = {Invest_Diversif},
  owner        = {zkgst0c},
  timestamp    = {2020-02-25 22:01},
}

@Article{Pittman-et-al-2019,
  author         = {Pittman, Sam and Singh, Amneet and Srinivasan, Sangeetha},
  date           = {2019-09-07},
  journaltitle   = {The Journal of Wealth Management},
  title          = {Diversification benefits, where art thou?},
  doi            = {10.3905/jwm.2019.1.081},
  issn           = {1534-7524},
  pages          = {jwm.2019.1.081},
  urldate        = {2019-09-28},
  abstract       = {Following the global financial crisis, a portfolio concentrated in US large cap equity and aggregate fixed income has provided higher returns than diversified portfolios through 2019. Such a prolonged experience causes investors to question the benefits of diversification. This leads us to use a longer history of data across 15 asset classes to understand the historical benefits of diversifying a portfolio with international equity, real assets, and below investment grade fixed income. Our results portray the frequency and magnitude of risk-adjusted return improvement coming from different diversifying asset classes over five-year holding periods. We find that certain asset classes, such as below investment grade fixed income, regularly improve risk-adjusted return of the portfolio, while other asset classes like commodities improve risk-adjusted returns less frequently. Further, we observe that some asset classes do not deliver meaningful risk-adjusted return improvements in the presence of other asset classes. Our conclusion is that investors should continue to build diversified portfolios, but in doing so they should consider that some asset classes more consistently improved risk-adjusted returns than others.},
  day            = {7},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  timestamp      = {2020-02-25 22:01},
}

@Article{Pourbabaee-et-al-2016,
  author               = {Pourbabaee, Farzad and Kwak, Minsuk and Pirvu, Traian A.},
  date                 = {2016-09},
  journaltitle         = {Quantitative Finance},
  title                = {Risk minimization and portfolio diversification},
  doi                  = {10.1080/14697688.2015.1115891},
  number               = {9},
  pages                = {1325--1332},
  volume               = {16},
  citeulike-article-id = {14150627},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2015.1115891},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2015.1115891},
  day                  = {1},
  groups               = {Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-10-03 01:18:37},
  publisher            = {Routledge},
  timestamp            = {2020-02-25 22:02},
}

@Article{Vandenbroucke-2019,
  author         = {Vandenbroucke, Jurgen},
  date           = {2019-05-09},
  journaltitle   = {The Journal of Investing},
  title          = {Adaptive Portfolios and the Power of Diversification},
  url            = {https://joi.iijournals.com/content/early/2019/05/09/joi.2019.1.089},
  urldate        = {2019-05-09},
  abstract       = {The article gives a qualitative description of an advisory or discretionary investment process that manages the emotional aspect of investing. Portfolios are adaptive, meaning they automatically adjust their allocation in response to changing market conditions. The adjustments are model-based and transparent, and align in terms of frequency and magnitude with the investors emotionality. The process looks beyond the risk-focused paradigm in relation to investor profiling, product positioning, and portfolio construction. First, investor profiles distinguish between the attitude toward risk and the attitude toward loss. Second, products differentiate in terms of variance and in terms of skewness. Finally, adaptive portfolios represent a client centric combination of products that lifts the power of diversification to a higher level and ultimately contributes to long term buy-and-hold investor behavior.},
  day            = {9},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Diversif},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-02-25 22:03},
}

@Article{Adachi-Trendafilov-2017,
  author               = {Adachi, Kohei and Trendafilov, NickolayT},
  date                 = {2017},
  journaltitle         = {Advances in Data Analysis and Classification},
  title                = {Sparsest factor analysis for clustering variables: a matrix decomposition approach},
  doi                  = {10.1007/s11634-017-0284-z},
  pages                = {1--27},
  abstract             = {We propose a new procedure for sparse factor analysis (FA) such that each variable loads only one common factor. Thus, the loading matrix has a single nonzero element in each row and zeros elsewhere. Such a loading matrix is the sparsest possible for certain number of variables and common factors. For this reason, the proposed method is named sparsest FA (SSFA). It may also be called FA-based variable clustering, since the variables loading the same common factor can be classified into a cluster. In SSFA, all model parts of FA (common factors, their correlations, loadings, unique factors, and unique variances) are treated as fixed unknown parameter matrices and their least squares function is minimized through specific data matrix decomposition. A useful feature of the algorithm is that the matrix of common factor scores is re-parameterized using QR decomposition in order to efficiently estimate factor correlations. A simulation study shows that the proposed procedure can exactly identify the true sparsest models. Real data examples demonstrate the usefulness of the variable clustering performed by SSFA.},
  citeulike-article-id = {14433260},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11634-017-0284-z},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s11634-017-0284-z},
  groups               = {Sparse factor analysis, Clustering and network analysis},
  posted-at            = {2017-09-17 20:19:21},
  publisher            = {Springer Berlin Heidelberg},
  timestamp            = {2020-02-27 03:51},
}

@Article{Aghabozorgi-et-al-2015,
  author               = {Aghabozorgi, Saeed and Seyed Shirkhorshidi, Ali and Ying Wah, Teh},
  date                 = {2015-10},
  journaltitle         = {Information Systems},
  title                = {Time-series clustering - A decade review},
  doi                  = {10.1016/j.is.2015.04.007},
  issn                 = {0306-4379},
  pages                = {16--38},
  volume               = {53},
  abstract             = {Anatomy of time-series clustering is revealed by introducing its 4 main component. Research works in each of the four main components are reviewed in detail and compared. Analysis of research works published in the last decade. Enlighten new paths for future works for time-series clustering and its components. Clustering is a solution for classifying enormous data when there is not any early knowledge about classes. With emerging new concepts like cloud computing and big data and their vast applications in recent years, research works have been increased on unsupervised solutions like clustering algorithms to extract knowledge from this avalanche of data. Clustering time-series data has been used in diverse scientific areas to discover patterns which empower data analysts to extract valuable information from complex and massive datasets. In case of huge datasets, using supervised classification solutions is almost impossible, while clustering can solve this problem using un-supervised approaches. In this research work, the focus is on time-series data, which is one of the popular data types in clustering problems and is broadly used from gene expression data in biology to stock market analysis in finance. This review will expose four main components of time-series clustering and is aimed to represent an updated investigation on the trend of improvements in efficiency, quality and complexity of clustering time-series approaches during the last decade and enlighten new paths for future works.},
  citeulike-article-id = {14168675},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.is.2015.04.007},
  keywords             = {pdf},
  posted-at            = {2016-10-19 19:24:13},
  timestamp            = {2020-02-27 03:51},
}

@InCollection{Alaiz-et-al-2014,
  author               = {Alaz, Carlos M and Fernandez, Angela and Gala, Yvonne and Dorronsoro, Jose R},
  booktitle            = {Intelligent Data Engineering and Automated Learning - IDEAL 2014},
  date                 = {2014},
  title                = {Kernel K-Means Low Rank Approximation for Spectral Clustering and Diffusion Maps},
  doi                  = {10.1007/978-3-319-10840-7\_30},
  editor               = {Corchado, Emilio and Lozano, JoseA and Quintian, Hector and Yin, Hujun},
  pages                = {239--246},
  publisher            = {Springer International Publishing},
  series               = {Lecture Notes in Computer Science},
  volume               = {8669},
  abstract             = {Spectral Clustering and Diffusion Maps are currently the leading methods for advanced clustering or dimensionality reduction. However, they require the eigenanalysis of a sample's graph Laplacian L, something very costly for moderately sized samples and prohibitive for very large ones. We propose to build a low rank approximation to L using essentially the centroids obtained applying kernel K-means over the similarity matrix. We call this approach kernel KASP (kKASP) as it follows the KASP procedure of Yan et al. but coupling centroid selection with the local geometry defined by the similarity matrix. As we shall see, kKASP's reconstructions are competitive with KASP's ones, particularly in the low rank range.},
  citeulike-article-id = {14212409},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-10840-730},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-10840-730},
  owner                = {cristi},
  posted-at            = {2016-11-21 22:14:56},
  timestamp            = {2020-02-27 03:51},
}

@Article{Albatineh-NiewiadomskaBugaj-2011,
  author         = {Albatineh, Ahmed N. and Niewiadomska-Bugaj, Magdalena},
  date           = {2011-07},
  journaltitle   = {Journal of Classification},
  title          = {MCS: A method for finding the number of clusters},
  doi            = {10.1007/s00357-010-9069-1},
  issn           = {0176-4268},
  number         = {2},
  pages          = {184--209},
  volume         = {28},
  abstract       = {This paper proposes a maximum clustering similarity (MCS) method for determining the number of clusters in a data set by studying the behavior of similarity indices comparing two (of several) clustering methods. The similarity between the two clusterings is calculated at the same number of clusters, using the indices of Rand (R), Fowlkes and Mallows (FM), and Kulczynski (K) each corrected for chance agreement. The number of clusters at which the index attains its maximum is a candidate for the optimal number of clusters. The proposed method is applied to simulated bivariate normal data, and further extended for use in circular data. Its performance is compared to the criteria discussed in Tibshirani, Walther, and Hastie (2001). The proposed method is not based on any distributional or data assumption which makes it widely applicable to any type of data that can be clustered using at least two clustering algorithms.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:51},
}

@Article{Ando-Bai-2017,
  author               = {Ando, Tomohiro and Bai, Jushan},
  date                 = {2017-06},
  journaltitle         = {Journal of the American Statistical Association},
  title                = {Clustering Huge Number of Financial Time Series: A Panel Data Approach With High-Dimensional Predictors and Factor Structures},
  doi                  = {10.1080/01621459.2016.1195743},
  number               = {519},
  pages                = {1182--1198},
  volume               = {112},
  abstract             = {AbstractThis article introduces a new procedure for clustering a large number of financial time series based on high-dimensional panel data with grouped factor structures. The proposed method attempts to capture the level of similarity of each of the time series based on sensitivity to observable factors as well as to the unobservable factor structure. The proposed method allows for correlations between observable and unobservable factors and also allows for cross-sectional and serial dependence and heteroscedasticities in the error structure, which are common in financial markets. In addition, theoretical properties are established for the procedure. We apply the method to analyze the returns for over 6000 international stocks from over 100 financial markets. The empirical analysis quantifies the extent to which the U.S. subprime crisis spilled over to the global financial markets. Furthermore, we find that nominal classifications based on either listed market, industry, country or region are insufficient to characterize the heterogeneity of the global financial markets. Supplementary materials for this article are available online.},
  citeulike-article-id = {14431349},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/01621459.2016.1195743},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/01621459.2016.1195743},
  day                  = {10},
  groups               = {Clustering and network analysis, Scenario_TimeSeries},
  posted-at            = {2017-09-16 16:57:06},
  publisher            = {Taylor \& Francis},
  timestamp            = {2020-02-27 03:51},
}

@Article{Arakelian-Karlis-2014,
  author               = {Arakelian, Veni and Karlis, Dimitris},
  date                 = {2014-01},
  journaltitle         = {Communications in Statistics - Simulation and Computation},
  title                = {Clustering Dependencies Via Mixtures of Copulas},
  doi                  = {10.1080/03610918.2012.752832},
  number               = {7},
  pages                = {1644--1661},
  volume               = {43},
  abstract             = {The use of mixture models for clustering purposes has been considerably increased the last years primarily due to the existence of efficient computational methods that facilitate estimation. Nowadays, there are several clustering procedures based on mixtures for certain types of data. On the other hand, copulas are becoming very popular models to model dependencies as one of their appealing properties is the separation of the marginal properties of the data from the dependence properties. The purpose of this article is to put together the two distinct ideas, namely mixtures and copulas, so as to use mixtures of copulas aiming at using them for clustering with respect to the dependence properties of the data. This is accomplished by considering finite mixture of different copulas to represent different dependence structures. We provide properties of the derived models along with the description of an estimation method using an EM algorithm based on the standard approach for mixture models. Using daily returns from major stock markets, we illustrate the potential of our method.},
  citeulike-article-id = {14151134},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/03610918.2012.752832},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/03610918.2012.752832},
  day                  = {1},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-10-03 20:23:53},
  publisher            = {Taylor and Francis},
  timestamp            = {2020-02-27 03:51},
}

@Article{Arbelaitz-et-al-2013,
  author               = {Arbelaitz, Olatz and Gurrutxaga, Ibai and Muguerza, Javier and Perez, Jesus M. and Perona, Inigo},
  date                 = {2013-01},
  journaltitle         = {Pattern Recognition},
  title                = {An extensive comparative study of cluster validity indices},
  doi                  = {10.1016/j.patcog.2012.07.021},
  issn                 = {0031-3203},
  number               = {1},
  pages                = {243--256},
  volume               = {46},
  abstract             = {The validation of the results obtained by clustering algorithms is a fundamental part of the clustering process. The most used approaches for cluster validation are based on internal cluster validity indices. Although many indices have been proposed, there is no recent extensive comparative study of their performance. In this paper we show the results of an experimental work that compares 30 cluster validity indices in many different environments with different characteristics. These results can serve as a guideline for selecting the most suitable index for each possible application and provide a deep insight into the performance differences between the currently available indices. We compare 30 cluster validity indices (CVIs) in 720 synthetic and 20 real datasets. We use a new comparison methodology and three clustering algorithms: k-means, Ward and Average-linkage. The CVI performance drops dramatically when noise is present or clusters overlap. Statistical tests suggest a division of three groups of CVIs.},
  citeulike-article-id = {11208386},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.patcog.2012.07.021},
  posted-at            = {2017-11-22 22:50:31},
  timestamp            = {2020-02-27 03:51},
}

@Article{Augustynski-LaskosGrabowski-2018,
  author         = {Augustynski, Iwo and Laskos-Grabowski, Pawel},
  date           = {2018},
  journaltitle   = {Econometrics},
  title          = {Clustering Macroeconomic Time Series},
  doi            = {10.15611/eada.2018.2.06},
  issn           = {1507-3866},
  number         = {2},
  pages          = {74--88},
  volume         = {22},
  abstract       = {The data mining technique of time series clustering is well established in many fields. However, as an unsupervised learning method, it requires making choices that are nontrivially influenced by the nature of the data involved. The aim of this paper is to verify usefulness of the time series clustering method for macroeconomics research, and to develop the most suitable methodology. By extensively testing various possibilities, we arrive at a choice of a dissimilarity measure (compression-based dissimilarity measure, or CDM) which is particularly suitable for clustering macroeconomic variables. We check that the results are stable in time and reflect large-scale phenomena such as crises. We also successfully apply our findings to analysis of national economies, specifically to identifying their structural relations.},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Network, ML_ClustTimeSrs},
  timestamp      = {2020-02-27 03:51},
}

@Article{Bacidore-et-al-2018,
  author         = {Bacidore, Jeff and Berkow, Kathryn and Polidore, Ben and Saraiya, Nigam},
  date           = {2018-10-31},
  journaltitle   = {The Journal of Trading},
  title          = {Cluster Analysis for Evaluating Trading Strategies},
  doi            = {10.3905/jot.2012.7.3.006},
  url            = {https://jot.pm-research.com/content/7/3/6},
  abstract       = {In this article, we introduce a new methodology to empirically identify the primary strategies used by a trader using only post-trade fill data. To do this, we apply a well-established statistical clustering technique called k-means to a sample of progress charts, representing the portion of the order completed by each point in the day as a measure of a trade aggressiveness. Our methodology identifies the primary strategies used by a trader and determines which strategy the trader used for each order in the sample. Having identified the strategy used for each order, trading cost analysis can be performed by strategy. We also discuss ways to exploit this technique to characterize trader behavior, assess trader performance, and suggest the appropriate benchmarks for each distinct trading strategy.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Network},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-02-27 03:51},
}

@Article{Balzer-et-al-2018,
  author         = {Balzer, Laura B. and Zheng, Wenjing and van der Laan, Mark J. and Petersen, Maya L.},
  date           = {2018-01-01},
  journaltitle   = {Statistical Methods in Medical Research},
  title          = {A new approach to hierarchical data analysis: Targeted maximum likelihood estimation for the causal effect of a cluster-level exposure.},
  doi            = {10.1177/0962280218774936},
  pages          = {962280218774936},
  abstract       = {We often seek to estimate the impact of an exposure naturally occurring or randomly assigned at the cluster-level. For example, the literature on neighborhood determinants of health continues to grow. Likewise, community randomized trials are applied to learn about real-world implementation, sustainability, and population effects of interventions with proven individual-level efficacy. In these settings, individual-level outcomes are correlated due to shared cluster-level factors, including the exposure, as well as social or biological interactions between individuals. To flexibly and efficiently estimate the effect of a cluster-level exposure, we present two targeted maximum likelihood estimators (TMLEs). The first TMLE is developed under a non-parametric causal model, which allows for arbitrary interactions between individuals within a cluster. These interactions include direct transmission of the outcome (i.e. contagion) and influence of one individual's covariates on another's outcome (i.e. covariate interference). The second TMLE is developed under a causal sub-model assuming the cluster-level and individual-specific covariates are sufficient to control for confounding. Simulations compare the alternative estimators and illustrate the potential gains from pairing individual-level risk factors and outcomes during estimation, while avoiding unwarranted assumptions. Our results suggest that estimation under the sub-model can result in bias and misleading inference in an observational setting. Incorporating working assumptions during estimation is more robust than assuming they hold in the underlying causal model. We illustrate our approach with an application to HIV prevention and treatment.},
  day            = {1},
  f1000-projects = {QuantInvest},
  groups         = {Estim_MaxLikelihood, Data_Explo_Analysis},
  pmid           = {29921160},
  timestamp      = {2020-02-27 03:51},
}

@Article{Bandara-et-al-2019,
  author         = {Bandara, Kasun and Bergmeir, Christoph and Smyl, Slawek},
  date           = {2019-10-09},
  journaltitle   = {Expert Systems with Applications},
  title          = {Forecasting Across Time Series Databases using Recurrent Neural Networks on Groups of Similar Series: A Clustering Approach},
  url            = {https://www.sciencedirect.com/science/article/pii/S0957417419306128},
  urldate        = {2019-03-07},
  abstract       = {With the advent of Big Data, nowadays in many applications databases containing large quantities of similar time series are available. Forecasting time series in these domains with traditional univariate forecasting procedures leaves great potentials for producing accurate forecasts untapped. Recurrent neural networks (RNNs), and in particular Long Short-Term Memory (LSTM) networks, have proven recently that they are able to outperform state-of-the-art univariate time series forecasting methods in this context when trained across all available time series. However, if the time series database is heterogeneous, accuracy may degenerate, so that on the way towards fully automatic forecasting methods in this space, a notion of similarity between the time series needs to be built into the methods. To this end, we present a prediction model that can be used with different types of RNN models on subgroups of similar time series, which are identified by time series clustering techniques. We assess our proposed methodology using LSTM networks, a widely popular RNN variant. Our method achieves competitive results on benchmarking datasets under competition evaluation procedures. In particular, in terms of mean sMAPE accuracy, it consistently outperforms the baseline LSTM model and outperforms all other methods on the CIF2016 forecasting competition dataset.},
  day            = {9},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:51},
}

@Article{Basalto-et-al-2007,
  author               = {Basalto, Nicolas and Bellotti, Roberto and De Carlo, Francesco and Facchi, Paolo and Pantaleo, Ester and Pascazio, Saverio},
  date                 = {2007-06},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Hausdorff clustering of financial time series},
  doi                  = {10.1016/j.physa.2007.01.011},
  issn                 = {0378-4371},
  number               = {2},
  pages                = {635--644},
  volume               = {379},
  abstract             = {A clustering procedure is introduced based on the Hausdorff distance as a similarity measure between clusters of elements. The method is applied to the financial time series of the Dow Jones industrial average (DJIA) index to find companies that share a similar behavior. Comparisons are made with other linkage algorithms.},
  citeulike-article-id = {14148581},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2007.01.011},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-09-28 19:44:58},
  timestamp            = {2020-02-27 03:51},
}

@Article{Bastos-Caiado-2014,
  author               = {Bastos, Joao A. and Caiado, Jorge},
  date                 = {2014-12},
  journaltitle         = {Quantitative Finance},
  title                = {Clustering financial time series with variance ratio statistics},
  doi                  = {10.1080/14697688.2012.726736},
  number               = {12},
  pages                = {2121--2133},
  volume               = {14},
  abstract             = {This study introduces a new distance measure for clustering financial time series based on variance ratio test statistics. The proposed metric attempts to assess the level of interdependence of time series from the point of view of return predictability. Simulation results show that this metric aggregates time series according to their serial dependence structure better than a metric based on the sample autocorrelations. An empirical application of this approach to international stock market returns is presented. The results suggest that this metric discriminates stock markets reasonably well according to size and the level of development. Furthermore, despite the substantial evolution of individual variance ratio statistics, the clustering pattern remains fairly stable across different time periods.},
  citeulike-article-id = {14316690},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2012.726736},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2012.726736},
  day                  = {2},
  groups               = {Networks and investment management},
  posted-at            = {2017-03-23 08:25:58},
  publisher            = {Routledge},
  timestamp            = {2020-02-27 03:51},
}

@InProceedings{Begusic-Kostanjcar-2019,
  author         = {Begusic, Stjepan and Kostanjcar, Zvonko},
  booktitle      = {11th International Symposium on Image and Signal Processing and Analysis (ISPA)},
  date           = {2019-09-23},
  title          = {Cluster-Based Shrinkage of Correlation Matrices for Portfolio Optimization},
  doi            = {10.1109/{ISPA}.2019.8868482},
  isbn           = {978-1-7281-3140-5},
  pages          = {301--305},
  publisher      = {IEEE},
  url            = {https://ieeexplore.ieee.org/document/8868482/},
  urldate        = {2020-01-13},
  day            = {23},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:51},
}

@Article{Berthold-Hoppner-2016,
  author               = {Berthold, Michael R. and Hoppner, Frank},
  date                 = {2016-01-10},
  journaltitle         = {arXiv Electronic Journal},
  title                = {On Clustering Time Series Using Euclidean Distance and Pearson Correlation},
  eprint               = {1601.02213},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1601.02213},
  abstract             = {For time series comparisons, it has often been observed that z-score normalized Euclidean distances far outperform the unnormalized variant. In this paper we show that a z-score normalized, squared Euclidean Distance is, in fact, equal to a distance based on Pearson Correlation. This has profound impact on many distance-based classification or clustering methods. In addition to this theoretically sound result we also show that the often used k-Means algorithm formally needs a mod ification to keep the interpretation as Pearson correlation strictly valid. Experimental results demonstrate that in many cases the standard k-Means algorithm generally produces the same results.},
  citeulike-article-id = {13904331},
  citeulike-linkout-0  = {http://arxiv.org/abs/1601.02213},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1601.02213},
  day                  = {10},
  posted-at            = {2017-11-23 21:43:08},
  timestamp            = {2020-02-27 03:51},
}

@Article{Bien-Tibshirani-2011,
  author               = {Bien, Jacob and Tibshirani, Robert},
  date                 = {2011-09},
  journaltitle         = {Journal of the American Statistical Association},
  title                = {Hierarchical Clustering With Prototypes via Minimax Linkage},
  doi                  = {10.1198/jasa.2011.tm10183},
  number               = {495},
  pages                = {1075--1084},
  volume               = {106},
  abstract             = {Agglomerative hierarchical clustering is a popular class of methods for understanding the structure of a dataset. The nature of the clustering depends on the choice of linkage, that is, on how one measures the distance between clusters. In this article we investigate minimax linkage, a recently introduced but little-studied linkage. Minimax linkage is unique in naturally associating a prototype chosen from the original dataset with every interior node of the dendrogram. These prototypes can be used to greatly enhance the interpretability of a hierarchical clustering. Furthermore, we prove that minimax linkage has a number of desirable theoretical properties; for example, minimax-linkage dendrograms cannot have inversions (unlike centroid linkage) and is robust against certain perturbations of a dataset. We provide an efficient implementation and illustrate minimax linkage's strengths as a data analysis and visualization tool on a study of words from encyclopedia articles and on a dataset of images of human faces.},
  citeulike-article-id = {14148576},
  citeulike-linkout-0  = {http://dx.doi.org/10.1198/jasa.2011.tm10183},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm10183},
  day                  = {1},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-09-28 19:30:51},
  publisher            = {Taylor and Francis},
  timestamp            = {2020-02-27 03:51},
}

@Article{Biswas-Biswas-2017,
  author               = {Biswas, Anupam and Biswas, Bhaskar},
  date                 = {2017-04},
  journaltitle         = {Expert Systems with Applications},
  title                = {Defining quality metrics for graph clustering evaluation},
  doi                  = {10.1016/j.eswa.2016.11.011},
  issn                 = {0957-4174},
  pages                = {1--17},
  volume               = {71},
  abstract             = {Evaluation of clustering has significant importance in various applications of expert and intelligent systems. Clusters are evaluated in terms of quality and accuracy. Measuring quality is a unsupervised approach that completely depends on edges, whereas measuring accuracy is a supervised approach that measures similarity between the real clustering and the predicted clustering. Accuracy cannot be measured for most of the real-world networks since real clustering is unavailable. Thus, it will be advantageous from the viewpoint of expert systems to develop a quality metric that can assure certain level of accuracy along with the quality of clustering. In this paper we have proposed a set of three quality metrics for graph clustering that have the ability to ensure accuracy along with the quality. The effectiveness of the metrics has been evaluated on benchmark graphs as well as on real-world networks and compared with existing metrics. Results indicate competency of the suggested metrics while dealing with accuracy, which will definitely improve the decision-making in expert and intelligent systems. We have also shown that our metrics satisfy all of the six quality-related properties.},
  citeulike-article-id = {14447477},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.eswa.2016.11.011},
  posted-at            = {2017-10-08 14:20:33},
  timestamp            = {2020-02-27 03:51},
}

@Article{Blau-2019,
  author         = {Blau, Benjamin M.},
  date           = {2019-01-02},
  journaltitle   = {Journal of Behavioral Finance},
  title          = {Price clustering and investor sentiment},
  doi            = {10.1080/15427560.2018.1431887},
  issn           = {1542-7560},
  number         = {1},
  pages          = {19--30},
  urldate        = {2019-09-01},
  volume         = {20},
  abstract       = {Among the anomalous findings in the finance literature, perhaps the most persistent is the finding that security prices tend to cluster on round pricing increments. The author examines how investor sentiment influences the degree of price clustering. Both univariate and multivariate tests show a contemporaneous correlation between price clustering and investor sentiment. Recognizing the need to make stronger causal inferences, the author conducts 2 additional sets of tests. First, the author uses the technology bubble period as natural experiment and examine the price clustering of technology vis-a-vis nontechnology stocks. Results show that price clustering is markedly higher in tech stocks than in nontech stocks during this period of rising, sector-specific, investor sentiment. Second, the author estimates a vector autoregression process and examines the impulse responses of price clustering to exogenous shocks in investor sentiment. The results from these tests indicate that causation flows from sentiment to clustering instead of the other way around.},
  day            = {2},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:51},
}

@Article{Bnouachir-Mkhadri-2019,
  author         = {Bnouachir, Najla and Mkhadri, Abdallah},
  date           = {2019-06-03},
  journaltitle   = {Communications in Statistics - Simulation and Computation},
  title          = {Efficient cluster-based portfolio optimization},
  doi            = {10.1080/03610918.2019.1621341},
  issn           = {0361-0918},
  pages          = {1--15},
  urldate        = {2020-01-13},
  abstract       = {The sample mean and covariance matrix of historical data provide a disappointing out-of-sample performance in mean-variance portfolio rules. This poor performance is certainly due to the high estimation error incurred in the optimization model. Our purpose in this article is to find a method that enhances the out-of-sample performance of the portfolio weights. Using hierarchical clustering, we propose an alternative cluster-based portfolio to obtain a sequence of cluster assets. On the basis of Gram-Schmidt orthogonalization, the estimation risk of the data set becomes the sum of the estimations of the clusters in the sequence. The performance of our method and its competitors is compared empirically and via some simulations in high dimension.},
  day            = {3},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:51},
}

@Article{Bulo-Pelillo-2017,
  author               = {Bulo, Samuel Rota and Pelillo, Marcello},
  date                 = {2017-10},
  journaltitle         = {European Journal of Operational Research},
  title                = {Dominant-set clustering: A review},
  doi                  = {10.1016/j.ejor.2017.03.056},
  issn                 = {0377-2217},
  number               = {1},
  pages                = {1--13},
  volume               = {262},
  abstract             = {Clustering refers to the process of extracting maximally coherent groups from a set of objects using pairwise, or high-order, similarities. Traditional approaches to this problem are based on the idea of partitioning the input data into a predetermined number of classes, thereby obtaining the clusters as a by-product of the partitioning process. A radically different perspective of the problem consists in providing a formalization of the very notion of a cluster and considering the clustering process as a sequential search of structures in the data adhering to this cluster notion. In this manuscript we review one of the pioneering approaches falling in the latter class of algorithms, which has been proposed in the early 2000s and has been found since then a number of applications in different domains. It is known as dominant set clustering and provides a notion of a cluster (a.k.a. dominant set) that has intriguing links to game-theory, graph-theory and optimization theory. From the game-theoretic perspective, clusters are regarded as equilibria of non-cooperative "clustering" games; in the graph-theoretic context, it can be shown that they generalize the notion of maximal clique to edge-weighted graphs; finally, from an optimization point of view, they can be characterized in terms of solutions to a simplex-constrained, quadratic optimization problem, as well as in terms of an exquisitely combinatorial entity. Besides introducing dominant sets from a theoretical perspective, we will also focus on the related algorithmic issues by reviewing two state-of-the-art methods that are used in the literature to find dominant sets clusters, namely the Replicator Dynamics and the Infection and Immunization Dynamics. Finally, we conclude with an overview of different extensions of the dominant set framework and of applications where dominant sets have been successfully employed.},
  citeulike-article-id = {14500761},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2017.03.056},
  posted-at            = {2017-12-11 09:13:21},
  timestamp            = {2020-02-27 03:51},
}

@Article{Cai-et-al-2016,
  author               = {Cai, Fan and Le-Khac, Nhien-An and Kechadi, Tahar},
  date                 = {2016-09},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Clustering Approaches for Financial Data Analysis: a Survey},
  eprint               = {1609.08520},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1609.08520},
  abstract             = {Nowadays, financial data analysis is becoming increasingly important in the business market. As companies collect more and more data from daily operations, they expect to extract useful knowledge from existing collected data to help make reasonable decisions for new customer requests, e.g. user credit category, confidence of expected return, etc. Banking and financial institutes have applied different data mining techniques to enhance their business performance. Among these techniques, clustering has been considered as a significant method to capture the natural structure of data. However, there are not many studies on clustering approaches for financial data analysis. In this paper, we evaluate different clustering algorithms for analysing different financial datasets varied from time series to transactions. We also discuss the advantages and disadvantages of each method to enhance the understanding of inner structure of financial datasets as well as the capability of each clustering method in this context.},
  citeulike-article-id = {14148629},
  citeulike-linkout-0  = {http://arxiv.org/abs/1609.08520},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1609.08520},
  day                  = {4},
  groups               = {Networks and investment management, Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-09-28 21:25:38},
  timestamp            = {2020-02-27 03:51},
}

@Article{Cai-et-al-2017,
  author               = {Cai, Yumei and Cui, Xiaomei and Huang, Qianyun and Sun, Jianqiang},
  date                 = {2017-09},
  journaltitle         = {International Review of Economics \& Finance},
  title                = {Hierarchy, cluster, and time-stable information structure of correlations between international financial markets},
  doi                  = {10.1016/j.iref.2017.07.024},
  issn                 = {1059-0560},
  pages                = {562--573},
  volume               = {51},
  abstract             = {This paper investigates the correlations between 52 financial markets located in different countries or regions from July 2004 through June 2011. By using a correlation matrix time series and a participation frequency method based on the random matrix theory, we show that a time-stable information structure is contained in the correlations between global financial markets. We further find that the information structure is closely associated with global market and global geographical factors, and that each financial index's participation in the global market factor varies over time and presents dynamics. Two patterns, hierarchy and cluster effects, are found to be in the dynamics of the indices' participation in the global market factor. The cluster effect implies a more concentrated participation during the 2008 financial crisis.},
  citeulike-article-id = {14429814},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.iref.2017.07.024},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-09-13 09:33:44},
  timestamp            = {2020-02-27 03:51},
}

@Article{Carlsson-et-al-2016,
  author               = {Carlsson, Gunnar and Memoli, Facundo and Ribeiro, Alejandro and Segarra, Santiago},
  date                 = {2016-07},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Excisive Hierarchical Clustering Methods for Network Data},
  eprint               = {1607.06339},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1607.06339},
  abstract             = {We introduce two practical properties of hierarchical clustering methods for (possibly asymmetric) network data: excisiveness and linear scale preservation. The latter enforces imperviousness to change in units of measure whereas the former ensures local consistency of the clustering outcome. Algorithmically, excisiveness implies that we can reduce computational complexity by only clustering a data subset of interest while theoretically guaranteeing that the same hierarchical outcome would be observed when clustering the whole dataset. Moreover, we introduce the concept of representability, i.e. a generative model for describing clustering methods through the specification of their action on a collection of networks. We further show that, within a rich set of admissible methods, requiring representability is equivalent to requiring both excisiveness and linear scale preservation. Leveraging this equivalence, we show that all excisive and linear scale preserving methods can be factored into two steps: a transformation of the weights in the input network followed by the application of a canonical clustering method. Furthermore, their factorization can be used to show stability of excisive and linear scale preserving methods in the sense that a bounded perturbation in the input network entails a bounded perturbation in the clustering output.},
  citeulike-article-id = {14357923},
  citeulike-linkout-0  = {http://arxiv.org/abs/1607.06339},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1607.06339},
  day                  = {21},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-05-16 14:04:03},
  timestamp            = {2020-02-27 03:51},
}

@Article{Carlsson-et-al-2016a,
  author               = {Carlsson, Gunnar and Memoli, Facundo and Ribeiro, Alejandro and Segarra, Santiago},
  date                 = {2016-07},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Hierarchical Clustering of Asymmetric Networks},
  eprint               = {1607.06294},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1607.06294},
  abstract             = {This paper considers networks where relationships between nodes are represented by directed dissimilarities. The goal is to study methods that, based on the dissimilarity structure, output hierarchical clusters, i.e., a family of nested partitions indexed by a connectivity parameter. Our construction of hierarchical clustering methods is built around the concept of admissible methods, which are those that abide by the axioms of value - nodes in a network with two nodes are clustered together at the maximum of the two dissimilarities between them - and transformation - when dissimilarities are reduced, the network may become more clustered but not less. Two particular methods, termed reciprocal and nonreciprocal clustering, are shown to provide upper and lower bounds in the space of admissible methods. Furthermore, alternative clustering methodologies and axioms are considered. In particular, modifying the axiom of value such that clustering in two-node networks occurs at the minimum of the two dissimilarities entails the existence of a unique admissible clustering method.},
  citeulike-article-id = {14357925},
  citeulike-linkout-0  = {http://arxiv.org/abs/1607.06294},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1607.06294},
  day                  = {21},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-05-16 14:05:05},
  timestamp            = {2020-02-27 03:51},
}

@Article{Carlsson-et-al-2017,
  author               = {Carlsson, Gunnar and Memoli, Facundo and Ribeiro, Alejandro and Segarra, Santiago},
  date                 = {2017},
  journaltitle         = {IEEE Transactions on Signal and Information Processing over Networks},
  title                = {Admissible Hierarchical Clustering Methods and Algorithms for Asymmetric Networks},
  doi                  = {10.1109/tsipn.2017.2662622},
  issn                 = {2373-776X},
  pages                = {1},
  abstract             = {This paper characterizes hierarchical clustering methods that abide by two previously introduced axioms - thus, denominated admissible methods - and proposes tractable algorithms for their implementation. We leverage the fact that, for asymmetric networks, every admissible method must be contained between reciprocal and nonreciprocal clustering, and describe three families of intermediate methods. Grafting methods exchange branches between dendrograms generated by different admissible methods. The convex combination family combines admissible methods through a convex operation in the space of dendrograms, and thirdly, the semi-reciprocal family clusters nodes that are related by strong cyclic influences in the network. An algorithmic framework for the computation of hierarchical clusters generated by reciprocal and nonreciprocal clustering as well as the grafting, convex combination, and semi-reciprocal families is presented via matrix operations in a dioid algebra. Finally, the introduced clustering methods and algorithms are exemplified through their application to a network describing the interrelation between sectors of the United States (U.S.) economy.},
  citeulike-article-id = {14357921},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/tsipn.2017.2662622},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-05-16 14:02:52},
  timestamp            = {2020-02-27 03:51},
}

@Article{Chamroukhi-et-al-2013,
  author               = {Chamroukhi, Faicel and Same, Allou and Aknin, Patrice and Govaert, Gerard},
  date                 = {2013-12-25},
  journaltitle         = {Proceedings of the 2011 International Joint Conference on Neural Networks (IJCNN)},
  title                = {Model-based clustering with Hidden Markov Model regression for time series with regime changes},
  doi                  = {10.1109/ijcnn.2011.6033590},
  pages                = {2814--2821},
  abstract             = {This paper introduces a novel model-based clustering approach for clustering time series which present changes in regime. It consists of a mixture of polynomial regressions governed by hidden Markov chains. The underlying hidden process for each cluster activates successively several polynomial regimes during time. The parameter estimation is performed by the maximum likelihood method through a dedicated Expectation-Maximization (EM) algorithm. The proposed approach is evaluated using simulated time series and real-world time series issued from a railway diagnosis application. Comparisons with existing approaches for time series clustering, including the stand EM for Gaussian mixtures, K-means clustering, the standard mixture of regression models and mixture of Hidden Markov Models, demonstrate the effectiveness of the proposed approach.},
  booktitle            = {The 2011 International Joint Conference on Neural Networks},
  citeulike-article-id = {14510865},
  citeulike-linkout-0  = {http://arxiv.org/abs/1312.7024},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1312.7024},
  citeulike-linkout-2  = {http://dx.doi.org/10.1109/ijcnn.2011.6033590},
  day                  = {25},
  isbn                 = {978-1-4244-9635-8},
  location             = {San Jose, CA, USA},
  posted-at            = {2018-01-02 02:30:30},
  publisher            = {IEEE},
  timestamp            = {2020-02-27 03:51},
}

@Article{Chamroukhi-Nguyen-2019,
  author         = {Chamroukhi, Faicel and Nguyen, Hien D.},
  date           = {2019-01-18},
  journaltitle   = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  title          = {Model-based clustering and classification of functional data},
  doi            = {10.1002/widm.1298},
  issn           = {1942-4787},
  pages          = {e1298},
  urldate        = {2019-10-09},
  abstract       = {The problem of complex data analysis is a central topic of modern statistical science and learning systems and is becoming of broader interest with the increasing prevalence of high-dimensional data. The challenge is to develop statistical models and autonomous algorithms that are able to acquire knowledge from raw data for exploratory analysis, which can be achieved through clustering techniques or to make predictions of future data via classification (i.e., discriminant analysis) techniques. Latent data models, including mixture model-based approaches are one of the most popular and successful approaches in both the unsupervised context (i.e., clustering) and the supervised one (i.e, classification or discrimination). Although traditionally tools of multivariate analysis, they are growing in popularity when considered in the framework of functional data analysis (FDA). FDA is the data analysis paradigm in which the individual data units are functions (e.g., curves, surfaces), rather than simple vectors. In many areas of application, the analyzed data are indeed often available in the form of discretized values of functions or curves (e.g., time series, waveforms) and surfaces (e.g., 2d-images, spatio-temporal data). This functional aspect of the data adds additional difficulties compared to the case of a classical multivariate (non-functional) data analysis. We review and present approaches for model-based clustering and classification of functional data. We derive well-established statistical models along with efficient algorithmic tools to address problems regarding the clustering and the classification of these high-dimensional data, including their heterogeneity, missing information, and dynamical hidden structure. The presented models and algorithms are illustrated on real-world functional data analysis problems from several application area.},
  day            = {18},
  f1000-projects = {QuantInvest},
  groups         = {ML_ClustTimeSrs},
  timestamp      = {2020-02-27 03:51},
}

@Article{Charrad-et-al-2014,
  author               = {Charrad, Malika and Ghazzali, Nadia and Boiteau, Veronique and Niknafs, Azam},
  date                 = {2014},
  journaltitle         = {Journal of Statistical Software},
  title                = {NbClust: An R Package for Determining theRelevant Number of Clusters in a Data Set},
  abstract             = {Clustering is the partitioning of a set of objects into groups (clusters) so that objects within a group are more similar to each others than objects in dierent groups. Most of the clustering algorithms depend on some assumptions in order to dene the subgroups present in a data set. As a consequence, the resulting clustering scheme requires some sort of evaluation as regards its validity. The evaluation procedure has to tackle dicult problems such as the quality of clusters, the degree with which a clustering scheme ts a specic data set and the optimal number of clusters in a partitioning. In the literature, a wide variety of indices have been proposed to nd the optimal number of clusters in a partitioning of a data set during the clustering process. However, for most of indices proposed in the literature, programs are unavailable to test these indices and compare them. The R package NbClust has been developed for that purpose. It provides 30 indices which determine the number of clusters in a data set and it oers also the best clus- tering scheme from dierent results to the user. In addition, it provides a function to perform k-means and hierarchical clustering with dierent distance measures and aggre- gation methods. Any combination of validation indices and clustering methods can be requested in a single function call. This enables the user to simultaneously evaluate sev- eral clustering schemes while varying the number of clusters, to help determining the most appropriate number of clusters for the data set of interest.},
  citeulike-article-id = {14468583},
  posted-at            = {2017-10-29 20:23:51},
  timestamp            = {2020-02-27 03:51},
}

@Article{Chatziafratis-et-al-2018,
  author         = {Chatziafratis, Vaggos and Niazadeh, Rad and Charikar, Moses},
  date           = {2018-07-03},
  journaltitle   = {Proceedings of Machine Learning Research},
  title          = {Hierarchical Clustering with Structural Constraints},
  url            = {http://proceedings.mlr.press/v80/chatziafratis18a.html},
  urldate        = {2019-09-15},
  abstract       = {Hierarchical clustering is a popular unsupervised data analysis method. For many real-world applications, we would like to exploit prior information about the data that imposes constraints on the clustering hierarchy, and is not captured by the set of features available to the algorithm. This gives rise to the problem of hierarchical clustering with structural constraints. Structural constraints pose major challenges for bottom-up approaches like average/single linkage and even though they can be naturally incorporated into top-down divisive algorithms, no formal guarantees exist on the quality of their output. In this paper, we provide provable approximation guarantees for two simple top-down algorithms, using a recently introduced optimization viewpoint of hierarchical clustering with pairwise similarity information (Dasgupta, 2016). We show how to find good solutions even in the presence of conflicting prior information, by formulating a constraint-based regularization of the objective. Furthemore, we explore a variation of this objective for dissimilarity information (Cohen-Addad et al., 2018) and improve upon current techniques. Finally, we demonstrate our approach on a real dataset for the taxonomy application.},
  day            = {3},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:51},
}

@Article{Clemente-Grassi-2018,
  author               = {Clemente, Gian P. and Grassi, Rosanna},
  date                 = {2018-06-19},
  journaltitle         = {Chaos, Solitons \& Fractals},
  title                = {Directed clustering in weighted networks: a new perspective},
  doi                  = {10.1016/j.chaos.2017.12.007},
  eprint               = {1706.07322},
  eprinttype           = {arXiv},
  issn                 = {0960-0779},
  pages                = {26--38},
  volume               = {107},
  abstract             = {In this paper, we consider the problem of assessing local clustering in complex networks. Various definitions for this measure have been proposed for the cases of networks having weighted edges, but less attention has been paid to both weighted and directed networks. We provide a new local clustering coefficient for this kind of networks, starting from those existing in the literature for the weighted and undirected case. Furthermore, we extract from our coefficient four specific components, in order to separately consider different link patterns of triangles. Empirical applications on several real networks from different frameworks and with different order are provided. The performance of our coefficient is also compared with that of existing coefficients in the literature.},
  citeulike-article-id = {14515534},
  citeulike-linkout-0  = {http://arxiv.org/abs/1706.07322},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1706.07322},
  citeulike-linkout-2  = {http://dx.doi.org/10.1016/j.chaos.2017.12.007},
  day                  = {19},
  posted-at            = {2018-01-11 22:01:47},
  timestamp            = {2020-02-27 03:51},
}

@Article{Crawford-Milenkovic-2018,
  author         = {Crawford, Joseph and Milenkovic, Tijana},
  date           = {2018-05-08},
  journaltitle   = {PLOS ONE},
  title          = {ClueNet: Clustering a temporal network based on topological similarity rather than denseness.},
  doi            = {10.1371/journal.pone.0195993},
  number         = {5},
  pages          = {e0195993},
  volume         = {13},
  abstract       = {Network clustering is a very popular topic in the network science field. Its goal is to divide (partition) the network into groups (clusters or communities) of "topologically related" nodes, where the resulting topology-based clusters are expected to "correlate" well with node label information, i.e., metadata, such as cellular functions of genes/proteins in biological networks, or age or gender of people in social networks. Even for static data, the problem of network clustering is complex. For dynamic data, the problem is even more complex, due to an additional dimension of the data-their temporal (evolving) nature. Since the problem is computationally intractable, heuristic approaches need to be sought. Existing approaches for dynamic network clustering (DNC) have drawbacks. First, they assume that nodes should be in the same cluster if they are densely interconnected within the network. We hypothesize that in some applications, it might be of interest to cluster nodes that are topologically similar to each other instead of or in addition to requiring the nodes to be densely interconnected. Second, they ignore temporal information in their early steps, and when they do consider this information later on, they do so implicitly. We hypothesize that capturing temporal information earlier in the clustering process and doing so explicitly will improve results. We test these two hypotheses via our new approach called ClueNet. We evaluate ClueNet against six existing DNC methods on both social networks capturing evolving interactions between individuals (such as interactions between students in a high school) and biological networks capturing interactions between biomolecules in the cell at different ages. We find that ClueNet is superior in over 83\% of all evaluation tests. As more real-world dynamic data are becoming available, DNC and thus ClueNet will only continue to gain importance.},
  day            = {8},
  f1000-projects = {QuantInvest},
  pmcid          = {PMC5940177},
  pmid           = {29738568},
  timestamp      = {2020-02-27 03:51},
}

@Article{Dantas-Oliveira-2018,
  author         = {Dantas, Tiago Mendes and Oliveira, Fernando Luiz Cyrino},
  date           = {2018-10},
  journaltitle   = {International Journal of Forecasting},
  title          = {Improving time series forecasting: An approach combining bootstrap aggregation, clusters and exponential smoothing},
  doi            = {10.1016/j.ijforecast.2018.05.006},
  issn           = {0169-2070},
  number         = {4},
  pages          = {748--761},
  volume         = {34},
  abstract       = {Some recent papers have demonstrated that combining bagging (bootstrap aggregating) with exponential smoothing methods can produce highly accurate forecasts and improve the forecast accuracy relative to traditional methods. We therefore propose a new approach that combines the bagging, exponential smoothing and clustering methods. The existing methods use bagging to generate and aggregate groups of forecasts in order to reduce the variance. However, none of them consider the effect of covariance among the group of forecasts, even though it could have a dramatic impact on the variance of the group, and therefore on the forecast accuracy. The proposed approach, referred to here as Bagged Cluster ETS, aims to reduce the covariance effect by using partitioning around medoids (PAM) to produce clusters of similar forecasts, then selecting several forecasts from each cluster to create a group with a reduced variance. This approach was tested on various different time series sets from the M3 and CIF 2016 competitions. The empirical results have shown a substantial reduction in the forecast error, considering sMAPE and MASE.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs, Scenario_TimeSeries},
  timestamp      = {2020-02-27 03:56},
}

@Article{DeLuca-Zuccolotto-2016,
  author               = {{De Luca}, Giovanni and Zuccolotto, Paola},
  date                 = {2016-01},
  journaltitle         = {Statistics and Risk Modeling},
  title                = {A double clustering algorithm for financial time series based on extreme events},
  doi                  = {10.1515/strm-2015-0026},
  issn                 = {2193-1402},
  number               = {0},
  volume               = {0},
  abstract             = {This paper is concerned with a procedure for financial time series clustering, aimed at creating groups of time series characterized by similar behavior with regard to extreme events. The core of our proposal is a double clustering procedure: the former is based on the lower tail dependence of all the possible pairs of time series, the latter on the upper tail dependence. Tail dependence coefficients are estimated with copula functions. The final goal is to exploit the two clustering solutions in an algorithm designed to create a portfolio that maximizes the probability of joint positive extreme returns while minimizing the risk of joint negative extreme returns. In financial crisis scenarios, such a portfolio is expected to outperform portfolios generated by the traditional methods. We describe the results of a simulation study and, finally, we apply the procedure to a dataset composed of the 50 assets included in the EUROSTOXX index.},
  citeulike-article-id = {14150079},
  citeulike-linkout-0  = {http://dx.doi.org/10.1515/strm-2015-0026},
  day                  = {20},
  groups               = {Networks and investment management, [nbkcbu3:]},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:49:20},
  timestamp            = {2020-02-27 03:56},
}

@Article{dePrado-2020a,
  author         = {{de Prado}, Marcos Lopez},
  date           = {2020},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Clustering},
  doi            = {10.2139/ssrn.3512998},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3512998},
  urldate        = {2020-01-19},
  abstract       = {Many problems in finance require the clustering of variables or observations. Despite its usefulness, clustering is almost never taught in Econometrics courses. In this seminar we review two general clustering approaches: partitional and hierarchical.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Dias-et-al-2015,
  author       = {Dias, Jose G. and Vermunt, Jeroen K. and Ramos, Sofia},
  date         = {2015},
  journaltitle = {European Journal of Operational Research},
  title        = {Clustering financial time series: New insights from an extended hidden Markov model},
  doi          = {10.1016/j.ejor.2014.12.041},
  number       = {3},
  pages        = {852--864},
  url          = {https://www.sciencedirect.com/science/article/abs/pii/S0377221714010595},
  volume       = {243},
  abstract     = {In recent years, large amounts of financial data have become available for analysis. We propose exploring returns from 21 European stock markets by model-based clustering of regime switching models. These econometric models identify clusters of time series with similar dynamic patterns and moreover allow relaxing assumptions of existing approaches, such as the assumption of conditional Gaussian returns.

The proposed model handles simultaneously the heterogeneity across stock markets and over time, i.e., time-constant and time-varying discrete latent variables capture unobserved heterogeneity between and within stock markets, respectively. The results show a clear distinction between two groups of stock markets, each one characterized by different regime switching dynamics that correspond to different expected return-risk patterns.

We identify three regimes: the so-called bull and bear regimes, as well as a stable regime with returns close to 0, which turns out to be the most frequently occurring regime. This is consistent with stylized facts in financial econometrics.},
  groups       = {Networks and investment management, Clustering and network analysis, Scenario_TimeSeries},
  keywords     = {Data mining; Hidden Markov model; Stock indexes; Latent class model; Regime-switching model;},
  owner        = {zkgst0c},
  timestamp    = {2020-02-27 03:56},
}

@Article{DiLascio-et-al-2018,
  author         = {Di Lascio, F. Marta L. and Giammusso, Davide and Puccetti, Giovanni},
  date           = {2018-11},
  journaltitle   = {Journal of banking \& finance},
  title          = {A clustering approach and a rule of thumb for risk aggregation},
  doi            = {10.1016/j.jbankfin.2018.07.002},
  issn           = {0378-4266},
  pages          = {236--248},
  volume         = {96},
  abstract       = {Abstract The problem of establishing reliable estimates or bounds for the (T)VaR of a joint risk portfolio is a relevant subject in connection with the computation of total economic capital in the Basel regulatory framework for the finance sector as well as with the Solvency regulations for the insurance sector. In the computation of total economic capital, a financial institution faces a considerable amount of model uncertainty related to the estimation of the interdependence amongst the marginal risks. In this paper, we propose to apply a clustering procedure in order to partition a risk portfolio into independent subgroups of positively dependent risks. Based on available data, the portfolio partition so obtained can be statistically validated and allows for a reduction of capital and the corresponding model uncertainty. We illustrate the proposed methodology in a simulation study and two case studies considering an Operational and a Market Risk portfolio. A rule of thumb stems from the various examples proposed: in a mathematical model where the risk portfolio is split into independent subsets with comonotonic dependence within, the smallest VaR-based capital estimate (at the high regulatory probability levels typically used) is produced by assuming that the infinite-mean risks are comonotonic and the finite-mean risks are independent. The largest VaR estimate is instead generated by obtaining the maximum number of independent infinite-mean sums.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Ding-et-al-2017b,
  author         = {Ding, Jiajun and He, Xiongxiong and Yuan, Junqing and Jiang, Bo},
  date           = {2017-08-02},
  journaltitle   = {Soft Computing},
  title          = {Automatic clustering based on density peak detection using generalized extreme value distribution},
  doi            = {10.1007/s00500-017-2748-7},
  issn           = {1432-7643},
  number         = {9},
  pages          = {1--20},
  volume         = {22},
  abstract       = {Density peaks clustering (DPC) algorithm is able to get a satisfactory result with the help of artificial selecting the clustering centers, but such selection can be hard for a large amount of clustering tasks or the data set with a complex decision diagram. The purpose of this paper is to propose an automatic clustering approach without human intervention. Inspired by the visual selection rule of DPC, the judgment index which equals the lower value within density and distance (after normalization) is proposed for selecting the clustering centers. The judgment index approximately follows the generalized extreme value (GEV) distribution, and each clustering center judgment index is much higher. Hence, it is reasonable that the points are selected as clustering centers if their judgment indices are larger than the upper quantile of GEV. This proposed method is called density peaks clustering based on generalized extreme value distribution (DPC-GEV). Furthermore, taking the computational complexity into account, an alternative method based on density peak detection using Chebyshev inequality (DPC-CI) is also given. Experiments on both synthetic and real-world data sets show that DPC-GEV and DPC-CI can achieve the same accuracy as DPC on most data sets but consume much less time.},
  day            = {2},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Durante-et-al-2013,
  author               = {Durante, Fabrizio and Pappada, Roberta and Torelli, Nicola},
  date                 = {2013-12},
  journaltitle         = {Advances in Data Analysis and Classification},
  title                = {Clustering of financial time series in risky scenarios},
  doi                  = {10.1007/s11634-013-0160-4},
  issn                 = {1862-5347},
  number               = {4},
  pages                = {359--376},
  volume               = {8},
  abstract             = {A methodology is presented for clustering financial time series according to the association in the tail of their distribution. The procedure is based on the calculation of suitable pairwise conditional Spearman's correlation coefficients extracted from the series. The performance of the method has been tested via a simulation study. As an illustration, an analysis of the components of the Italian FTSE-MIB is presented. The results could be applied to construct financial portfolios that can manage to reduce the risk in case of simultaneous large losses in several markets.},
  citeulike-article-id = {14150074},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11634-013-0160-4},
  day                  = {22},
  groups               = {Networks and investment management, Scenario generation, Scenario_Market, Scenario_TimeSeries},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:45:04},
  timestamp            = {2020-02-27 03:56},
}

@Article{Durante-et-al-2015,
  author               = {Durante, Fabrizio and Pappada, Roberta and Torelli, Nicola},
  date                 = {2015},
  journaltitle         = {Statistical Papers},
  title                = {Clustering of time series via non-parametric tail dependence estimation},
  doi                  = {10.1007/s00362-014-0605-7},
  number               = {3},
  pages                = {701--721},
  volume               = {56},
  abstract             = {We present a procedure for clustering time series according to their tail dependence behaviour as measured via a suitable copula-based tail coefficient, estimated in a non-parametric way. Simulation results about the proposed methodology together with an application to financial data are presented showing the usefulness of the proposed approach.},
  citeulike-article-id = {14150076},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s00362-014-0605-7},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s00362-014-0605-7},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:45:35},
  publisher            = {Springer Berlin Heidelberg},
  timestamp            = {2020-02-27 03:56},
}

@InCollection{Durante-Pappada-2015,
  author               = {Durante, Fabrizio and Pappada, Roberta},
  booktitle            = {Strengthening Links Between Data Analysis and Soft Computing},
  date                 = {2015},
  title                = {Cluster Analysis of Time Series via Kendall Distribution},
  doi                  = {10.1007/978-3-319-10765-3\_25},
  editor               = {Grzegorzewski, Przemyslaw and Gagolewski, Marek and Hryniewicz, Olgierd and Gil, Mara},
  pages                = {209--216},
  publisher            = {Springer International Publishing},
  series               = {Advances in Intelligent Systems and Computing},
  volume               = {315},
  abstract             = {We present a method to cluster time series according to the calculation of the pairwise Kendall distribution function between them. A case study with environmental data illustrates the introduced methodology.},
  citeulike-article-id = {14150077},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-10765-325},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-10765-325},
  groups               = {Networks and investment management},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:46:12},
  timestamp            = {2020-02-27 03:56},
}

@InCollection{Durstewitz-2017e,
  author         = {Durstewitz, Daniel},
  booktitle      = {Advanced data analysis in neuroscience},
  date           = {2017},
  title          = {Clustering and density estimation},
  doi            = {10.1007/978-3-319-59976-2\_5},
  isbn           = {978-3-319-59974-8},
  pages          = {85--103},
  publisher      = {Springer International Publishing},
  series         = {Bernstein series in computational neuroscience},
  abstract       = {In classification approaches as described in Chap. 3, we have a training sample X with known class labels C, and we use this information either to estimate the conditional probabilities p(C = k), or to set up class boundaries (decision surfaces) by some other more direct criterion. In clustering we likewise assume that there is some underlying class structure in the data, just that we don know it and have no access to class labels C for our sample X, so that we have to infer it from X alone. This is also called an unsupervised statistical learning problem. In neurobiology this problem frequently occurs, for instance, when we suspect that neural cells in a brain area from their morphological and/or electrophysiological characteristics into different types, when gene sets cluster in functional pathways, when we believe that neural spiking patterns generated spontaneously in a given area are not arranged along a continuum but come from discrete categories (as possibly indicative of an attractor dynamics, see Chap. 9), or when rodents appear to utilize a discrete set of behavioral patterns or response strategies. In many such circumstances, we may feel that similarities between observations (observed feature sets) speak for an underlying mechanism that produces discrete types, but how could we extract such apparent structure and characterize it more formally? In fact, we may not just search for one such specific partition but may aim for a hierarchically nested set of partitions, that is, classes may split into subclasses and so on, as is the case with many natural categories and biological taxonomies. For instance, at a superordinate level, we may group cortical cell types into pyramidal cells and interneurons, which then in turn would split into several subclasses (like fast-spiking, bursting, etc.).},
  f1000-projects = {QuantInvest},
  issn           = {2520-{159X}},
  timestamp      = {2020-02-27 03:56},
}

@Article{Farrokhnia-Karimi-2016,
  author               = {Farrokhnia, Maryam and Karimi, Sadegh},
  date                 = {2016-01},
  journaltitle         = {Analytica Chimica Acta},
  title                = {Variable selection in multivariate calibration based on clustering of variable concept},
  doi                  = {10.1016/j.aca.2015.11.002},
  issn                 = {0003-2670},
  pages                = {70--81},
  volume               = {902},
  abstract             = {A new and efficient variable selection based on clustering of variable concept has been suggested for PLS. Selection the most useful variable is simple and straightforward. CLoVA concept can be used as alternative instead of using interval based variable selections for PLS. Analyses of different data sets indicate the superiority of CLoVA respect to available variable selection algorithms. Recently we have proposed a new variable selection algorithm, based on clustering of variable concept (CLoVA) in classification problem. With the same idea, this new concept has been applied to a regression problem and then the obtained results have been compared with conventional variable selection strategies for PLS. The basic idea behind the clustering of variable is that, the instrument channels are clustered into different clusters via clustering algorithms. Then, the spectral data of each cluster are subjected to PLS regression. Different real data sets (Cargill corn, Biscuit dough, ACE QSAR, Soy, and Tablet) have been used to evaluate the influence of the clustering of variables on the prediction performances of PLS. Almost in the all cases, the statistical parameter especially in prediction error shows the superiority of CLoVA-PLS respect to other variable selection strategies. Finally the synergy clustering of variable (sCLoVA-PLS), which is used the combination of cluster, has been proposed as an efficient and modification of CLoVA algorithm. The obtained statistical parameter indicates that variable clustering can split useful part from redundant ones, and then based on informative cluster; stable model can be reached.},
  citeulike-article-id = {14071205},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.aca.2015.11.002},
  owner                = {zkgst0c},
  posted-at            = {2016-06-17 22:22:46},
  timestamp            = {2020-02-27 03:56},
}

@Article{Fenn-et-al-2012,
  author               = {Fenn, Daniel J. and Porter, Mason A. and Mucha, Peter J. and McDonald, Mark and Williams, Stacy and Johnson, Neil F. and Jones, Nick S.},
  date                 = {2012-10-01},
  journaltitle         = {Quantitative Finance},
  title                = {Dynamical clustering of exchange rates},
  doi                  = {10.1080/14697688.2012.668288},
  number               = {10},
  pages                = {1493--1520},
  volume               = {12},
  abstract             = {We use techniques from network science to study correlations in the foreign exchange (FX) market during the period 1991?2008. We consider an FX market network in which each node represents an exchange rate and each weighted edge represents a time-dependent correlation between the rates. To provide insights into the clustering of the exchange-rate time series, we investigate dynamic communities in the network. We show that there is a relationship between an exchange rate's functional role within the market and its position within its community and use a node-centric community analysis to track the temporal dynamics of such roles. This reveals which exchange rates dominate the market at particular times and also identifies exchange rates that experienced significant changes in market role. We also use the community dynamics to uncover major structural changes that occurred in the FX market. Our techniques are general and will be similarly useful for investigating correlations in other markets.},
  citeulike-article-id = {14460851},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2012.668288},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2012.668288},
  day                  = {1},
  posted-at            = {2017-10-19 00:29:20},
  publisher            = {Routledge},
  timestamp            = {2020-02-27 03:56},
}

@Article{Ferraro-et-al-2019,
  author    = {Maria Brigida Ferraro and Paolo Giordani and Alessio Serafini},
  title     = {{fclust: An R Package for Fuzzy Clustering}},
  doi       = {10.32614/RJ-2019-017},
  url       = {https://doi.org/10.32614/RJ-2019-017},
  abstract  = {Fuzzy clustering methods discover fuzzy partitions where observations can be softly assigned to more than one cluster. The package fclust is a toolbox for fuzzy clustering in the R programming language. It not only implements the widely used fuzzy k-means (FkM) algorithm, but also many FkM variants. Fuzzy cluster similarity measures, cluster validity indices and cluster visualization tools are also offered. In the current version, all the functions are rewritten in the C++ language allowing their application in large-size problems. Moreover, new fuzzy relational clustering algorithms for partitioning qualitative/mixed data are provided together with an improved version of the so-called Gustafson-Kessel algorithm to avoid singularity in the cluster covariance matrices. Finally, it is now possible to automatically select the number of clusters by means of the available fuzzy cluster validity indices.},
  journal   = {{The R Journal}},
  timestamp = {2020-02-27 03:56},
  year      = {2019},
}

@Article{Ferreira-Zhao-2015,
  author               = {Ferreira, Leonardo N. and Zhao, Liang},
  date                 = {2015-08-19},
  journaltitle         = {Information Sciences},
  title                = {Time Series Clustering via Community Detection in Networks},
  doi                  = {10.1016/j.ins.2015.07.046},
  issn                 = {0020-0255},
  pages                = {227--242},
  volume               = {326},
  abstract             = {In this paper, we propose a technique for time series clustering using community detection in complex networks. Firstly, we present a method to transform a set of time series into a network using different distance functions, where each time series is represented by a vertex and the most similar ones are connected. Then, we apply community detection algorithms to identify groups of strongly connected vertices (called a community) and, consequently, identify time series clusters. Still in this paper, we make a comprehensive analysis on the influence of various combinations of time series distance functions, network generation methods and community detection techniques on clustering results. Experimental study shows that the proposed network-based approach achieves better results than various classic or up-to-date clustering techniques under consideration. Statistical tests confirm that the proposed method outperforms some classic clustering algorithms, such as k-medoids, diana, median-linkage and centroid-linkage in various data sets. Interestingly, the proposed method can effectively detect shape patterns presented in time series due to the topological structure of the underlying network constructed in the clustering process. At the same time, other techniques fail to identify such patterns. Moreover, the proposed method is robust enough to group time series presenting similar pattern but with time shifts and/or amplitude variations. In summary, the main point of the proposed method is the transformation of time series from time-space domain to topological domain. Therefore, we hope that our approach contributes not only for time series clustering, but also for general time series analysis tasks.},
  citeulike-article-id = {14042013},
  citeulike-linkout-0  = {http://arxiv.org/abs/1508.04757},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1508.04757},
  citeulike-linkout-2  = {http://dx.doi.org/10.1016/j.ins.2015.07.046},
  day                  = {19},
  posted-at            = {2018-01-02 02:20:04},
  timestamp            = {2020-02-27 03:56},
}

@Article{Fop-Murphy-2017,
  author         = {Fop, Michael and Murphy, Thomas Brendan},
  date           = {2017-07-02},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Variable Selection Methods for Model-based Clustering},
  url            = {https://arxiv.org/abs/1707.00306},
  abstract       = {Model-based clustering is a popular approach for clustering multivariate data which has seen applications in numerous fields. Nowadays, high-dimensional data are more and more common and the model-based clustering approach has adapted to deal with the increasing dimensionality. In particular, the development of variable selection techniques has received a lot of attention and research effort in recent years. Even for small size problems, variable selection has been advocated to facilitate the interpretation of the clustering results. This review provides a summary of the methods developed for variable selection in model-based clustering. Existing R packages implementing the different methods are indicated and illustrated in application to two data analysis examples.},
  day            = {2},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Fragkiskos-Bauman-2018,
  author               = {Fragkiskos, Apollon and Bauman, Evgeny},
  date                 = {2018},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Factor Based Clustering},
  url                  = {https://ssrn.com/abstract=3089985},
  abstract             = {We propose a novel approach to cluster funds based on their factor exposures. The approach uses investment returns as input data and calculates similarity scores across funds, which are then used to form clusters. The derived clusters avoid common pitfalls that correlation based or other cluster methods fall into. They can be used as peer group alternatives to what vendors provide or to further refine existing categories that might be too obscure to make sense of. When tested against long/short equity funds, we find that we can form clusters with relatively high levels of stability across time.},
  citeulike-article-id = {14516008},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3089985},
  groups               = {Invest_Network},
  posted-at            = {2018-01-12 20:49:21},
  timestamp            = {2020-02-27 03:56},
}

@InCollection{Gagolewski-et-al-2016a,
  author               = {Gagolewski, Marek and Cena, Anna and Bartoszuk, Maciej},
  booktitle            = {Modeling Decisions for Artificial Intelligence},
  date                 = {2016},
  title                = {Hierarchical Clustering via Penalty-Based Aggregation and the Genie Approach},
  doi                  = {10.1007/978-3-319-45656-0\_16},
  editor               = {Torra, Vicenc and Narukawa, Yasuo and Navarro-Arribas, Guillermo and Yanez, Cristina},
  pages                = {191--202},
  publisher            = {Springer International Publishing},
  series               = {Lecture Notes in Computer Science},
  volume               = {9880},
  abstract             = {The paper discusses a generalization of the nearest centroid hierarchical clustering algorithm. A first extension deals with the incorporation of generic distance-based penalty minimizers instead of the classical aggregation by means of centroids. Due to that the presented algorithm can be applied in spaces equipped with an arbitrary dissimilarity measure (images, DNA sequences, etc.). Secondly, a correction preventing the formation of clusters of too highly unbalanced sizes is applied: just like in the recently introduced Genie approach, which extends the single linkage scheme, the new method averts a chosen inequity measure (e.g., the Gini-, de Vergottini-, or Bonferroni-index) of cluster sizes from raising above a predefined threshold. Numerous benchmarks indicate that the introduction of such a correction increases the quality of the resulting clusterings significantly.},
  citeulike-article-id = {14468579},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-45656-016},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-45656-016},
  posted-at            = {2017-10-29 19:59:00},
  timestamp            = {2020-02-27 03:56},
}

@Article{Gan-2013,
  author               = {Gan, Guojun},
  date                 = {2013-11},
  journaltitle         = {Insurance: Mathematics and Economics},
  title                = {Application of data clustering and machine learning in variable annuity valuation},
  doi                  = {10.1016/j.insmatheco.2013.09.021},
  issn                 = {0167-6687},
  number               = {3},
  pages                = {795--801},
  volume               = {53},
  abstract             = {We study the pricing of a large portfolio of VA policies. A clustering method is used to select representative policies. A machine learning method is used to estimate the guarantee value. The proposed method performs well in terms of accuracy and speed. The valuation of variable annuity guarantees has been studied extensively in the past four decades. However, almost all the studies focus on the valuation of guarantees embedded in a single variable annuity contract. How to efficiently price the guarantees for a large portfolio of variable annuity contracts has not received enough attention. This paper fills the gap by introducing a novel method based on data clustering and machine learning to price the guarantees for a large portfolio of variable annuity contracts. Our test results show that this method performs very well in terms of accuracy and speed.},
  citeulike-article-id = {13934351},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.insmatheco.2013.09.021},
  groups               = {Networks and investment management, Machine learning and investment strategies, Annuities, ML_InvestSelect},
  owner                = {cristi},
  posted-at            = {2016-03-29 11:15:25},
  timestamp            = {2020-02-27 03:56},
}

@Article{Garvey-Madhavan-2019,
  author         = {Garvey, Gerald and Madhavan, Ananth},
  date           = {2019-09-09},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {Reconstructing Emerging and Developed Markets Using Hierarchical Clustering},
  url            = {https://jfds.pm-research.com/content/early/2019/09/08/jfds.2019.1.014},
  urldate        = {2019-09-10},
  day            = {9},
  f1000-projects = {QuantInvest},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-02-27 03:56},
}

@Article{Gates-Ahn-2017,
  author               = {Gates, Alexander J. and Ahn, Yong-Yeol},
  date                 = {2017-01},
  journaltitle         = {arXiv Electronic Journal},
  title                = {The Impact of Random Models on Clustering Similarity},
  eprint               = {1701.06508},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1701.06508},
  abstract             = {Clustering is a central approach for unsupervised learning. After clustering is applied, the most fundamental analysis is to quantitatively compare clusterings. Such comparisons are crucial for the evaluation of clustering methods as well as other tasks such as consensus clustering. It is often argued that, in order to establish a baseline, clustering similarity should be assessed in the context of a random ensemble of clusterings. The prevailing assumption for the random clustering ensemble is the permutation model in which the number and sizes of clusters are fixed. However, this assumption does not necessarily hold in practice; for example, while multiple runs of K-means clustering returns clusterings with a fixed number of clusters, the cluster size distribution varies greatly. Here, we derive corrected variants of two clustering similarity measures (the Rand index and Mutual Information) in the context of two random clustering ensembles in which the number and sizes of clusters vary. In addition, we study the impact of one-sided comparisons in the scenario with a reference clustering. The consequences of different random models are illustrated using synthetic examples, handwriting recognition, and gene expression data. We demonstrate that the choice of random model can have a drastic impact on results and argue that the choice should be carefully justified.},
  citeulike-article-id = {14262970},
  citeulike-linkout-0  = {http://arxiv.org/abs/1701.06508},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1701.06508},
  day                  = {23},
  posted-at            = {2017-01-26 15:41:14},
  timestamp            = {2020-02-27 03:56},
}

@Article{Ghoshdastidar-et-al-2018,
  author         = {Ghoshdastidar, Debarghya and Perrot, Michael and von Luxburg, Ulrike},
  date           = {2018-11-02},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Foundations of Comparison-Based Hierarchical Clustering},
  url            = {https://arxiv.org/abs/1811.00928},
  urldate        = {2019-12-18},
  abstract       = {We address the classical problem of hierarchical clustering, but in a framework where one does not have access to a representation of the objects or their pairwise similarities. Instead, we assume that only a set of comparisons between objects is available, that is, statements of the form "objects ii and jj are more similar than objects kk and ll." Such a scenario is commonly encountered in crowdsourcing applications. The focus of this work is to develop comparison-based hierarchical clustering algorithms that do not rely on the principles of ordinal embedding. We show that single and complete linkage are inherently comparison-based and we develop variants of average linkage. We provide statistical guarantees for the different methods under a planted hierarchical partition model. We also empirically demonstrate the performance of the proposed approaches on several datasets.},
  day            = {2},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Goswami-et-al-2017,
  author         = {Goswami, Saptarsi and Das, Amit Kumar and Chakrabarti, Amlan and Chakraborty, Basabi},
  date           = {2017-08},
  journaltitle   = {Expert Systems with Applications},
  title          = {A feature cluster taxonomy based feature selection technique},
  doi            = {10.1016/j.eswa.2017.01.044},
  issn           = {0957-4174},
  pages          = {76--89},
  volume         = {79},
  abstract       = {Feature subset selection is basically an optimization problem for choosing the most important features from various alternatives in order to facilitate classification or mining problems. Though lots of algorithms have been developed so far, none is considered to be the best for all situations and researchers are still trying to come up with better solutions. In this work, a flexible and user-guided feature subset selection algorithm, named as FCTFS (Feature Cluster Taxonomy based Feature Selection) has been proposed for selecting suitable feature subset from a large feature set. The proposed algorithm falls under the genre of clustering based feature selection techniques in which features are initially clustered according to their intrinsic characteristics following the filter approach. In the second step the most suitable feature is selected from each cluster to form the final subset following a wrapper approach. The two stage hybrid process lowers the computational cost of subset selection, especially for large feature data sets. One of the main novelty of the proposed approach lies in the process of determining optimal number of feature clusters. Unlike currently available methods, which mostly employ a trial and error approach, the proposed method characterises and quantifies the feature clusters according to the quality of the features inside the clusters and defines a taxonomy of the feature clusters. The selection of individual features from a feature cluster can be done judiciously considering both the relevancy and redundancy according to user intention and requirement. The algorithm has been verified by simulation experiments with different bench mark data set containing features ranging from 10 to more than 800 and compared with other currently used feature selection algorithms. The simulation results prove the superiority of our proposal in terms of model performance, flexibility of use in practical problems and extendibility to large feature sets. Though the current proposal is verified in the domain of unsupervised classification, it can be easily used in case of supervised classification.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Grun-2018,
  author         = {Grun, Bettina},
  date           = {2018-07-05},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Model-based Clustering},
  url            = {https://arxiv.org/abs/1807.01987},
  abstract       = {Mixture models extend the toolbox of clustering methods available to the data analyst. They allow for an explicit definition of the cluster shapes and structure within a probabilistic framework and exploit estimation and inference techniques available for statistical models in general. In this chapter an introduction to cluster analysis is provided, model-based clustering is related to standard heuristic clustering methods and an overview on different ways to specify the cluster model is given. Post-processing methods to determine a suitable clustering, infer cluster distribution characteristics and validate the cluster solution are discussed. The versatility of the model-based clustering approach is illustrated by giving an overview on the different areas of applications.},
  day            = {5},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Guenther-Lee-2016,
  author               = {Guenther, John and Lee, Herbert K. H.},
  date                 = {2016},
  journaltitle         = {Applied Mathematics},
  title                = {Cluster Search Algorithm for Finding Multiple Optima},
  doi                  = {10.4236/am.2016.77067},
  issn                 = {2152-7385},
  number               = {07},
  pages                = {736--752},
  volume               = {07},
  abstract             = {The black box functions found in computer experiments often result in multimodal optimization programs. Optimization that focuses on a single best optimum may not achieve the goal of getting the best answer for the purposes of the experiment. This paper builds upon an algorithm introduced in [1] that is successful for finding multiple optima within the input space of the objective function. Here we introduce an alternative cluster search algorithm for finding these optima, making use of clustering. The cluster search algorithm has several advantages over the earlier algorithm. It gives a forward view of the optima that are present in the input space so the user has a preview of what to expect as the optimization process continues. It employs pattern search, in many instances, closer to the minimum's location in input space, saving on simulator point computations. At termination, this algorithm does not need additional verification that a minimum is a duplicate of a previously found minimum, which also saves on simulator point computations. Finally, it finds minima that can be "hidden" by close larger minima.},
  citeulike-article-id = {14497899},
  citeulike-linkout-0  = {http://dx.doi.org/10.4236/am.2016.77067},
  posted-at            = {2017-12-06 19:43:47},
  timestamp            = {2020-02-27 03:56},
}

@Article{GuijoRubio-et-al-2018,
  author         = {Guijo-Rubio, David and Duran-Rosal, Antonio Manuel and Gutierrez, Pedro Antonio and Troncoso, Alicia and Hervas-Martinez, Cesar},
  date           = {2018-10-27},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Time series clustering based on the characterisation of segment typologies},
  url            = {https://arxiv.org/abs/1810.11624},
  abstract       = {Time series clustering is the process of grouping time series with respect to their similarity or characteristics. Previous approaches usually combine a specific distance measure for time series and a standard clustering method. However, these approaches do not take the similarity of the different subsequences of each time series into account, which can be used to better compare the time series objects of the dataset. In this paper, we propose a novel technique of time series clustering based on two clustering stages. In a first step, a least squares polynomial segmentation procedure is applied to each time series, which is based on a growing window technique that returns different-length segments. Then, all the segments are projected into same dimensional space, based on the coefficients of the model that approximates the segment and a set of statistical features. After mapping, a first hierarchical clustering phase is applied to all mapped segments, returning groups of segments for each time series. These clusters are used to represent all time series in the same dimensional space, after defining another specific mapping process. In a second and final clustering stage, all the time series objects are grouped. We consider internal clustering quality to automatically adjust the main parameter of the algorithm, which is an error threshold for the segmenta- tion. The results obtained on 84 datasets from the UCR Time Series Classification Archive have been compared against two state-of-the-art methods, showing that the performance of this methodology is very promising.},
  day            = {27},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Gu-Louca-2019,
  author         = {Gu, Ariel and Louca, Christodoulos},
  date           = {2019},
  journaltitle   = {SSRN Electronic Journal},
  title          = {What drives new mutual fund clustering?},
  doi            = {10.2139/ssrn.3409491},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3409491},
  urldate        = {2019-11-28},
  abstract       = {Over time, the aggregate new mutual fund volume is considerably larger in markets. The naive explanation that new fund volume correlates with the economic environment is incomplete because active mutual funds, on average, underperform. However, we demonstrate that fund families exploit IPO-related investment opportunities, which correlate cross-sectionally for economy-wide reasons, more by creating new funds than by using existing funds. This naturally leads to new fund volume clustering that is correlated with the economic environment. In addition, consistent with fund families strategically exploiting the economic environment, new funds with access to IPO offerings outperform and attract higher investment flows.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@InCollection{Gupta-Chatterjee-2018,
  author               = {Gupta, Kartikay and Chatterjee, Niladri},
  booktitle            = {Information and Communication Technology for Intelligent Systems (ICTIS 2017) - Volume 2},
  date                 = {2018},
  title                = {Financial Time Series Clustering},
  doi                  = {10.1007/978-3-319-63645-0\_16},
  editor               = {Satapathy, Suresh C. and Joshi, Amit},
  pages                = {146--156},
  publisher            = {Springer International Publishing},
  series               = {Smart Innovation, Systems and Technologies},
  volume               = {84},
  abstract             = {Financial time series clustering finds application in forecasting, noise reduction and enhanced index tracking. The central theme in all the available clustering algorithms is the dissimilarity measure employed by the algorithm. The dissimilarity measures, applicable in financial domain, as used or suggested in past researches, are correlation based dissimilarity measure, temporal correlation based dissimilarity measure and dynamic time wrapping (DTW) based dissimilarity measure. One shortcoming of these dissimilarity measures is that they do not take into account the lead or lag existing between the returns of different stocks which changes with time. Mostly, such stocks with high value of correlation at some lead or lag belong to the same cluster (or sector). The present paper, proposes two new dissimilarity measures which show superior clustering results as compared to past measures when compared over 3 data sets comprising of 526 companies.},
  citeulike-article-id = {14435138},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-63645-016},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-63645-016},
  groups               = {FrcstQWIM_TimeSrs},
  posted-at            = {2017-09-21 00:23:24},
  timestamp            = {2020-02-27 03:56},
}

@Article{Han-Ge-2020,
  author         = {Han, Jingti and Ge, Zhipeng},
  date           = {2020-01},
  journaltitle   = {Expert systems with applications},
  title          = {Effect of dimensionality reduction on stock selection with cluster analysis in different market situations},
  doi            = {10.1016/j.eswa.2020.113226},
  issn           = {0957-4174},
  pages          = {113226},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/{S095741742030052X}},
  urldate        = {2020-01-23},
  abstract       = {Dimensionality reduction is inevitable in stock selection with cluster analysis. Considering relations among dimensionality reduction, noise trading, and market situations, we empirically investigate the effect of dimensionality-reduction methods-principal component analysis, stacked autoencoder, and stacked restricted Boltzmann machine-on stock selection with cluster analysis in different market situations. Based on the index fluctuation, the market is divided into sideways and trend situations. For the CSI 100 and Nikkei 225 constituent stocks, experimental results show that: (1) in sideways situations, dimensionality reduction hardly improves the performance of stock selection with cluster analysis; (2) the advantage of dimensionality reduction is mainly reflected in trend situations, but whether it is in an up or down trend depends on the market analyzed. More importantly, according to the above findings and assuming that the dimensionality-reduction effect will continue, we propose a rotation strategy with and without dimensionality reduction. The results of experiments show that the proposed rotation strategy outperforms the stock market indices as well as the stock-selection strategies based on dimensionality reduction and cluster analysis. These findings offer practical insights into how dimensionality reduction can be efficiently used for stock selection.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Hennig-et-al-2018,
  author         = {Hennig, Christian and Viroli, Cinzia and Anderlucci, Laura},
  date           = {2018-06-27},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Quantile-based clustering},
  url            = {https://arxiv.org/abs/1806.10403},
  abstract       = {A new cluster analysis method, K-quantiles clustering, is introduced. K-quantiles clustering can be computed by a simple greedy algorithm in the style of the classical Lloyd's algorithm for K-means. It can be applied to large and high-dimensional datasets. It allows for within-cluster skewness and internal variable scaling based on within-cluster variation. Different versions allow for different levels of parsimony and computational efficiency. Although K-quantiles clustering is conceived as nonparametric, it can be connected to a fixed partition model of generalized asymmetric Laplace-distributions. The consistency of K-quantiles clustering is proved, and it is shown that K-quantiles clusters correspond to well separated mixture components in a nonparametric mixture. In a simulation, K-quantiles clustering is compared with a number of popular clustering methods with good results. A high-dimensional microarray dataset is clustered by K-quantiles.},
  day            = {27},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Hofmeyr-2017,
  author               = {Hofmeyr, David P.},
  date                 = {2017-08-01},
  journaltitle         = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title                = {Clustering by Minimum Cut Hyperplanes},
  doi                  = {10.1109/tpami.2016.2609929},
  issn                 = {0162-8828},
  number               = {8},
  pages                = {1547--1560},
  volume               = {39},
  abstract             = {Minimum normalised graph cuts are highly effective ways of partitioning unlabeled data, having been made popular by the success of spectral clustering. This work presents a novel method for learning hyperplane separators which minimise this graph cut objective, when data are embedded in Euclidean space. The optimisation problem associated with the proposed method can be formulated as a sequence of univariate subproblems, in which the optimal hyperplane orthogonal to a given vector is determined. These subproblems can be solved in log-linear time, by exploiting the trivial factorisation of the exponential function. Experimentation suggests that the empirical runtime of the overall algorithm is also log-linear in the number of data. Asymptotic properties of the minimum cut hyperplane, both for a finite sample, and for an increasing sample assumed to arise from an underlying probability distribution are discussed. In the finite sample case the minimum cut hyperplane converges to the maximum margin hyperplane as the scaling parameter is reduced to zero. Applying the proposed methodology, both for fixed scaling, and the large margin asymptotes, is shown to produce high quality clustering models in comparison with state-of-the-art clustering algorithms in experiments using a large collection of benchmark datasets.},
  citeulike-article-id = {14486419},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/tpami.2016.2609929},
  day                  = {1},
  posted-at            = {2017-11-30 19:03:59},
  timestamp            = {2020-02-27 03:56},
}

@InCollection{Hofmeyr-Pavlidis-2015,
  author               = {Hofmeyr, David and Pavlidis, Nicos},
  booktitle            = {IEEE Symposium Series on Computational Intelligence},
  date                 = {2015-12},
  title                = {Maximum Clusterability Divisive Clustering},
  doi                  = {10.1109/ssci.2015.116},
  isbn                 = {978-1-4799-7560-0},
  location             = {Cape Town, South Africa},
  pages                = {780--786},
  publisher            = {IEEE},
  abstract             = {The notion of cluster ability is often used to determine how strong the cluster structure within a set of data is, as well as to assess the quality of a clustering model. In multivariate applications, however, the cluster ability of a data set can be obscured by irrelevant or noisy features. We study the problem of finding low dimensional projections which maximise the cluster ability of a data set. In particular, we seek low dimensional representations of the data which maximise the quality of a binary partition. We use this bi-partitioning recursively to generate high quality clustering models. We illustrate the improvement over standard dimension reduction and clustering techniques, and evaluate our method in experiments on real and simulated data sets.},
  citeulike-article-id = {14486417},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/ssci.2015.116},
  posted-at            = {2017-11-30 19:02:58},
  timestamp            = {2020-02-27 03:56},
}

@InProceedings{Holst-et-al-2019,
  author         = {Holst, Anders and Bae, Juhee and Karlsson, Alexander and Bouguelia, Mohamed-Rafik},
  booktitle      = {Proceedings of the Workshop on Interactive Data Mining - WIDM'19},
  date           = {2019-02-15},
  title          = {Interactive clustering for exploring multiple data streams at different time scales and granularity},
  doi            = {10.1145/3304079.3310286},
  isbn           = {9781450362962},
  location       = {New York, New York, USA},
  pages          = {1--7},
  publisher      = {ACM Press},
  url            = {http://dl.acm.org/citation.cfm?doid=3304079.3310286},
  urldate        = {2019-09-06},
  abstract       = {We approach the problem of identifying and interpreting clusters over different time scales and granularity in multivariate time series data. We extract statistical features over a sliding window of each time series, and then use a Gaussian mixture model to identify clusters which are then projected back on the data streams. The human analyst can then further analyze this projection and adjust the size of the sliding window and the number of clusters in order to capture the different types of clusters over different time scales. We demonstrate the effectiveness of our approach in two different application scenarios: (1) fleet management and (2) district heating, wherein each scenario, several different types of meaningful clusters can be identified when varying over these dimensions.},
  day            = {15},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Hong-et-al-2017,
  author               = {Hong, Dezhi and Gu, Quanquan and Whitehouse, Kamin},
  date                 = {2017},
  journaltitle         = {Proceedings of Machine Learning Research},
  title                = {High-dimensional Time Series Clustering via Cross-Predictability},
  url                  = {http://proceedings.mlr.press/v54/hong17a.html},
  abstract             = {The key to time series clustering is how to characterize the similarity between any two time series. In this paper, we explore a new similarity metric called "cross-predictability": the degree to which a future value in each time series is predicted by past values of the others. However, it is challenging to estimate such cross-predictability among time series in the high-dimensional regime, where the number of time series is much larger than the length of each time series. We address this challenge with a sparsity assumption: only time series in the same cluster have significant cross-predictability with each other. We demonstrate that this approach is computationally attractive, and provide a theoretical proof that the proposed algorithm will identify the correct clustering structure with high probability under certain conditions. To the best of our knowledge, this is the first practical high-dimensional time series clustering algorithm with a provable guarantee. We evaluate with experiments on both synthetic data and real-world data, and results indicate that our method can achieve more than 80\% clustering accuracy on real-world data, which is 20\% higher than the state-of-art baselines.},
  citeulike-article-id = {14435135},
  groups               = {Predictability_FinInfo, ML_ClustTimeSrs},
  posted-at            = {2017-09-20 23:56:17},
  timestamp            = {2020-02-27 03:56},
}

@Article{Horel-et-al-2019,
  author         = {Horel, Enguerrand and Giesecke, Kay and Storchan, Victor and Chittar, Naren},
  date           = {2019},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Explainable Clustering and Application to Wealth Management Compliance},
  url            = {https://arxiv.org/abs/1909.13381},
  urldate        = {2019-09-07},
  abstract       = {Many applications from the financial industry successfully leverage clustering algorithms to reveal meaningful patterns among a vast amount of unstructured financial data. However, these algorithms suffer from a lack of interpretability that is required both at a business and regulatory level. In order to overcome this issue, we propose a novel two-steps method to explain clusters. A classifier is first trained to predict the clusters labels, then the Single Feature Introduction Test (SFIT) method is run on the model to identify the statistically significant features that characterise each cluster. We describe a real wealth management compliance use-case that highlights the necessity of such an interpretable clustering method. We illustrate the performance of our method through an experiment on financial ratios of U.S. companies.},
  day            = {23},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 03:56},
}

@Article{Huang-et-al-2013,
  author               = {Huang, Hanwen and Liu, Yufeng and Yuan, Ming and Marron, J. S.},
  date                 = {2013},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Statistical Significance of Clustering using Soft Thresholding},
  eprint               = {1305.5879},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1305.5879},
  abstract             = {Clustering methods have led to a number of important discoveries in bioinformatics and beyond. A major challenge in their use is determining which clusters represent important underlying structure, as opposed to spurious sampling artifacts. This challenge is especially serious, and very few methods are available when the data are very high in dimension. Statistical Significance of Clustering (SigClust) is a recently developed cluster evaluation tool for high dimensional low sample size data. An important component of the SigClust approach is the very definition of a single cluster as a subset of data sampled from a multivariate Gaussian distribution. The implementation of SigClust requires the estimation of the eigenvalues of the covariance matrix for the null multivariate Gaussian distribution. We show that the original eigenvalue estimation can lead to a test that suffers from severe inflation of type-I error, in the important case where there are huge single spikes in the eigenvalues. This paper addresses this critical challenge using a novel likelihood based soft thresholding approach to estimate these eigenvalues which leads to a much improved SigClust. These major improvements in SigClust performance are shown by both theoretical work and an extensive simulation study. Applications to some cancer genomic data further demonstrate the usefulness of these improvements.},
  citeulike-article-id = {14444684},
  citeulike-linkout-0  = {http://arxiv.org/abs/1305.5879},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1305.5879},
  day                  = {29},
  posted-at            = {2017-10-03 17:02:54},
  timestamp            = {2020-02-27 03:56},
  year                 = {2013},
}

@Article{Huang-et-al-2017a,
  author               = {Huang, Jinlong and Zhu, Qingsheng and Yang, Lijun and Cheng, Dongdong and Wu, Quanwang},
  date                 = {2017},
  journaltitle         = {Machine Learning},
  title                = {QCC: a novel clustering algorithm based on Quasi-Cluster Centers},
  doi                  = {10.1007/s10994-016-5608-2},
  number               = {3},
  pages                = {337--357},
  volume               = {106},
  abstract             = {Cluster analysis aims at classifying objects into categories on the basis of their similarity and has been widely used in many areas such as pattern recognition and image processing. In this paper, we propose a novel clustering algorithm called QCC mainly based on the following ideas: the density of a cluster center is the highest in its K nearest neighborhood or reverse K nearest neighborhood, and clusters are divided by sparse regions. Besides, we define a novel concept of similarity between clusters to solve the complex-manifold problem. In experiments, we compare the proposed algorithm QCC with DBSCAN, DP and DAAP algorithms on synthetic and real-world datasets. Results show that QCC performs the best, and its superiority on clustering non-spherical data and complex-manifold data is especially large.},
  citeulike-article-id = {14334821},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10994-016-5608-2},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10994-016-5608-2},
  posted-at            = {2017-04-10 01:08:53},
  publisher            = {Springer US},
  timestamp            = {2020-02-27 03:56},
}

@Article{Huang-Ribeiro-2016,
  author               = {Huang, Weiyu and Ribeiro, Alejandro},
  date                 = {2016-10},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Hierarchical Clustering Given Confidence Intervals of Metric Distances},
  eprint               = {1610.04274},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1610.04274},
  abstract             = {This paper considers metric spaces where distances between a pair of nodes are represented by distance intervals. The goal is to study methods for the determination of hierarchical clusters, i.e., a family of nested partitions indexed by a resolution parameter, induced from the given distance intervals of the metric spaces. Our construction of hierarchical clustering methods is based on defining admissible methods to be those methods that abide to the axioms of value - nodes in a metric space with two nodes are clustered together at the convex combination of the distance bounds between them - and transformation - when both distance bounds are reduced, the output may become more clustered but not less. Two admissible methods are constructed and are shown to provide universal upper and lower bounds in the space of admissible methods. Practical implications are explored by clustering moving points via snapshots and by clustering networks representing brain structural connectivity using the lower and upper bounds of the network distance. The proposed clustering methods succeed in identifying underlying clustering structures via the maximum and minimum distances in all snapshots, as well as in differentiating brain connectivity networks of patients from those of healthy controls.},
  citeulike-article-id = {14170685},
  citeulike-linkout-0  = {http://arxiv.org/abs/1610.04274},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1610.04274},
  day                  = {13},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-05-16 14:06:50},
  timestamp            = {2020-02-27 03:56},
}

@Article{Isogai-2017,
  author               = {Isogai, Takashi},
  date                 = {2017-05},
  journaltitle         = {Applied Network Science},
  title                = {Dynamic correlation network analysis of financial asset returns with network clustering},
  doi                  = {10.1007/s41109-017-0031-6},
  issn                 = {2364-8228},
  number               = {1},
  volume               = {2},
  abstract             = {In this study, we propose a novel approach to analyze a dynamic correlation network of highly volatile financial asset returns by using a network clustering algorithm to deal with high dimensionality issues. We analyze the dynamic correlation network of selected Japanese stock returns as an empirical study of the correlation dynamics at the market level by applying the proposed method. Two types of network clustering algorithms are employed for the dimensionality reduction. Firstly, several stock groups instead of the existing business sector classification are generated by the hierarchical recursive network clustering of filtered stock returns in order to overcome the high dimensionality problem due to the large number of stocks. The stock returns are then filtered in advance to control for volatility fluctuations that can distort the correlation between stocks. Thus, the correlation network of individual stock returns is transformed into a correlation network of group-based portfolio returns. Secondly, the reduced size of the correlation network is extended to a dynamic one by using a model-based correlation estimation method. A time series of adjacency matrices is created on a daily basis as a dynamic correlation network from the estimation results. Then, the correlation network is summarized into only three representative correlation networks by clustering along the time axis. Some intertemporal comparisons of the dynamic correlation network are conducted by examining the differences between the three sub-period networks. Our dynamic correlation network analysis framework is not limited to stock returns, but can be applied to many other financial and non-financial volatile time series data.},
  citeulike-article-id = {14399609},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s41109-017-0031-6},
  day                  = {23},
  groups               = {Networks and investment management, Clustering and network analysis, Invest_Network, Vol_Cluster},
  posted-at            = {2017-07-26 14:35:01},
  timestamp            = {2020-02-27 03:56},
}

@Article{Izakian-et-al-2014,
  author               = {Izakian, Hesam and Pedrycz, Witold and Jamal, Iqbal},
  date                 = {2015-03},
  journaltitle         = {Engineering Applications of Artificial Intelligence},
  title                = {Fuzzy clustering of time series data using dynamic time warping distance},
  doi                  = {10.1016/j.engappai.2014.12.015},
  issn                 = {0952-1976},
  pages                = {235--244},
  volume               = {39},
  abstract             = {Clustering is a powerful vehicle to reveal and visualize structure of data. When dealing with time series, selecting a suitable measure to evaluate the similarities/dissimilarities within the data becomes necessary and subsequently it exhibits a significant impact on the results of clustering. This selection should be based upon the nature of time series and the application itself. When grouping time series based on their shape information is of interest (shape-based clustering), using a Dynamic Time Warping (DTW) distance is a desirable choice. Using stretching or compressing segments of temporal data, DTW determines an optimal match between any two time series. In this way, time series exhibiting similar patterns occurring at different time periods, are considered as being similar. Although DTW is a suitable choice for comparing data with respect to their shape information, calculating the average of a collection of time series (which is required in clustering methods) based on this distance becomes a challenging problem. As the result, employing clustering techniques like K-Means and Fuzzy C-Means (where the cluster centers - prototypes are calculated through averaging the data) along with the DTW distance is a challenging task and may produce unsatisfactory results. In this study, three alternatives for fuzzy clustering of time series using DTW distance are proposed. In the first method, a DTW-based averaging technique proposed in the literature, has been applied to the Fuzzy C-Means clustering. The second method considers a Fuzzy C-Medoids clustering, while the third alternative comes as a hybrid technique, which exploits the advantages of both the Fuzzy C-Means and Fuzzy C-Medoids when clustering time series. Experimental studies are reported over a set of time series coming from the UCR time series database.},
  citeulike-article-id = {14485204},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.engappai.2014.12.015},
  groups               = {Scenario_TimeSeries},
  posted-at            = {2017-11-28 14:19:15},
  timestamp            = {2020-02-27 03:56},
}

@InProceedings{Jha-et-al-2015,
  author         = {Jha, Abhay and Ray, Shubhankar and Seaman, Brian and Dhillon, Inderjit S.},
  booktitle      = {2015 IEEE 31st International Conference on Data Engineering},
  date           = {2015-04-13},
  title          = {Clustering to forecast sparse time-series data},
  doi            = {10.1109/{ICDE}.2015.7113385},
  isbn           = {978-1-4799-7964-6},
  pages          = {1388--1399},
  publisher      = {IEEE},
  url            = {http://ieeexplore.ieee.org/document/7113385/},
  urldate        = {2019-09-11},
  abstract       = {Forecasting accurately is essential to successful inventory planning in retail. Unfortunately, there is not always enough historical data to forecast items individually- this is particularly true in e-commerce where there is a long tail of low selling items, and items are introduced and phased out quite frequently, unlike physical stores. In such scenarios, it is preferable to forecast items in well-designed groups of similar items, so that data for different items can be pooled together to fit a single model. In this paper, we first discuss the desiderata for such a grouping and how it differs from the traditional clustering problem. We then describe our approach which is a scalable local search heuristic that can naturally handle the constraints required in this setting, besides being capable of producing solutions competitive with well-known clustering algorithms. We also address the complementary problem of estimating similarity, particularly in the case of new items which have no past sales. Our solution is to regress the sales profile of items against their semantic features, so that given just the semantic features of a new item we can predict its relation to other items, in terms of as yet unobserved sales. Our experiments demonstrate both the scalability of our approach and implications for forecast accuracy.},
  day            = {13},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Jiang-et-al-2014a,
  author               = {Jiang, Kening and Li, Duan and Gao, Jianjun and Yu, Jeffrey X.},
  date                 = {2014},
  journaltitle         = {IFAC Proceedings Volumes},
  title                = {Factor Model Based Clustering Approach for Cardinality Constrained Portfolio Selection},
  doi                  = {10.3182/20140824-6-za-1003.00663},
  issn                 = {1474-6670},
  number               = {3},
  pages                = {10713--10718},
  volume               = {47},
  abstract             = {Portfolio selection concerns identifying an optimal composition of various risky assets and their corresponding holding amounts such that the corresponding investment strategy strikes a balance between maximizing the expected investment return and minimizing investment risk. While market frictions make full diversification impractical, cardinality constrained mean-variance (CCMV) portfolio selection problem emerges as a natural remedy: Given an asset pool with total n assets and a given cardinality s n, optimally choose s assets from the entire asset pool such as to achieve a mean-variance efficiency. Unfortunately, CCMV has been proved to be NP hard and has been posted in front of optimization society as a long-standing challenge. By invoking structural market information and utilizing fast clustering algorithm for classification, we develop in this paper an effective heuristic scheme to identify approximate solutions for large-scale CCMV problems. More specifically, by constructing grouping constraints generated from factor-model based clustering algorithm and attaching them to the mixed integer programming formulation associated with the CCMV problem, we are able to significantly reduce the computational complexity, thus offering a fast algorithm with relatively high quality solution.},
  citeulike-article-id = {14321944},
  citeulike-linkout-0  = {http://dx.doi.org/10.3182/20140824-6-za-1003.00663},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest, PortfOptim_Network, Invest_Network},
  posted-at            = {2017-03-28 18:07:33},
  timestamp            = {2020-02-27 04:01},
}

@Article{Ji-et-al-2018,
  author         = {Ji, Hao and Wang, Hao and Liseo, Brunero},
  date           = {2018-09},
  journaltitle   = {Australian economic papers},
  title          = {Portfolio Diversification Strategy Via Tail-Dependence Clustering and ARMA-GARCH Vine Copula Approach},
  doi            = {10.1111/1467-8454.12126},
  issn           = {0004-900X},
  number         = {3},
  pages          = {265--283},
  urldate        = {2019-12-04},
  volume         = {57},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Joe-Sang-2016,
  author               = {Joe, Harry and Sang, Peijun},
  date                 = {2016-05},
  journaltitle         = {Computational Statistics \& Data Analysis},
  title                = {Multivariate models for dependent clusters of variables with conditional independence given aggregation variables},
  doi                  = {10.1016/j.csda.2015.12.001},
  issn                 = {0167-9473},
  pages                = {114--132},
  volume               = {97},
  abstract             = {A general multivariate distributional approach, with conditional independence given aggregation variables, is presented to combine group-based submodels when variables are naturally divided into several non-overlapping groups. When the distributions are all multivariate Gaussian, the dependence among different groups is parsimonious based on conditional independence given linear combinations of variables in each group. For the case of multivariate t distributions in each group, a grouped t distribution is obtained. The approach can be extended so that the copula for each group is based on a skew-t distribution, and an application of this is given to financial returns of stocks in several different sectors. Another example of the modeling approach is given with variables separated into groups based on their units of measurements.},
  citeulike-article-id = {14219060},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.csda.2015.12.001},
  groups               = {Data_Independence},
  owner                = {cristi},
  posted-at            = {2016-12-02 15:44:07},
  timestamp            = {2020-02-27 04:01},
}

@Article{Jung-Chang-2016,
  author               = {Jung, Sean S. and Chang, Woojin},
  date                 = {2016-11},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Clustering stocks using partial correlation coefficients},
  doi                  = {10.1016/j.physa.2016.06.094},
  issn                 = {0378-4371},
  pages                = {410--420},
  volume               = {462},
  abstract             = {Correlation analyses are conducted on Korean stock market. Agglomerative hierarchical clustering is performed based on correlation matrices. Each cluster consists of firms from multiple business sectors. A partial correlation analysis is performed on the Korean stock market (KOSPI). The difference between Pearson correlation and the partial correlation is analyzed and it is found that when conditioned on the market return, Pearson correlation coefficients are generally greater than those of the partial correlation, which implies that the market return tends to drive up the correlation between stock returns. A clustering analysis is then performed to study the market structure given by the partial correlation analysis and the members of the clusters are compared with the Global Industry Classification Standard (GICS). The initial hypothesis is that the firms in the same GICS sector are clustered together since they are in a similar business and environment. However, the result is inconsistent with the hypothesis and most clusters are a mix of multiple sectors suggesting that the traditional approach of using sectors to determine the proximity between stocks may not be sufficient enough to diversify a portfolio.},
  citeulike-article-id = {14149947},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2016.06.094},
  groups               = {Networks and investment management, Clustering and network analysis, Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-10-01 13:50:28},
  timestamp            = {2020-02-27 04:01},
}

@Article{Kamalzadeh-et-al-2019,
  author         = {Kamalzadeh, Hossein and Ahmadi, Abbas and Mansour, Saeed},
  date           = {2019-12-05},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Clustering Time-Series by a Novel Slope-Based Similarity Measure Considering Particle Swarm Optimization},
  url            = {https://arxiv.org/abs/1912.02405},
  urldate        = {2019-12-22},
  abstract       = {Recently there has been an increase in the studies on time-series data mining specifically time-series clustering due to the vast existence of time-series in various domains. The large volume of data in the form of time-series makes it necessary to employ various techniques such as clustering to understand the data and to extract information and hidden patterns. In the field of clustering specifically, time-series clustering, the most important aspects are the similarity measure used and the algorithm employed to conduct the clustering. In this paper, a new similarity measure for time-series clustering is developed based on a combination of a simple representation of time-series, slope of each segment of time-series, Euclidean distance and the so-called dynamic time warping. It is proved in this paper that the proposed distance measure is metric and thus indexing can be applied. For the task of clustering, the Particle Swarm Optimization algorithm is employed. The proposed similarity measure is compared to three existing measures in terms of various criteria used for the evaluation of clustering algorithms. The results indicate that the proposed similarity measure outperforms the rest in almost every dataset used in this paper.},
  day            = {5},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Kang-et-al-2015,
  author               = {Kang, Zhao and Peng, Chong and Cheng, Qiang},
  date                 = {2015-11},
  journaltitle         = {IEEE Signal Processing Letters},
  title                = {Robust Subspace Clustering via Smoothed Rank Approximation},
  doi                  = {10.1109/lsp.2015.2460737},
  issn                 = {1070-9908},
  number               = {11},
  pages                = {2088--2092},
  volume               = {22},
  abstract             = {Matrix rank minimizing subject to affine constraints arises in many application areas, ranging from signal processing to machine learning. Nuclear norm is a convex relaxation for this problem which can recover the rank exactly under some restricted and theoretically interesting conditions. However, for many real-world applications, nuclear norm approximation to the rank function can only produce a result far from the optimum. To seek a solution of higher accuracy than the nuclear norm, in this letter, we propose a rank approximation based on Logarithm-Determinant. We consider using this rank approximation for subspace clustering application. Our framework can model different kinds of errors and noise. Effective optimization strategy is developed with theoretical guarantee to converge to a stationary point. The proposed method gives promising results on face clustering and motion segmentation tasks compared to the state-of-the-art subspace clustering algorithms.},
  citeulike-article-id = {14351210},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/lsp.2015.2460737},
  posted-at            = {2017-05-05 01:31:46},
  timestamp            = {2020-02-27 04:01},
}

@Article{Kang-et-al-2017b,
  author               = {Kang, Zhao and Peng, Chong and Cheng, Qiang},
  date                 = {2017-05},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Twin Learning for Similarity and Clustering: A Unified Kernel Approach},
  eprint               = {1705.00678},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1705.00678},
  abstract             = {Many similarity-based clustering methods work in two separate steps including similarity matrix computation and subsequent spectral clustering. However, similarity measurement is challenging because it is usually impacted by many factors, e.g., the choice of similarity metric, neighborhood size, scale of data, noise and outliers. Thus the learned similarity matrix is often not suitable, let alone optimal, for the subsequent clustering. In addition, nonlinear similarity often exists in many real world data which, however, has not been effectively considered by most existing methods. To tackle these two challenges, we propose a model to simultaneously learn cluster indicator matrix and similarity information in kernel spaces in a principled way. We show theoretical relationships to kernel k-means, k-means, and spectral clustering methods. Then, to address the practical issue of how to select the most suitable kernel for a particular clustering task, we further extend our model with a multiple kernel learning ability. With this joint model, we can automatically accomplish three subtasks of finding the best cluster indicator matrix, the most accurate similarity relations and the optimal combination of multiple kernels. By leveraging the interactions between these three subtasks in a joint framework, each subtask can be iteratively boosted by using the results of the others towards an overall optimal solution. Extensive experiments are performed to demonstrate the effectiveness of our method.},
  citeulike-article-id = {14351185},
  citeulike-linkout-0  = {http://arxiv.org/abs/1705.00678},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1705.00678},
  day                  = {3},
  posted-at            = {2017-05-04 22:18:58},
  timestamp            = {2020-02-27 04:01},
}

@Article{Kauffmann-et-al-2019,
  author         = {Kauffmann, Jacob and Esders, Malte and Montavon, Grgoire and Samek, Wojciech and Mller, Klaus-Robert},
  date           = {2019-06-18},
  journaltitle   = {arXiv Electronic Journal},
  title          = {From Clustering to Cluster Explanations via Neural Networks},
  url            = {https://arxiv.org/abs/1906.07633},
  urldate        = {2019-10-11},
  abstract       = {A wealth of algorithms have been developed to extract natural cluster structure in data. Identifying this structure is desirable but not always sufficient: We may also want to understand why the data points have been assigned to a given cluster. Clustering algorithms do not offer a systematic answer to this simple question. Hence we propose a new framework that can, for the first time, explain cluster assignments in terms of input features in a comprehensive manner. It is based on the novel theoretical insight that clustering models can be rewritten as neural networks, or 'neuralized'. Predictions of the obtained networks can then be quickly and accurately attributed to the input features. Several showcases demonstrate the ability of our method to assess the quality of learned clusters and to extract novel insights from the analyzed data and representations.},
  day            = {18},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Kawamoto-Kabashima-2017a,
  author               = {Kawamoto, Tatsuro and Kabashima, Yoshiyuki},
  date                 = {2017-06},
  journaltitle         = {Scientific Reports},
  title                = {Cross-validation estimate of the number of clusters in a network},
  doi                  = {10.1038/s41598-017-03623-x},
  issn                 = {2045-2322},
  number               = {1},
  volume               = {7},
  abstract             = {Network science investigates methodologies that summarise relational data to obtain better interpretability. Identifying modular structures is a fundamental task, and assessment of the coarse-grain level is its crucial step. Here, we propose principled, scalable, and widely applicable assessment criteria to determine the number of clusters in modular networks based on the leave-one-out cross-validation estimate of the edge prediction error.},
  citeulike-article-id = {14449671},
  citeulike-linkout-0  = {http://dx.doi.org/10.1038/s41598-017-03623-x},
  day                  = {12},
  posted-at            = {2017-10-12 23:55:55},
  timestamp            = {2020-02-27 04:01},
}

@Article{Kazor-Hering-2015,
  author               = {Kazor, Karen and Hering, AmandaS},
  date                 = {2015},
  journaltitle         = {Journal of Agricultural, Biological, and Environmental Statistics},
  title                = {Assessing the Performance of Model-Based Clustering Methods in Multivariate Time Series with Application to Identifying Regional Wind Regimes},
  doi                  = {10.1007/s13253-015-0203-8},
  number               = {2},
  pages                = {192--217},
  volume               = {20},
  abstract             = {The desire to group observations generated from multivariate time series is common in many applications with the goal to distinguish not only between differences in the means of individual variables but also changes in their covariances and in the temporal dependence of observations. In this analysis, we compare ten model-based clustering methods in terms of their ability to identify such features under four scenarios in which data are simulated with varying levels of variable and temporal dependence. To consider these methods in a realistic environment, we focus our analysis on wind data, where observations are often strongly correlated in time, and the dependence of variables is known to vary across different regional weather patterns. In particular, we assess each method's performance when applied to wind data simulated under a realistic two-regime Markov-switching vector autoregressive (VAR) model with a diurnally varying mean. A Gaussian mixture model and a basic Markov-switching model outperform the other methods considered in terms of misclassification rates and number of clusters identified. These two methods and an additional Markov-switching VAR model are then applied to one year of averaged hourly wind data from twenty meteorological stations, and we find that the methods can identify very different features in the data. Supplementary materials accompanying this paper appear on-line.},
  citeulike-article-id = {14014328},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s13253-015-0203-8},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s13253-015-0203-8},
  owner                = {cristi},
  posted-at            = {2016-04-17 17:30:34},
  publisher            = {Springer US},
  timestamp            = {2020-02-27 04:01},
}

@Article{Khaleghi-et-al-2016,
  author               = {Khaleghi, Azadeh and Ryabko, Daniil and Mary, Jeremie and Preux, Philippe},
  date                 = {2016},
  journaltitle         = {The Journal of Machine Learning Research},
  title                = {Consistent algorithms for clustering time series},
  pages                = {1--32},
  url                  = {http://jmlr.org/papers/v17/khaleghi16a.html},
  volume               = {17},
  abstract             = {The problem of clustering is considered for the case where every point is a time series. The time series are either given in one batch (offline setting), or they are allowed to grow with time and new time series can be added along the way (online setting). We propose a natural notion of consistency for this problem, and show that there are simple, computationally efficient algorithms that are asymptotically consistent under extremely weak assumptions on the distributions that generate the data. The notion of consistency is as follows. A clustering algorithm is called consistent if it places two time series into the same cluster if and only if the distribution that generates them is the same. In the considered framework the time series are allowed to be highly dependent, and the dependence can have arbitrary form. If the number of clusters is known, the only assumption we make is that the (marginal) distribution of each time series is stationary ergodic. No parametric, memory or mixing assumptions are made. When the number of clusters is unknown, stronger assumptions are provably necessary, but it is still possible to devise nonparametric algorithms that are consistent under very general conditions. The theoretical findings of this work are illustrated with experiments on both synthetic and real data.},
  citeulike-article-id = {14148585},
  groups               = {Networks and investment management, Clustering and network analysis, ML_ClustTimeSrs},
  owner                = {cristi},
  posted-at            = {2016-09-28 19:51:44},
  timestamp            = {2020-02-27 04:01},
}

@Article{Kou-et-al-2014,
  author               = {Kou, Gang and Peng, Yi and Wang, Guoxun},
  date                 = {2014-08},
  journaltitle         = {Information Sciences},
  title                = {Evaluation of clustering algorithms for financial risk analysis using MCDM methods},
  doi                  = {10.1016/j.ins.2014.02.137},
  issn                 = {0020-0255},
  pages                = {1--12},
  volume               = {275},
  abstract             = {The evaluation of clustering algorithms is intrinsically difficult because of the lack of objective measures. Since the evaluation of clustering algorithms normally involves multiple criteria, it can be modeled as a multiple criteria decision making (MCDM) problem. This paper presents an MCDM-based approach to rank a selection of popular clustering algorithms in the domain of financial risk analysis. An experimental study is designed to validate the proposed approach using three MCDM methods, six clustering algorithms, and eleven cluster validity indices over three real-life credit risk and bankruptcy risk data sets. The results demonstrate the effectiveness of MCDM methods in evaluating clustering algorithms and indicate that the repeated-bisection method leads to good 2-way clustering solutions on the selected financial risk data sets.},
  citeulike-article-id = {14435123},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ins.2014.02.137},
  posted-at            = {2017-09-20 22:42:34},
  timestamp            = {2020-02-27 04:01},
}

@Article{Kozdoba-Mannor-2016,
  author               = {Kozdoba, Mark and Mannor, Shie},
  date                 = {2016-09},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Clustering Time Series and the Surprising Robustness of HMMs},
  eprint               = {1605.02531},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1605.02531},
  abstract             = {Suppose that we are given a time series where consecutive samples are believed to come from a probabilistic source, that the source changes from time to time and that the total number of sources is fixed. Our objective is to estimate the distributions of the sources. A standard approach to this problem is to model the data as a hidden Markov model (HMM). However, since the data often lacks the Markov or the stationarity properties of an HMM, one can ask whether this approach is still suitable or perhaps another approach is required. In this paper we show that a maximum likelihood HMM estimator can be used to approximate the source distributions in a much larger class of models than HMMs. Specifically, we propose a natural and fairly general non-stationary model of the data, where the only restriction is that the sources do not change too often. Our main result shows that for this model, a maximum-likelihood HMM estimator produces the correct second moment of the data, and the results can be extended to higher moments.},
  citeulike-article-id = {14357388},
  citeulike-linkout-0  = {http://arxiv.org/abs/1605.02531},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1605.02531},
  day                  = {14},
  groups               = {Clustering and network analysis, NonStatry_FinTimeSrs},
  posted-at            = {2017-05-15 17:42:58},
  timestamp            = {2020-02-27 04:01},
}

@InCollection{Lemieux-et-al-2014,
  author               = {Lemieux, Victoria and Rahmdel, Payam S. and Walker, Rick and Wong, B. L. William and Flood, Mark},
  booktitle            = {Proceedings of the International Workshop on Data Science for Macro-Modeling},
  date                 = {2014},
  title                = {Clustering Techniques And Their Effect on Portfolio Formation and Risk Analysis},
  doi                  = {10.1145/2630729.2630749},
  isbn                 = {978-1-4503-3012-1},
  location             = {Snowbird, UT, USA},
  publisher            = {ACM},
  series               = {DSMM'14},
  abstract             = {This paper explores the application of three different portfolio formation rules using standard clustering techniques---K-means, K-mediods, and hierarchical---to a large financial data set (16 years of daily CRSP stock data) to determine how the choice of clustering technique may affect analysts' perceptions of the riskiness of different portfolios in the context of a prototype visual analytics system designed for financial stability monitoring. We use a two-phased experimental approach with visualizations to explore the effects of the different clustering techniques. The choice of clustering technique matters. There is significant variation among techniques, resulting in different "pictures"of the riskiness of the same underlying data when plotted to the visual analytics tool. This sensitivity to clustering methodolgy has the potential to mislead analysts about the riskiness of portfolios. We conclude that further research into the implications of portfolio formation rules is needed, and that visual analytics tools should not limit analysts to a single clustering technique, but instead should provide the facility to explore the data using different techniques.},
  address              = {New York, NY, USA},
  citeulike-article-id = {14148038},
  citeulike-linkout-0  = {http://portal.acm.org/citation.cfm?id=2630749},
  citeulike-linkout-1  = {http://dx.doi.org/10.1145/2630729.2630749},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest},
  owner                = {cristi},
  posted-at            = {2016-09-28 18:10:37},
  timestamp            = {2020-02-27 04:01},
}

@Article{Leon-et-al-2017,
  author               = {Leon, Diego and Aragon, Arbey and Sandoval, Javier and Hernandez, German and Arevalo, Andres and Nino, Jaime},
  date                 = {2017},
  journaltitle         = {Procedia Computer Science},
  title                = {Clustering algorithms for Risk-Adjusted Portfolio Construction},
  doi                  = {10.1016/j.procs.2017.05.185},
  issn                 = {1877-0509},
  pages                = {1334--1343},
  volume               = {108},
  abstract             = {This paper presents the performance of seven portfolios created using clustering analysis techniques to sort out assets into categories and then applying classical optimization inside every cluster to select best assets inside each asset category. The proposed clustering algorithms are tested constructing portfolios and measuring their performances over a two month dataset of 1-minute asset returns from a sample of 175 assets of the Russell 1000 index. A three-week sliding window is used for model calibration, leaving an out of sample period of five weeks for testing. Model calibration is done weekly. Three different rebalancing periods are tested: every 1, 2 and 4 hours. The results show that all clustering algorithms produce more stable portfolios with similar volatility. In this sense, the portfolios volatilities generated by the clustering algorithms are smaller when compare to the portfolio obtained using classical Mean-Variance Optimization (MVO) over all the dataset. Hierarchical clustering algorithms achieve the best financial performance obtaining an adequate trade-off between accumulated financial returns and the risk-adjusted measure, Omega Ratio, during the out of sample testing period.},
  citeulike-article-id = {14379414},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.procs.2017.05.185},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest, PortfOptim_Network, Invest_Network, Vol_Cluster},
  posted-at            = {2017-06-19 22:00:54},
  timestamp            = {2020-02-27 04:01},
}

@Article{Leon-et-al-2017a,
  author               = {Leon, Carlos and Kim, Geun-Young and Martinez, Constanza and Lee, Daeyup},
  date                 = {2017-08},
  journaltitle         = {Quantitative Finance},
  title                = {Equity markets' clustering and the global financial crisis},
  doi                  = {10.1080/14697688.2017.1357970},
  pages                = {1--18},
  abstract             = {The effect of the Global Financial Crisis (GFC) has been substantial across markets and countries worldwide. We examine how the GFC has changed the way equity markets group together based on the similarity of stock indices? daily returns. Our examination is based on agglomerative clustering methods, which yield a hierarchical structure that represents how stock markets relate to each other based on their cross-section similarity. Main results show that both hierarchical structures, before and after the GFC, are readily interpretable, and indicate that geographical factors dominate the hierarchy. The main features of equity markets? hierarchical structure agree with most stylized facts reported in related literature. The most noticeable change after the GFC is an increase in (geographical) clustering. However, the increase in clusters? compactness and the decrease in clusters? separateness point out that world equity markets became more interconnected after the GFC. Some changes in the hierarchy that do not conform to geographical clustering are explained by well-known idiosyncratic features or shocks.},
  citeulike-article-id = {14438484},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2017.1357970},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2017.1357970},
  day                  = {25},
  posted-at            = {2017-09-26 21:07:18},
  publisher            = {Routledge},
  timestamp            = {2020-02-27 04:01},
}

@InCollection{Leverger-et-al-2019,
  author         = {Leverger, Colin and Malinowski, Simon and Guyet, Thomas and Lemaire, Vincent and Bondu, Alexis and Termier, Alexandre},
  booktitle      = {Intelligent data engineering and automated learning - IDEAL 2019: 20th international conference, manchester, UK, november 14-16, 2019, proceedings, part I},
  date           = {2019},
  title          = {Toward a framework for seasonal time series forecasting using clustering},
  doi            = {10.1007/978-3-030-33607-3\_36},
  editor         = {Yin, Hujun and Camacho, David and Tino, Peter and Tallon-Ballesteros, Antonio J. and Menezes, Ronaldo and Allmendinger, Richard},
  isbn           = {978-3-030-33606-6},
  pages          = {328--340},
  publisher      = {Springer International Publishing},
  series         = {Lecture notes in computer science},
  url            = {http://link.springer.com/10.1007/978-3-030-33607-3\_36},
  urldate        = {2020-01-03},
  volume         = {11871},
  abstract       = {Seasonal behaviours are widely encountered in various applications. For instance, requests on web servers are highly influenced by our daily activities. Seasonal forecasting consists in forecasting the whole next season for a given seasonal time series. It may help a service provider to provision correctly the potentially required resources, avoiding critical situations of over- or under provision. In this article, we propose a generic framework to make seasonal time series forecasting. The framework combines machine learning techniques (1) to identify the typical seasons and (2) to forecast the likelihood of having a season type in one season ahead. We study this framework by comparing the mean squared errors of forecasts for various settings and various datasets. The best setting is then compared to state-of-the-art time series forecasting methods. We show that it is competitive with them.},
  f1000-projects = {QuantInvest},
  issn           = {0302-9743},
  timestamp      = {2020-02-27 04:01},
}

@Article{Liechti-Bonhoeffer-2019,
  author         = {Liechti, Jonas I. and Bonhoeffer, Sebastian},
  date           = {2019-12-09},
  journaltitle   = {arXiv Electronic Journal},
  title          = {A time resolved clustering method revealing longterm structures and their short-term internal dynamics},
  url            = {https://arxiv.org/abs/1912.04261v1},
  urldate        = {2019-12-15},
  abstract       = {The last decades have not only been characterized by an explosive growth of data, but also an increasing appreciation of data as a valuable resource. It's value comes with the ability to extract meaningful patterns that are of economic, societal or scientific relevance. A particular challenge is to identify patterns across time, including patterns that might only become apparent when the temporal dimension is taken into account. Here, we present a novel method that aims to achieve this by detecting dynamic clusters, i.e. structural elements that can be present over prolonged durations. It is based on an adaptive identification of majority overlaps between groups at different time points and allows the accommodation of transient decompositions in otherwise persistent dynamic clusters. As such, our method enables the detection of persistent structural elements with internal dynamics and can be applied to any classifiable data, ranging from social contact networks to arbitrary sets of time stamped feature vectors. It provides a unique tool to study systems with non-trivial temporal dynamics with a broad applicability to scientific, societal and economic data.},
  day            = {9},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Lin-et-al-2018a,
  author         = {Lin, Alexander and Zhang, Yingzhuo and Heng, Jeremy and Allsop, Stephen A. and Tye, Kay M. and Jacob, Pierre E. and Ba, Demba},
  date           = {2018-10-23},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Clustering Time Series with Nonlinear Dynamics: A Bayesian Non-Parametric and Particle-Based Approach},
  url            = {https://arxiv.org/abs/1810.09920},
  abstract       = {We propose a general statistical framework for clustering multiple time series that exhibit nonlinear dynamics into an a-priori-unknown number of sub-groups. Our motivation comes from neuroscience, where an important problem is to identify, within a large assembly of neurons, subsets that respond similarly to a stimulus or contingency. Upon modeling the multiple time series as the output of a Dirichlet process mixture of nonlinear state-space models, we derive a Metropolis-within-Gibbs algorithm for full Bayesian inference that alternates between sampling cluster assignments and sampling parameter values that form the basis of the clustering. The Metropolis step employs recent innovations in particle-based methods. We apply the framework to clustering time series acquired from the prefrontal cortex of mice in an experiment designed to characterize the neural underpinnings of fear.},
  day            = {23},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Lin-Li-2017,
  author               = {Lin, Lin and Li, Jia},
  date                 = {2017},
  journaltitle         = {Journal of Machine Learning Research},
  title                = {Clustering with Hidden Markov Model on Variable Blocks},
  number               = {110},
  pages                = {1--49},
  url                  = {http://jmlr.org/papers/v18/16-342.html},
  volume               = {18},
  abstract             = {Large-scale data containing multiple important rare clusters, even at moderately high dimensions, pose challenges for existing clustering methods. To address this issue, we propose a new mixture model called Hidden Markov Model on Variable Blocks (HMM-VB) and a new mode search algorithm called Modal Baum-Welch (MBW) for mode-association clustering. HMM-VB leverages prior information about chain-like dependence among groups of variables to achieve the effect of dimension reduction. In case such a dependence structure is unknown or assumed merely for the sake of parsimonious modeling, we develop a recursive search algorithm based on BIC to optimize the formation of ordered variable blocks. The MBW algorithm ensures the feasibility of clustering via mode association, achieving linear complexity in terms of the number of variable blocks despite the exponentially growing number of possible state sequences in HMM-VB. In addition, we provide theoretical investigations about the identifiability of HMM-VB as well as the consistency of our approach to search for the block partition of variables in a special case. Experiments on simulated and real data show that our proposed method outperforms other widely used methods.},
  citeulike-article-id = {14509669},
  citeulike-linkout-0  = {http://jmlr.org/papers/v18/16-342.html},
  keywords             = {*file-import-17-12-29},
  posted-at            = {2017-12-29 02:10:57},
  timestamp            = {2020-02-27 04:01},
}

@Article{Lisi-Menardi-2015,
  author               = {Lisi, Francesco and Menardi, Giovanna},
  date                 = {2015},
  journaltitle         = {Electronic Journal of Applied Statistical Analysis},
  title                = {Double clustering for rating mutual funds},
  number               = {1},
  pages                = {44--56},
  url                  = {http://siba-ese.unisalento.it/index.php/ejasa/article/view/13761},
  volume               = {8},
  abstract             = {Due to the increasing proliferation of mutual funds, in-depth evaluation of the available products for portfolio selection purposes is a difficult task. Hence, classiffication schemes giving quick information about which funds are worth to be monitored, are often provided. The aim of this work is to show an application of clustering methods to the mutual funds historical data. Starting from the monthly time series of the Net Asset Values of a specific style-based category, namely the Large Blend US mutual funds, we apply distance-based clustering methods twice on a set of return, risk and performance measures: firstly, with the aim of reducing data dimension, and secondly to cluster funds in homogeneous classes. The adopted procedure claims the feature of producing a partition of funds that are readily interpretable from a financial point of view and it is further possible to rank the identified groups, thus obtaining a rating of funds that turns out to account for different propensities toward the risk exposure.},
  citeulike-article-id = {14367329},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-06-02 22:26:50},
  timestamp            = {2020-02-27 04:01},
}

@Article{Lisi-Otranto-2010,
  author       = {Francesco Lisi and Edoardo Otranto},
  date         = {2010},
  journaltitle = {Mathematical and Statistical Methods for Actuarial Sciences and Finance},
  title        = {Clustering mutual funds by return and risk levels},
  url          = {https://link.springer.com/chapter/10.1007/978-88-470-1481-7_19},
  abstract     = {Mutual funds classifications, often made by rating agencies, are very common and sometimes criticised. In this work, a three-step statistical procedure for mutual funds classification is proposed. In the first step fund time series are characterised in terms of returns. In the second step, a clustering analysis is performed in order to obtain classes of homogeneous funds with respect to the risk levels. In particular, the risk is defined starting from an Asymmetric Threshold-GARCH model aimed to describe minimum, normal and turmoil risk. The third step merges the previous two. An application to 75 European funds belonging to 5 different categories is presented.},
  timestamp    = {2020-02-27 04:01},
}

@Article{Li-Zhu-2016,
  author               = {Li, Jinghua and Zhu, Dunlin},
  date                 = {2016-06},
  journaltitle         = {IET Renewable Power Generation},
  title                = {Combination of moment-matching, Cholesky and clustering methods to approximate discrete probability distribution of multiple wind farms},
  doi                  = {10.1049/iet-rpg.2015.0568},
  issn                 = {1752-1416},
  abstract             = {This study focuses on approximating a reduced discrete probability distribution (RDPD) of wind power from the original discrete probability distribution (ODPD), consisting of a large number of observed original scenarios (OSs), to relieve the burden of solving stochastic programs of wind power generation. The proposed method, namely, the MMCC method, aims to achieve high approximation accuracy and computational efficiency by combining an improved moment-matching (MM) method with the clustering (C) method and the Cholesky decomposition (CD) method. First, the C method is used to reduce the number of OSs by minimising the space distance between the reduced scenarios (RSs) and the OSs. Next, the CD method is used to rectify the correlation of the RSs to satisfy that of the ODPD. Finally, the RS probabilities are optimally determined by the MM method in order to minimise the stochastic features (first four moments and correlation matrix) between the RDPD and the ODPD. Simulations of RDPD approximation for three wind farms with 10, 20, 40, 60, 80, and 100 scenarios were carried out using the Latin hypercube sampling, importance sampling, C, moment-matching-clustering (MMC), and MMCC methods. The results showed that the MMCC method exhibits the best performance in terms of capturing the features of the ODPD.},
  citeulike-article-id = {14171142},
  citeulike-linkout-0  = {http://dx.doi.org/10.1049/iet-rpg.2015.0568},
  day                  = {13},
  owner                = {cristi},
  posted-at            = {2016-10-24 20:11:06},
  timestamp            = {2020-02-27 04:01},
}

@Article{Lorimer-et-al-2017,
  author         = {Lorimer, Tom and Held, Jenny and Stoop, Ruedi},
  date           = {2017-06-28},
  journaltitle   = {Philos Transact A Math Phys Eng Sci},
  title          = {Clustering: how much bias do we need?},
  doi            = {10.1098/rsta.2016.0293},
  number         = {2096},
  volume         = {375},
  abstract       = {Scientific investigations in medicine and beyond increasingly require observations to be described by more features than can be simultaneously visualized. Simply reducing the dimensionality by projections destroys essential relationships in the data. Similarly, traditional clustering algorithms introduce data bias that prevents detection of natural structures expected from generic nonlinear processes. We examine how these problems can best be addressed, where in particular we focus on two recent clustering approaches, Phenograph and Hebbian learning clustering, applied to synthetic and natural data examples. Our results reveal that already for very basic questions, minimizing clustering bias is essential, but that results can benefit further from biased post-processing.This article is part of the themed issue 'Mathematical methods in medicine: neuroscience, cardiology and pathology'.},
  day            = {28},
  f1000-projects = {QuantInvest},
  pmcid          = {PMC5434083},
  pmid           = {28507238},
  timestamp      = {2020-02-27 04:01},
}

@Article{Lu-et-al-2018a,
  author         = {Lu, Ya-Nan and Li, Sai-Ping and Zhong, Li-Xin and Jiang, Xiong-Fei and Ren, Fei},
  date           = {2018-12},
  journaltitle   = {Chaos, Solitons \& Fractals},
  title          = {A clustering-based portfolio strategy incorporating momentum effect and market trend prediction},
  doi            = {10.1016/j.chaos.2018.10.012},
  issn           = {0960-0779},
  pages          = {1--15},
  volume         = {117},
  abstract       = {Abstract The hierarchical clustering algorithm has been proved useful in portfolio investment, which is one of the hottest issues in finance. In our new portfolio strategy, central, peripheral and dispersed portfolios constructed from clusters detected using unweighted and weighted modularity are compared according to their past performances, and the optimal portfolio is used in the investment period only if the market index return predicted by the LR, WMA or BP models is positive to avoid losses when the market drops. Our strategy is tested using the daily data of Chinese A-share market from January 4, 2008 and December 31, 2016, and the average investment return during different moving investment periods and 200 repeated runs is calculated. We find that although incorporating dispersed portfolio into our strategy has no significant effect in raising the investment return, it shows a similar performance as the peripheral portfolio, and the strategy constructed using unweighted modularity generally outperforms its counterpart by using weighted modularity. In addition, the market trend prediction can refine the investment return of our strategy. In brief, the strategy constructed using the BP model and unweighted modularity has the best investment return, which also outperforms the Markowitz portfolio.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:01},
}

@Article{Lu-Wan-2013,
  author               = {Lu, Yonggang and Wan, Yi},
  date                 = {2013-05},
  journaltitle         = {Pattern Recognition},
  title                = {PHA: A fast potential-based hierarchical agglomerative clustering method},
  doi                  = {10.1016/j.patcog.2012.11.017},
  issn                 = {0031-3203},
  number               = {5},
  pages                = {1227--1239},
  volume               = {46},
  abstract             = {A novel potential-based hierarchical agglomerative (PHA) clustering method is proposed. In this method, we first construct a hypothetical potential field of all the data points, and show that this potential field is closely related to nonparametric estimation of the global probability density function of the data points. Then we propose a new similarity metric incorporating both the potential field which represents global data distribution information and the distance matrix which represents local data distribution information. Finally we develop another equivalent similarity metric based on an edge weighted tree of all the data points, which leads to a fast agglomerative clustering algorithm with time complexity O(N2). The proposed PHA method is evaluated by comparing with six other typical agglomerative clustering methods on four synthetic data sets and two real data sets. Experiments show that it runs much faster than the other methods and produces the most satisfying results in most cases. A potential field is used to represent global data distribution information. Both local and global data distribution information are used in the clustering. Designed an efficient hierarchical clustering method based on an edge-weighted tree. The potential field can be viewed as an estimated probability density function. PHA usually produces more satisfying results in much less time than other methods.},
  citeulike-article-id = {14148672},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.patcog.2012.11.017},
  owner                = {cristi},
  posted-at            = {2016-09-28 22:54:06},
  timestamp            = {2020-02-27 04:01},
}

@InCollection{Mackrechi-2016,
  author               = {Makrehchi, Masoud},
  booktitle            = {IEEE/WIC/ACM International Conference on Web Intelligence (WI)},
  date                 = {2016-10},
  title                = {Hierarchical Agglomerative Clustering Using Common Neighbours Similarity},
  doi                  = {10.1109/wi.2016.0093},
  isbn                 = {978-1-5090-4470-2},
  location             = {Omaha, NE, USA},
  pages                = {546--551},
  publisher            = {IEEE},
  abstract             = {Hierarchical clustering has been well-studied in the community of machine learning. Hierarchical clustering algorithms are deterministic, stable, and do not need a pre-determined number of clusters as input. However, they are not scalable for very large data due to their non-linear complexity. In this paper, a new approach is proposed to reduce the complexity of Hierarchical Clustering, improve the purity of the clustering algorithm, and reduce the chaining factor. The proposed method has the following components: (i) A new combination similarity based on common-neighbours of graph theory is proposed, (ii) In every iteration, instead of calculating the centroids for new clusters, new centroids are estimated from centroids in previous iteration, and (iii) In each iteration, instead of merging only one pair of objects, multiple pairs are merged at the same time. In addition to the proposed combination similarity, four well-known methods including centroid-based, group-based, complete-link, and single-link, have been also implemented. All five methods are tested and evaluated using two metrics: purity and imbalance or chaining factor. We show that our proposed algorithm outperforms other classic methods.},
  citeulike-article-id = {14335040},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/wi.2016.0093},
  posted-at            = {2017-04-10 12:18:55},
  timestamp            = {2020-02-27 04:01},
}

@Book{Maharaj-et-al-2019,
  author    = {Elizabeth Ann Maharaj and Pierpaolo {D'Urso} and Jorge Caiado},
  date      = {2019},
  title     = {Time Series Clustering and Classification},
  publisher = {CRC Press},
  url       = {https://www.crcpress.com/Time-Series-Clustering-and-Classification/Maharaj-DUrso-Caiado/p/book/9781498773218},
  timestamp = {2020-02-27 04:01},
}

@Article{Marbac-Sedki-2017,
  author         = {Marbac, Matthieu and Sedki, Mohammed},
  date           = {2017-07},
  journaltitle   = {Statistics and Computing},
  title          = {Variable selection for model-based clustering using the integrated complete-data likelihood},
  doi            = {10.1007/s11222-016-9670-1},
  issn           = {0960-3174},
  number         = {4},
  pages          = {1049--1063},
  volume         = {27},
  abstract       = {Variable selection in cluster analysis is important yet challenging. It can be achieved by regularization methods, which realize a trade-off between the clustering accuracy and the number of selected variables by using a lasso-type penalty. However, the calibration of the penalty term can suffer from criticisms. Model selection methods are an efficient alternative, yet they require a difficult optimization of an information criterion which involves combinatorial problems. First, most of these optimization algorithms are based on a suboptimal procedure (e.g. stepwise method). Second, the algorithms are often computationally expensive because they need multiple calls of EM algorithms. Here we propose to use a new information criterion based on the integrated complete-data likelihood. It does not require the maximum likelihood estimate and its maximization appears to be simple and computationally efficient. The original contribution of our approach is to perform the model selection without requiring any parameter estimation. Then, parameter inference is needed only for the unique selected model. This approach is used for the variable selection of a Gaussian mixture model with conditional independence assumed. The numerical experiments on simulated and benchmark datasets show that the proposed method often outperforms two classical approaches for variable selection. The proposed approach is implemented in the R package VarSelLCM available on CRAN.},
  f1000-projects = {QuantInvest},
  groups         = {Estim_MaxLikelihood},
  timestamp      = {2020-02-27 04:01},
}

@InCollection{Markovic-et-al-2019,
  author         = {Markovic, Ivana P. and Stankovic, Jelena Z. and Stojanovic, Milos B. and Stankovic, Jovica M.},
  booktitle      = {ICT innovations 2019. big data processing and mining: 11th international conference, ICT innovations 2019, ohrid, north macedonia, october 17-19, 2019, proceedings},
  date           = {2019},
  title          = {A Hybrid Model for Financial Portfolio Optimization Based on LS-SVM and a Clustering Algorithm},
  doi            = {10.1007/978-3-030-33110-8\_15},
  editor         = {Gievska, Sonja and Madjarov, Gjorgji},
  isbn           = {978-3-030-33109-2},
  pages          = {173--186},
  publisher      = {Springer International Publishing},
  series         = {Communications in computer and information science},
  url            = {http://link.springer.com/10.1007/978-3-030-33110-8\_15},
  urldate        = {2019-12-14},
  volume         = {1110},
  abstract       = {An investment decision is one of the most important financial decisions. With the aim of optimizing investment in securities from the aspect of return and risk, investors usually diversify their portfolio securities. This paper presents a hybrid model for portfolio optimization, which consist of two steps. The first step predicts future returns on the shares, and the second step, by applying hierarchical clustering algorithm, identifies various groups of shares. The test results indicate that the suggested model is suitable for optimization of a financial portfolio as a hybrid model based on selected shares, which if included in the portfolio, enable the diversification of risk.},
  f1000-projects = {QuantInvest},
  issn           = {1865-0929},
  timestamp      = {2020-02-27 04:01},
}

@Article{Marti-et-al-2016,
  author               = {Marti, Gautier and Andler, Sebastien and Nielsen, Frank and Donnat, Philippe},
  date                 = {2016-03},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Clustering Financial Time Series: How Long is Enough?},
  eprint               = {1603.04017},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1603.04017},
  abstract             = {Researchers have used from 30 days to several years of daily returns as source data for clustering financial time series based on their correlations. This paper sets up a statistical framework to study the validity of such practices. We first show that clustering correlated random variables from their observed values is statistically consistent. Then, we also give a first empirical answer to the much debated question: How long should the time series be? If too short, the clusters found can be spurious; if too long, dynamics can be smoothed out.},
  citeulike-article-id = {13987284},
  citeulike-linkout-0  = {http://arxiv.org/abs/1603.04017},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1603.04017},
  day                  = {13},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-03-24 14:12:20},
  timestamp            = {2020-02-27 04:01},
}

@Article{Marti-et-al-2016a,
  author               = {Marti, Gautier and Nielsen, Frank and Donnat, Philippe and Andler, Sebastien},
  date                 = {2016-03},
  journaltitle         = {arXiv Electronic Journal},
  title                = {On clustering financial time series: a need for distances between dependent random variables},
  eprint               = {1603.07822},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1603.07822},
  abstract             = {The following working document summarizes our work on the clustering of financial time series. It was written for a workshop on information geometry and its application for image and signal processing. This workshop brought several experts in pure and applied mathematics together with applied researchers from medical imaging, radar signal processing and finance. The authors belong to the latter group. This document was written as a long introduction to further development of geometric tools in financial applications such as risk or portfolio analysis. Indeed, risk and portfolio analysis essentially rely on covariance matrices. Besides that the Gaussian assumption is known to be inaccurate, covariance matrices are difficult to estimate from empirical data. To filter noise from the empirical estimate, Mantegna proposed using hierarchical clustering. In this work, we first show that this procedure is statistically consistent. Then, we propose to use clustering with a much broader application than the filtering of empirical covariance matrices from the estimate correlation coefficients. To be able to do that, we need to obtain distances between the financial time series that incorporate all the available information in these cross-dependent random processes.},
  citeulike-article-id = {14147455},
  citeulike-linkout-0  = {http://arxiv.org/abs/1603.07822},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1603.07822},
  day                  = {25},
  owner                = {cristi},
  posted-at            = {2016-09-27 19:10:13},
  timestamp            = {2020-02-27 04:01},
}

@Article{Marti-et-al-2016c,
  author               = {Marti, Gautier and Andler, Sebastien and Nielsen, Frank and Donnat, Philippe},
  date                 = {2016-10},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Exploring and measuring non-linear correlations: Copulas, Lightspeed Transportation and Clustering},
  eprint               = {1610.09659},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1610.09659},
  abstract             = {We propose a methodology to explore and measure the pairwise correlations that exist between variables in a dataset. The methodology leverages copulas for encoding dependence between two variables, state-of-the-art optimal transport for providing a relevant geometry to the copulas, and clustering for summarizing the main dependence patterns found between the variables. Some of the clusters centers can be used to parameterize a novel dependence coefficient which can target or forget specific dependence patterns. Finally, we illustrate and benchmark the methodology on several datasets. Code and numerical experiments are available online for reproducible research.},
  citeulike-article-id = {14291484},
  citeulike-linkout-0  = {http://arxiv.org/abs/1610.09659},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1610.09659},
  day                  = {30},
  posted-at            = {2017-03-03 18:30:47},
  timestamp            = {2020-02-27 04:01},
}

@InCollection{Marti-et-al-2017a,
  author               = {Marti, Gautier and Nielsen, Frank and Donnat, Philippe and Andler, Sebastien},
  booktitle            = {Computational Information Geometry},
  date                 = {2017},
  title                = {On Clustering Financial Time Series: A Need for Distances Between Dependent Random Variables},
  doi                  = {10.1007/978-3-319-47058-0\_8},
  editor               = {Nielsen, Frank and Critchley, Frank and Dodson, Christopher T. J.},
  pages                = {149--174},
  publisher            = {Springer International Publishing},
  series               = {Signals and Communication Technology},
  abstract             = {This artilce summarizes our work on the clustering of financial time series. It was written for a workshop on information geometry and its application for image and signal processing. This workshop brought several experts in pure and applied mathematics together with applied researchers from medical imaging, radar signal processing and finance. The authors belong to the latter group. This document was written as a long introduction to further development of geometric tools in financial applications such as risk or portfolio analysis. Indeed, risk and portfolio analysis essentially rely on covariance matrices. Besides that the Gaussian assumption is known to be inaccurate, covariance matrices are difficult to estimate from empirical data. To filter noise from the empirical estimate, Mantegna proposed using hierarchical clustering. In this work, we first show that this procedure is statistically consistent. Then, we propose to use clustering with a much broader application than the filtering of empirical covariance matrices from the estimated correlation coefficients. To be able to do that, we need to obtain distances between the financial time series that incorporate all the available information in these cross-dependent random processes.},
  citeulike-article-id = {14324929},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-47058-08},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-47058-08},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-03-30 22:23:54},
  timestamp            = {2020-02-27 04:01},
}

@Article{Marti-et-al-2019,
  author               = {Marti, Gautier and Nielsen, Frank and Bihkowski, Mikolaj and Donnat, Philippe},
  date                 = {2019-03},
  journaltitle         = {arXiv Electronic Journal},
  title                = {A review of two decades of correlations, hierarchies, networks and clustering in financial markets},
  eprinttype           = {arXiv},
  url                  = {https:://arxiv.org/abs/1703.00485},
  urldate              = {2018-10-07},
  abstract             = {This document is a preliminary version of an in-depth review on the state of the art of clustering financial time series and the study of correlation networks. This preliminary document is intended for researchers in this field so that they can feedback to allow amendments, corrections and addition of new material unknown to the authors of this review. The aim of the document is to gather in one place the relevant material that can help the researcher in the field to have a bigger picture, the quantitative researcher to play with this alternative modeling of the financial time series, and the decision maker to leverage the insights obtained from these methods. We hope that this document will form a basis for implementation of an open toolbox of standard tools to study correlations, hierarchies, networks and clustering in financial markets. We also plan to maintain pointers to online material and an updated version of this work at www.datagrapple.com/Tech.},
  citeulike-article-id = {14291433},
  citeulike-linkout-0  = {http://arxiv.org/abs/1703.00485},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1703.00485},
  day                  = {1},
  groups               = {Networks and investment management, Clustering and network analysis, Invest_Network},
  posted-at            = {2017-03-03 16:58:42},
  timestamp            = {2020-02-27 04:01},
}

@Article{Micciche-et-al-2005,
  author               = {Micciche, S. and Lillo, F. and Mantegna, R. N.},
  date                 = {2005-09},
  journaltitle         = {Workshop of the International School of Solid State Physics},
  title                = {Correlation Based Hierarchical Clustering in Financial Time Series},
  doi                  = {10.1142/9789812701558\_0037},
  pages                = {327--335},
  abstract             = {Abstract We review a correlation based clustering procedure applied to a portfolio of assets synchronously traded in a financial market. The portfolio considered consists of the set of 500 highly capitalized stocks traded at the New York Stock Exchange during the time period 1987-1998. We show that meaningful economic information can be extracted from correlation matrices.},
  booktitle            = {Complexity, Metastability and Nonextensivity},
  citeulike-article-id = {14150002},
  citeulike-linkout-0  = {http://dx.doi.org/10.1142/97898127015580037},
  citeulike-linkout-1  = {http://adsabs.harvard.edu/cgi-bin/nph-bibquery?bibcode=2005cmn..conf..327M},
  citeulike-linkout-2  = {http://www.worldscientific.com/doi/abs/10.1142/97898127015580037},
  day                  = {1},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-10-01 16:38:57},
  publisher            = {WORLD SCIENTIFIC},
  timestamp            = {2020-02-27 04:01},
}

@InCollection{Mitra-et-al-2019,
  author         = {Mitra, Arup and Das, Abhra and Goswami, Saptarsi and Mustafi, Joy and Jalan, A. K.},
  booktitle      = {Computational intelligence, communications, and business analytics: second international conference, CICBA 2018, kalyani, india, july 27-28, 2018, revised selected papers, part II},
  date           = {2019},
  title          = {Portfolio management by time series clustering using correlation for stocks},
  doi            = {10.1007/978-981-13-8581-0\_11},
  editor         = {Mandal, Jyotsna Kumar and Mukhopadhyay, Somnath and Dutta, Paramartha and Dasgupta, Kousik},
  isbn           = {978-981-13-8580-3},
  location       = {Singapore},
  pages          = {134--144},
  publisher      = {Springer Singapore},
  series         = {Communications in computer and information science},
  url            = {http://link.springer.com/10.1007/978-981-13-8581-0\_11},
  urldate        = {2019-12-14},
  volume         = {1031},
  abstract       = {Investment diversification and portfolio building has been a great interest for share market investors, so as to minimize risk and maximize profit in a sensitive stock market. This paper gives an inside view of application of clustering for grouping 79 stocks (NSE), which can be used to build a diversified portfolio. Manually trying out different groupings to diversify portfolio is a computationally expensive task. In this paper, the closing price, time series of the stocks have been considered. Common effect due to market has been discounted using partial correlation, and a correlation based dissimilarity measure has been used for clustering. An equal investment strategy has been adopted to compare the portfolio performance with SENSEX. The empirical results of the portfolios have been studied and presented in details.},
  f1000-projects = {QuantInvest},
  issn           = {1865-0929},
  timestamp      = {2020-02-27 04:01},
}

@Article{Montero-Vilar-2015,
  author       = {Pablo Montero and Jose A. Vilar},
  date         = {2015},
  journaltitle = {Journal of Statistical Software},
  title        = {TSclust: An R Package for Time Series Clustering},
  url          = {https://www.jstatsoft.org/article/view/v062i01},
  volume       = {62},
  abstract     = {Time series clustering is an active research area with applications in a wide range of fields. One key component in cluster analysis is determining a proper dissimilarity measure between two data objects, and many criteria have been proposed in the literature to assess dissimilarity between two time series. The R package TSclust is aimed to implement a large set of well-established peer-reviewed time series dissimilarity measures, including measures based on raw data, extracted features, underlying parametric models, complexity levels, and forecast behaviors. Computation of these measures allows the user to perform clustering by using conventional clustering algorithms. TSclust also includes a clustering procedure based on p values from checking the equality of generating models, and some utilities to evaluate cluster solutions. The implemented dissimilarity functions are accessible individually for an easier extension and possible use out of the clustering context. The main features of TSclust are described and examples of its use are presented.},
  groups       = {FrcstQWIM_TimeSrs},
  timestamp    = {2020-02-27 04:01},
}

@Article{Muca-et-al-2015,
  author               = {Muca, Markela and Kutrolli, Gleda and Kutrolli, Maksi},
  date                 = {2015},
  journaltitle         = {European Scientific Journal},
  title                = {A proposed algorithm for determining the optimal number of clusters},
  url                  = {https://eujournal.org/index.php/esj/article/view/6756},
  abstract             = {Data clustering is a data exploration technique that allows objects with similar characteristics to be grouped together in order to facilitate their further processing. The K-means algorithm is a popular data-clustering algorithm. However, one of its drawbacks is the requirement for the number of clusters, K, to be specified before the algorithm is applied. This paper first reviews existing methods for selecting the number of clusters for the algorithm. Factors that affect this selection are then discussed and an improvement of the existing k-means algorithm to assist the selection is proposed. The paper concludes with an analysis of the results of using cluster validation referring to some measures that are classified as internal and external indexes to determine the optimal number of clusters for the K-means algorithm. There are applied some stopping criterion referring to those indexes for evaluating a clustering against a gold standard.},
  citeulike-article-id = {14435139},
  posted-at            = {2017-09-21 00:35:42},
  timestamp            = {2020-02-27 04:01},
}

@Article{Murtagh-Contreras-2011,
  author               = {Murtagh, Fionn and Contreras, Pedro},
  date                 = {2011-04},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Methods of Hierarchical Clustering},
  eprint               = {1105.0121},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1105.0121},
  abstract             = {We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments. We look at hierarchical self-organizing maps, and mixture models. We review grid-based clustering, focusing on hierarchical density-based approaches. Finally we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm.},
  citeulike-article-id = {10504808},
  citeulike-linkout-0  = {http://arxiv.org/abs/1105.0121},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1105.0121},
  day                  = {30},
  groups               = {Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-09-28 19:36:48},
  timestamp            = {2020-02-27 04:01},
}

@Article{Murtagh-Contreras-2012,
  author               = {Murtagh, Fionn and Contreras, Pedro},
  date                 = {2012-01},
  journaltitle         = {WIREs Data Mining Knowl Discov},
  title                = {Algorithms for hierarchical clustering: an overview},
  doi                  = {10.1002/widm.53},
  number               = {1},
  pages                = {86--97},
  volume               = {2},
  abstract             = {We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments. We look at hierarchical self-organizing maps, and mixture models. We review grid-based clustering, focusing on hierarchical density-based approaches. Finally, we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm.},
  citeulike-article-id = {13987836},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/widm.53},
  day                  = {1},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-03-25 04:22:46},
  publisher            = {John Wiley and Sons, Inc.},
  timestamp            = {2020-02-27 04:01},
}

@Article{Murtagh-Contreras-2017,
  author               = {Murtagh, Fionn and Contreras, Pedro},
  date                 = {2017-11},
  journaltitle         = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  title                = {Algorithms for hierarchical clustering: an overview, II},
  doi                  = {10.1002/widm.1219},
  issn                 = {1942-4787},
  number               = {6},
  pages                = {e1219--n/a},
  volume               = {7},
  abstract             = {We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments. We look at hierarchical self-organizing maps and mixture models. We review grid-based clustering, focusing on hierarchical density-based approaches. Finally, we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm. This review adds to the earlier version, Murtagh F, Contreras P. Algorithms for hierarchical clustering: an overview, Wiley Interdiscip Rev: Data Mining Knowl Discov 2012, 2, 86-97. WIREs Data Mining Knowl Discov 2017, 7:e1219. doi: 10.1002/widm.1219 For further resources related to this article, please visit the WIREs website.},
  citeulike-article-id = {14449844},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/widm.1219},
  day                  = {1},
  journal              = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  posted-at            = {2017-11-23 20:46:27},
  publisher            = {Wiley Periodicals, Inc},
  timestamp            = {2020-02-27 04:01},
  year                 = {2017},
}

@Article{Murtagh-Legendre-2014,
  author               = {Murtagh, Fionn and Legendre, Pierre},
  date                 = {2014-11},
  journaltitle         = {Journal of Classification},
  title                = {Ward's Hierarchical Agglomerative Clustering Method: Which Algorithms Implement Ward's Criterion?},
  doi                  = {10.1007/s00357-014-9161-z},
  number               = {3},
  pages                = {274--295},
  volume               = {31},
  abstract             = {The Ward error sum of squares hierarchical clustering method has been very widely used since its first description by Ward in a 1963 publication. It has also been generalized in various ways. Two algorithms are found in the literature and software, both announcing that they implement the Ward clustering method. When applied to the same distance matrix, they produce different results. One algorithm preserves Ward's criterion, the other does not. Our survey work and case studies will be useful for all those involved in developing software for data analysis using Ward's hierarchical clustering method.},
  citeulike-article-id = {14482080},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s00357-014-9161-z},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s00357-014-9161-z},
  posted-at            = {2017-11-23 20:38:36},
  publisher            = {Springer US},
  timestamp            = {2020-02-27 04:01},
  year                 = {2014},
}

@Article{Narabin-Boongasame-2018,
  author       = {Santit Narabin and Laor Boongasame},
  date         = {2018},
  journaltitle = {International Conference on Big Data and Artificial Intelligence},
  title        = {A Cluster Analysis of Mutual Funds Data},
  url          = {https://ieeexplore.ieee.org/abstract/document/8546679},
  abstract     = {The factors of clustering mutual fund (such as Net Asset Value (NAV)) do not direct to both return and risk of mutual funds which they are important factors for investors. This research helps an investor can estimate profit and loss rate of the mutual fund in his/her portfolio by using the net asset value change ratios (NAVCR). Then, both the NAVCR and value of each mutual fund will be used for clustering. For building a portfolio, the mutual funds could be selected from the diversified groups in order to reduce risk. The mutual fund data at different times from the set for the fiscal year 2010 - 2017 are used. The results of our analysis show that our models offer significantly better performance than the portfolio management model derived from the random portfolio management.},
  timestamp    = {2020-02-27 04:01},
}

@Article{Nguyen-et-al-2017a,
  author         = {Nguyen, Hien D. and McLachlan, Geoffrey J. and Orban, Pierre and Bellec, Pierre and Janke, Andrew L.},
  date           = {2017-04},
  journaltitle   = {Neural Computation},
  title          = {Maximum Pseudolikelihood Estimation for Model-Based Clustering of Time Series Data.},
  doi            = {10.1162/{NECO\_a\_00938}},
  number         = {4},
  pages          = {990--1020},
  volume         = {29},
  abstract       = {Mixture of autoregressions (MoAR) models provide a model-based approach to the clustering of time series data. The maximum likelihood (ML) estimation of MoAR models requires evaluating products of large numbers of densities of normal random variables. In practical scenarios, these products converge to zero as the length of the time series increases, and thus the ML estimation of MoAR models becomes infeasible without the use of numerical tricks. We propose a maximum pseudolikelihood (MPL) estimation approach as an alternative to the use of numerical tricks. The MPL estimator is proved to be consistent and can be computed with an EM (expectation-maximization) algorithm. Simulations are used to assess the performance of the MPL estimator against that of the ML estimator in cases where the latter was able to be calculated. An application to the clustering of time series data arising from a resting state fMRI experiment is presented as a demonstration of the methodology.},
  f1000-projects = {QuantInvest},
  groups         = {Estim_MaxLikelihood},
  pmid           = {28095191},
  timestamp      = {2020-02-27 04:01},
}

@Article{Nie-2017,
  author               = {Nie, Chun-Xiao},
  date                 = {2017-06},
  journaltitle         = {Chaos, Solitons and Fractals},
  title                = {Dynamics of cluster structure in financial correlation matrix},
  doi                  = {10.1016/j.chaos.2017.05.039},
  issn                 = {0960-0779},
  abstract             = {The relationship between the correlation dimension of the financial market and the cluster structure in the correlation coefficient matrix is studied. Empirical results based on model data show that the clearer cluster structure corresponds to a smaller dimension. We use the algorithm to verify the relationship between the cluster structure of the real market data and the correlation dimension. The correlation dimensions in the financial market are calculated and used as a measure to study the cluster structure in the correlation coefficient matrix. First, based on the existing model, we present a toy model. Using the model-generated data, we find that the clearer cluster structure corresponds to a smaller dimension. It implies that the correlation dimension can be used as a measure of the cluster structure in the correlation coefficient matrix. Finally, we use the algorithm to compute the clusters in the real market and verify the previous empirical evidence. The results show that the cluster structure in the financial correlation coefficient matrix may change with time. The correlation dimension is smaller after the financial crisis, indicating that the cluster structure is clearer after the financial crisis.},
  citeulike-article-id = {14384085},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.chaos.2017.05.039},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-06-28 18:36:06},
  timestamp            = {2020-02-27 04:01},
}

@Article{Nie-2017a,
  author               = {Nie, Chun-Xiao},
  date                 = {2017-09},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Cluster structure in the correlation coefficient matrix can be characterized by abnormal eigenvalues},
  doi                  = {10.1016/j.physa.2017.09.066},
  issn                 = {0378-4371},
  abstract             = {n a large number of previous studies, the researchers found that some of the eigenvalues of the financial correlation matrix were greater than the predicted values of the random matrix theory (). Here, we call these eigenvalues as anomalous eigenvalues. In order to reveal the hidden meaning of these anomalous eigenvalues, we study the toy model with cluster structure and find that these eigenvalues are related to the cluster structure of the correlation coefficient matrix. In this paper, model-based experiments show that in most cases, the number of anomalous eigenvalues of the correlation matrix is equal to the number of clusters. In addition, empirical studies show that the sum of the anomalous eigenvalues is related to the clarity of the cluster structure and is negatively correlated with the correlation dimension.},
  citeulike-article-id = {14444598},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2017.09.066},
  posted-at            = {2017-10-03 13:16:43},
  timestamp            = {2020-02-27 04:01},
}

@Article{Nieminen-et-al-2013,
  author               = {Nieminen, Paavo and Polonen, Ilkka and Sipola, Tuomo},
  date                 = {2013-10},
  journaltitle         = {Journal of Informetrics},
  title                = {Research literature clustering using diffusion maps},
  doi                  = {10.1016/j.joi.2013.08.004},
  issn                 = {1751-1577},
  number               = {4},
  pages                = {874--886},
  volume               = {7},
  abstract             = {Adapting knowledge discovery process to scientometrics. Diffusion map-based clustering framework introduced to scientometrics. Structure of data mining literature revealed as a case study. We apply the knowledge discovery process to the mapping of current topics in a particular field of science. We are interested in how articles form clusters and what are the contents of the found clusters. A framework involving web scraping, keyword extraction, dimensionality reduction and clustering using the diffusion map algorithm is presented. We use publicly available information about articles in high-impact journals. The method should be of use to practitioners or scientists who want to overview recent research in a field of science. As a case study, we map the topics in data mining literature in the year 2011.},
  citeulike-article-id = {14212401},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.joi.2013.08.004},
  owner                = {cristi},
  posted-at            = {2016-11-21 22:04:14},
  timestamp            = {2020-02-27 04:01},
}

@Article{Ning-et-al-2015,
  author               = {Ning, Cathy and Xu, Dinghai and Wirjanto, Tony S.},
  date                 = {2015-03},
  journaltitle         = {Journal of Banking and Finance},
  title                = {Is volatility clustering of asset returns asymmetric?},
  doi                  = {10.1016/j.jbankfin.2014.11.016},
  issn                 = {0378-4266},
  pages                = {62--76},
  volume               = {52},
  abstract             = {We investigate the structure of volatility clustering of asset returns. We employ copula-based univariate time-series models and realized kernel volatility. We find that volatility clustering is highly nonlinear and strongly asymmetric. Our paper is the first one that models and finds asymmetric nonlinear volatility clustering. This finding is important in asset pricing, derivatives pricing, and risk management. Volatility clustering is a well-known stylized feature of financial asset returns. This paper investigates asymmetric pattern in volatility clustering by employing a univariate copula approach of Chen and Fan (2006). Using daily realized kernel volatilities constructed from high frequency data from stock and foreign exchange markets, we find evidence that volatility clustering is highly nonlinear and strongly asymmetric in that clusters of high volatility occur more often than clusters of low volatility. To the best of our knowledge, this paper is the first one to address and uncover this phenomenon. In particular, the asymmetry in volatility clustering is found to be more pronounced in the stock markets than in the foreign exchange markets. Further, the volatility clusters are shown to remain persistent for over a month and asymmetric across different time periods. Our findings have important implications for risk management. A simulation study indicates that models which accommodate asymmetric volatility clustering can significantly improve the out-of-sample forecasts of Value-at-Risk},
  citeulike-article-id = {14313629},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jbankfin.2014.11.016},
  groups               = {Vol_Cluster},
  posted-at            = {2017-03-19 03:08:58},
  timestamp            = {2020-02-27 04:01},
}

@Article{NoelKoide-2016a,
  author               = {Noel-Koide, Kevin},
  date                 = {2016-03},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Asset Allocation and Intra-Sector Allocation Using Covariance and Precision Matrix Clustering},
  url                  = {https://ssrn.com/abstract=2745727},
  abstract             = {We investigate simply the usage of clustering method by inverse covariance estimation for asset allocation in finance. Allocation across various sectors of the market (i.e. sector ETF) is usually well understood through the usage of mean variance allocation. However, stocks inside a same sector (i.e. Biotechnology, IT,..) are little studied in the literature since high correlation between stocks leads to poor differentiation across stocks.Here, we apply covariance clustering method to study the behavior of one specific market sector: US listed biotechnology stocks.

We show that this methodology allows differentiating highly correlated stocks and it provides more detailed information inside this sector. Additionally, the use of higher frequency data (intraday data) allows refining the clustering method and characterizing further each stock. This method can give further insight to select stocks inside a same sector.},
  citeulike-article-id = {13978597},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2745727},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2745727code2498383.pdf?abstractid=2745727 and mirid=1},
  day                  = {11},
  groups               = {Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-03-12 22:40:18},
  timestamp            = {2020-02-27 04:01},
}

@Article{Otranto-Gargano-2015,
  author               = {Otranto, Edoardo and Gargano, Romana},
  date                 = {2015},
  journaltitle         = {Advances in Data Analysis and Classification},
  title                = {Financial clustering in presence of dominant markets},
  doi                  = {10.1007/s11634-014-0189-z},
  number               = {3},
  pages                = {315--339},
  volume               = {9},
  abstract             = {Clustering financial time series is a recent topic of statistical literature with important fields of applications, in particular portfolio composition and risk evaluation. The risk is generally linked to the volatility of the asset, but its level of predictability also plays a basic role in investment decisions. In particular, the classification of a certain asset could be linked to its dependence on the volatility of a dominant market: movements in the volatility of the dominant market can provide similar movements in the volatility of the asset and its predictability would depend on the strength of this dependence. Working in a model based framework, we base the classification of the volatility of an asset not only on its volatility level, but also on the presence of spillover effects from a dominant market, such as the US one, and on the similarity of the dynamics of the asset and the dominant market. The method is carried out using an extended version of the Multiplicative Error Model and is applied to a set of European assets, also performing a historical simulation experiment.},
  citeulike-article-id = {14014267},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11634-014-0189-z},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s11634-014-0189-z},
  groups               = {Networks and investment management, Clustering and network analysis, Vol_Cluster},
  owner                = {cristi},
  posted-at            = {2016-04-17 16:15:18},
  publisher            = {Springer Berlin Heidelberg},
  timestamp            = {2020-02-27 04:01},
}

@InProceedings{Palupi-et-al-2019,
  author         = {Palupi, Irma and Wahyudi, Bambang Ari and Indwiarti, Indwiarti},
  booktitle      = {7th International Conference on Information and Communication Technology (ICoICT)},
  date           = {2019-07-24},
  title          = {The clustering algorithms approach for decision efficiency in investment portfolio diversification},
  doi            = {10.1109/{ICoICT}.2019.8835314},
  isbn           = {978-1-5386-8052-0},
  pages          = {1--6},
  publisher      = {IEEE},
  url            = {https://ieeexplore.ieee.org/document/8835314/},
  urldate        = {2019-12-03},
  abstract       = {This paper performs a clustering algorithm for portfolio investment diversification. The clustering process is applied to choose the preferred assets among hundreds of assets provided in the market under the related features. This work experimentally provides four features as the coordinate of assets that are mean, variance, skewness, and kurtosis of the returns. The used data is the daily close price of 175 assets in Indonesian exchange (IDX). We perform 4 clustering algorithms to locate the assets that have the same similarity into the same cluster. Since the four features are being considered, the spherical-shape of data is difficult to observe. In fact, the portfolio return of all algorithm's outcome show the Agglomerative and DBScan algorithm yield higher performance evaluation with no dominant asset included. Assets representatives from each cluster are determined to be in the portfolio formation. By using the portfolio theory by Markowitz, i.e Mean-variance optimization (MVO) and Sharp ration optimization, the proportion of contained assets are computed, and we test their performance by applying the formation into the market data a month after as a testing data. Several interesting results are provided.},
  day            = {24},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Paparrizos-Gravano-2017,
  author               = {Paparrizos, John and Gravano, Luis},
  date                 = {2017-06},
  journaltitle         = {ACM Transactions on Database Systems},
  title                = {Fast and Accurate Time-Series Clustering},
  doi                  = {10.1145/3044711},
  issn                 = {0362-5915},
  number               = {2},
  volume               = {42},
  abstract             = {The proliferation and ubiquity of temporal data across many disciplines has generated substantial interest in the analysis and mining of time series. Clustering is one of the most popular data-mining methods, not only due to its exploratory power but also because it is often a preprocessing step or subroutine for other techniques. In this article, we present k-Shape and k-MultiShapes (k-MS), two novel algorithms for time-series clustering. k-Shape and k-MS rely on a scalable iterative refinement procedure. As their distance measure, k-Shape and k-MS use shape-based distance (SBD), a normalized version of the cross-correlation measure, to consider the shapes of time series while comparing them. Based on the properties of SBD, we develop two new methods, namely ShapeExtraction (SE) and MultiShapesExtraction (MSE), to compute cluster centroids that are used in every iteration to update the assignment of time series to clusters. k-Shape relies on SE to compute a single centroid per cluster based on all time series in each cluster. In contrast, k-MS relies on MSE to compute multiple centroids per cluster to account for the proximity and spatial distribution of time series in each cluster. To demonstrate the robustness of SBD, k-Shape, and k-MS, we perform an extensive experimental evaluation on 85 datasets against state-of-the-art distance measures and clustering methods for time series using rigorous statistical analysis. SBD, our efficient and parameter-free distance measure, achieves similar accuracy to Dynamic Time Warping (DTW), a highly accurate but computationally expensive distance measure that requires parameter tuning. For clustering, we compare k-Shape and k-MS against scalable and non-scalable partitional, hierarchical, spectral, density-based, and shapelet-based methods, with combinations of the most competitive distance measures. k-Shape outperforms all scalable methods in terms of accuracy. Furthermore, k-Shape also outperforms all non-scalable approaches, with one exception, namely k-medoids with DTW, which achieves similar accuracy. However, unlike k-Shape, this approach requires tuning of its distance measure and is significantly slower than k-Shape. k-MS performs similarly to k-Shape in comparison to rival methods, but k-MS is significantly more accurate than k-Shape. Beyond clustering, we demonstrate the effectiveness of k-Shape to reduce the search space of one-nearest-neighbor classifiers for time series. Overall, SBD, k-Shape, and k-MS emerge as domain-independent, highly accurate, and efficient methods for time-series comparison and clustering with broad applications.},
  citeulike-article-id = {14435136},
  citeulike-linkout-0  = {http://portal.acm.org/citation.cfm?id=3086510.3044711},
  citeulike-linkout-1  = {http://dx.doi.org/10.1145/3044711},
  groups               = {Scenario_TimeSeries},
  location             = {New York, NY, USA},
  posted-at            = {2017-09-21 00:10:48},
  publisher            = {ACM},
  timestamp            = {2020-02-27 04:04},
}

@Article{Park-2020,
  author         = {Park, Jinwoo},
  date           = {2020-01-09},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Clustering Approaches for Global Minimum Variance Portfolio},
  url            = {https://arxiv.org/abs/2001.02966v2},
  urldate        = {2020-01-17},
  abstract       = {The only input to attain the portfolio weights of global minimum variance portfolio (GMVP) is the covariance matrix of returns of assets being considered for investment. Since the population covariance matrix is not known, investors use historical data to estimate it. Even though sample covariance matrix is an unbiased estimator of the population covariance matrix, it includes a great amount of estimation error especially when the number of observed data is not much bigger than number of assets. As it is difficult to estimate the covariance matrix with high dimensionality all at once, clustering stocks is proposed to come up with covariance matrix in two steps: firstly, within a cluster and secondly, between clusters. It decreases the estimation error by reducing the number of features in the data matrix. The motivation of this dissertation is that the estimation error can still remain high even after clustering, if a large amount of stocks is clustered together in a single group. This research proposes to utilize a bounded clustering method in order to limit the maximum cluster size. The result of experiments shows that not only the gap between in-sample volatility and out-of-sample volatility decreases, but also the out-of-sample volatility gets reduced. It implies that we need a bounded clustering algorithm so that maximum clustering size can be precisely controlled to find the best portfolio performance.},
  day            = {9},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Pasteris-et-al-2018,
  author         = {Pasteris, Stephen and Vitale, Fabio and Gentile, Claudio and Herbster, Mark},
  date           = {2018-04-09},
  journaltitle   = {Proceedings of Machine Learning Research},
  title          = {On Similarity Prediction and Pairwise Clustering},
  url            = {http://proceedings.mlr.press/v83/pasteris18a.html},
  urldate        = {2019-09-15},
  abstract       = {We consider the problem of clustering a finite set of items from pairwise similarity information. Unlike what is done in the literature on this subject, we do so in a passive learning setting, and with no specific constraints on the cluster shapes other than their size. We investigate the problem in different settings: i. an online setting, where we provide a tight characterization of the prediction complexity in the mistake bound model, and ii. a standard stochastic batch setting, where we give tight upper and lower bounds on the achievable generalization error. Prediction performance is measured both in terms of the ability to recover the similarity function encoding the hidden clustering and in terms of how well we classify each item within the set. The proposed algorithms are time efficient.},
  day            = {9},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@InCollection{Patel-Thakral-2016,
  author               = {Patel, K. M. Archana and Thakral, Prateek},
  booktitle            = {International Conference on Communication and Signal Processing (ICCSP)},
  date                 = {2016-04},
  title                = {The best clustering algorithms in data mining},
  doi                  = {10.1109/iccsp.2016.7754534},
  isbn                 = {978-1-5090-0396-9},
  location             = {Melmaruvathur, Tamilnadu, India},
  pages                = {2042--2046},
  publisher            = {IEEE},
  abstract             = {In data mining, Clustering is the most popular, powerful and commonly used unsupervised learning technique. It is a way of locating similar data objects into clusters based on some similarity. Clustering algorithms can be categorized into seven groups, namely Hierarchical clustering algorithm, Density-based clustering algorithm, Partitioning clustering algorithm, Graph-based algorithm, Grid-based algorithm, Model-based clustering algorithm and Combinational clustering algorithm. These clustering algorithms give different result according to the conditions. Some clustering techniques are better for large data set and some gives good result for finding cluster with arbitrary shapes. This paper is planned to learn and relates various data mining clustering algorithms. Algorithms which are under exploration as follows: K-Means algorithm, K-Medoids, Distributed K-Means clustering algorithm, Hierarchical clustering algorithm, Grid-based Algorithm and Density based clustering algorithm. This paper compared all these clustering algorithms according to the many factors. After comparison of these clustering algorithms I describe that which clustering algorithms should be used in different conditions for getting the best result.},
  citeulike-article-id = {14335037},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/iccsp.2016.7754534},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-04-10 12:14:30},
  timestamp            = {2020-02-27 04:04},
}

@Article{Pattarin-et-al-2004,
  author               = {Pattarin, Francesco and Paterlini, Sandra and Minerva, Tommaso},
  date                 = {2004-09},
  journaltitle         = {Computational Statistics \& Data Analysis},
  title                = {Clustering financial time series: an application to mutual funds style analysis},
  doi                  = {10.1016/j.csda.2003.11.009},
  issn                 = {0167-9473},
  number               = {2},
  pages                = {353--372},
  volume               = {47},
  abstract             = {Classification can be useful in giving a synthetic and informative description of contexts characterized by high degrees of complexity. Different approaches could be adopted to tackle the classification problem: statistical tools may contribute to increase the degree of confidence in the classification scheme. A classification algorithm for mutual funds style analysis is proposed, which combines different statistical techniques and exploits information readily available at low cost. Objective, representative, consistent and empirically testable classification schemes are strongly sought for in this field in order to give reliable information to investors and fund managers who are interested in evaluating and comparing different financial products. Institutional classification schemes, when available, do not always provide consistent and representative peer groups of funds. A "return-based"classification scheme is proposed, which aims at identifying mutual funds' styles by analysing time series of past returns. The proposed classification procedure consists of three basic steps: (a) a dimensionality reduction step based on principal component analysis, (b) a clustering step that exploits a robust evolutionary clustering methodology, and (c) a style identification step via a constrained regression model first proposed by William Sharpe. The algorithm is tested on a sample of Italian mutual funds and achieves satisfactory results with respect to (i) the agreement with the existing institutional classification and (ii) the explanatory power of out of sample variability in the cross-section of returns.},
  citeulike-article-id = {14367330},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.csda.2003.11.009},
  groups               = {Networks and investment management, Clustering and network analysis},
  posted-at            = {2017-06-02 22:29:31},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Peker-Aktan-2014,
  author               = {Peker, Sinem and Aktan, Bora},
  booktitle            = {The 8th International Scientific Conference "Business and Management 2014"},
  date                 = {2014},
  title                = {Clustering In European Stock Indices In Crisis And Non-Crisis Periods},
  doi                  = {10.3846/bm.2014.037},
  isbn                 = {9786094576508},
  location             = {Vilnius, Lithuania},
  publisher            = {Vilnius Gediminas Technical University Publishing House Technika},
  abstract             = {Grouping the major indices of stock markets based on their homogeneities may facilitate the selection period for investors especially today's information rich financial world. This paper attempts to detect and group the homogenous stock indices in Europe both throughout the crisis and non-crisis periods. The daily index returns of leading stock exchanges over the period 03.01.2007-09.04.2013 are considered; one of the hierarchical clustering techniques so-called Ward's Method is applied and similar cases are evaluated respectively. Then, Wilcoxon signed rank test is employed for the same periods on daily index returns and meaningful differences are found.},
  citeulike-article-id = {14149920},
  citeulike-linkout-0  = {http://dx.doi.org/10.3846/bm.2014.037},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-10-01 13:28:33},
  timestamp            = {2020-02-27 04:04},
}

@Article{Peng-et-al-2016,
  author               = {Peng, Chong and Kang, Zhao and Yang, Ming and Cheng, Qiang},
  date                 = {2016-07},
  journaltitle         = {IEEE Signal Processing Letters},
  title                = {Feature Selection Embedded Subspace Clustering},
  doi                  = {10.1109/lsp.2016.2573159},
  issn                 = {1070-9908},
  number               = {7},
  pages                = {1018--1022},
  volume               = {23},
  abstract             = {We propose a new subspace clustering method that integrates feature selection into subspace clustering. Rather than using all features to construct a low-rank representation of the data, we find such a representation using only relevant features, which helps in revealing more accurate data relationships. Two variants are proposed by using both convex and nonconvex rank approximations. Extensive experimental results confirm the effectiveness of the proposed method and models.},
  citeulike-article-id = {14351207},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/lsp.2016.2573159},
  posted-at            = {2017-05-05 01:22:20},
  timestamp            = {2020-02-27 04:04},
}

@Article{Piccardi-et-al-2011,
  author               = {Piccardi, Carlo and Calatroni, Lisa and Bertoni, Fabio},
  date                 = {2011-01},
  journaltitle         = {International Journal of Modern Physics C},
  title                = {Clustering financial time series by network community analysis},
  doi                  = {10.1142/s012918311101604x},
  number               = {01},
  pages                = {35--50},
  volume               = {22},
  abstract             = {In this paper, we describe a method for clustering financial time series which is based on community analysis, a recently developed approach for partitioning the nodes of a network (graph). A network with N nodes is associated to the set of N time series. The weight of the link (i, j), which quantifies the similarity between the two corresponding time series, is defined according to a metric based on symbolic time series analysis, which has recently proved effective in the context of financial time series. Then, searching for network communities allows one to identify groups of nodes (and then time series) with strong similarity. A quantitative assessment of the significance of the obtained partition is also provided. The method is applied to two distinct case-studies concerning the US and Italy Stock Exchange, respectively. In the US case, the stability of the partitions over time is also thoroughly investigated. The results favorably compare with those obtained with the standard tools typically used for clustering financial time series, such as the minimal spanning tree and the hierarchical tree.},
  citeulike-article-id = {14150069},
  citeulike-linkout-0  = {http://dx.doi.org/10.1142/s012918311101604x},
  citeulike-linkout-1  = {http://www.worldscientific.com/doi/abs/10.1142/S012918311101604X},
  day                  = {1},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:30:18},
  publisher            = {World Scientific Publishing Co.},
  timestamp            = {2020-02-27 04:04},
}

@Article{Ponta-Carbone-2019,
  author         = {Ponta, L. and Carbone, A.},
  date           = {2019-08-01},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Quantifying horizon dependence of asset prices: a cluster entropy approach},
  url            = {https://arxiv.org/abs/1908.00257},
  urldate        = {2019-08-11},
  abstract       = {Market dynamic is studied by quantifying the dependence of the entropy S(,n) of the clusters formed by the series of the prices pt and its moving average pt,n on temporal horizon M. We report results of the analysis performed on high-frequency data of the Nasdaq Composite, Dow Jones Industrial Avg and Standard \& Poor 500 indexes downloaded from the Bloomberg terminal this http URL. Both raw and sampled data series have been analysed for a broad range of horizons M, varying from one to twelve months over the year 2018. A systematic dependence of the cluster entropy function S(,n) on the horizon M has been evidenced in the analysed assets. Hence, the cluster entropy function is integrated over the cluster to yield a synthetic indicator of price evolution: the Market Dynamic Index I(M,n). Moreover, the Market Horizon Dependence defined as H(M,n)=I(M,n)-I(1,n) is calculated and compared with the values of the horizon dependence of the pricing kernel with different representative agent models obtained by a Kullback-Leibler entropy approach.},
  day            = {1},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Raffinot-2018,
  author               = {Raffinot, Thomas},
  date                 = {2017-12-22},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Hierarchical Clustering-Based Asset Allocation},
  doi                  = {10.3905/jpm.2018.44.2.089},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {89--99},
  volume               = {44},
  abstract             = {This article proposes a hierarchical clustering-based asset allocation method, which uses graph theory and machine learning techniques. Hierarchical clustering refers to the formation of a recursive clustering, suggested by the data, not defined a priori. Several hierarchical clustering methods are presented and tested. Once the assets are hierarchically clustered, the authors compute a simple and efficient capital allocation within and across clusters of assets, so that many correlated assets receive the same total allocation as a single uncorrelated one. The out-of-sample performances of hierarchical clustering-based portfolios and more traditional risk-based portfolios are evaluated across three disparate datasets, which differ in term of the number of assets and the assets' composition. To avoid data snooping, the authors assess the comparison of profit measures using the bootstrap-based model confidence set procedure. Their empirical results indicate that hierarchical clustering-based portfolios are robust and truly diversified and achieve statistically better risk-adjusted performances than commonly used portfolio optimization techniques.},
  citeulike-article-id = {14510373},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2018.44.2.089},
  day                  = {22},
  groups               = {Network_Invest, ML_Network_QWIM, PortfOptim_Network, Invest_Network, ML_InvestSelect},
  posted-at            = {2017-12-30 13:27:26},
  timestamp            = {2020-02-27 04:04},
}

@Article{Rastelli-Friel-2017,
  author         = {Rastelli, Riccardo and Friel, Nial},
  date           = {2017-10-31},
  journaltitle   = {Statistics and Computing},
  title          = {Optimal Bayesian estimators for latent variable cluster models},
  doi            = {10.1007/s11222-017-9786-y},
  issn           = {0960-3174},
  pages          = {1--18},
  abstract       = {In cluster analysis interest lies in probabilistically capturing partitions of individuals, items or observations into groups, such that those belonging to the same group share similar attributes or relational profiles. Bayesian posterior samples for the latent allocation variables can be effectively obtained in a wide range of clustering models, including finite mixtures, infinite mixtures, hidden Markov models and block models for networks. However, due to the categorical nature of the clustering variables and the lack of scalable algorithms, summary tools that can interpret such samples are not available. We adopt a Bayesian decision theoretical approach to define an optimality criterion for clusterings and propose a fast and context-independent greedy algorithm to find the best allocations. One important facet of our approach is that the optimal number of groups is automatically selected, thereby solving the clustering and the model-choice problems at the same time. We consider several loss functions to compare partitions and show that our approach can accommodate a wide range of cases. Finally, we illustrate our approach on both artificial and real datasets for three different clustering models: Gaussian mixtures, stochastic block models and latent block models for networks.},
  day            = {31},
  f1000-projects = {QuantInvest},
  groups         = {Proba_Bayes},
  timestamp      = {2020-02-27 04:04},
}

@Article{Ren-et-al-2016,
  author               = {Ren, Fei and Lu, Ya-Nan and Li, Sai-Ping and Jiang, Xiong-Fei and Zhong, Li-Xin and Qiu, Tian},
  date                 = {2016-08},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Dynamic portfolio strategy using clustering approach},
  eprint               = {1608.03058},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1608.03058},
  abstract             = {The problem of portfolio optimization is one of the most important issues in asset management. This paper proposes a new dynamic portfolio strategy based on the time-varying structures of MST networks in Chinese stock markets, where the market condition is further considered when using the optimal portfolios for investment. A portfolio strategy comprises two stages: selecting the portfolios by choosing central and peripheral stocks in the selection horizon using five topological parameters, i.e., degree, betweenness centrality, distance on degree criterion, distance on correlation criterion and distance on distance criterion, then using the portfolios for investment in the investment horizon. The optimal portfolio is chosen by comparing central and peripheral portfolios under different combinations of market conditions in the selection and investment horizons. Market conditions in our paper are identified by the ratios of the number of trading days with rising index or the sum of the amplitudes of the trading days with rising index to the total number of trading days. We find that central portfolios outperform peripheral portfolios when the market is under a drawup condition, or when the market is stable or drawup in the selection horizon and is under a stable condition in the investment horizon. We also find that the peripheral portfolios gain more than central portfolios when the market is stable in the selection horizon and is drawdown in the investment horizon. Empirical tests are carried out based on the optimal portfolio strategy. Among all the possible optimal portfolio strategy based on different parameters to select portfolios and different criteria to identify market conditions, 65dollar; of our optimal portfolio strategies outperform the random strategy for the Shanghai A-Share market and the proportion is 70dollar; for the Shenzhen A-Share market.},
  citeulike-article-id = {14148628},
  citeulike-linkout-0  = {http://arxiv.org/abs/1608.03058},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1608.03058},
  day                  = {10},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest, Invest_Dynamic, Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-09-28 20:56:42},
  timestamp            = {2020-02-27 04:04},
}

@Article{Ren-et-al-2017,
  author               = {Ren, Fei and Lu, Ya-Nan and Li, Sai-Ping and Jiang, Xiong-Fei and Zhong, Li-Xin and Qiu, Tian},
  date                 = {2017-01},
  journaltitle         = {PLOS ONE},
  title                = {Dynamic Portfolio Strategy Using Clustering Approach},
  doi                  = {10.1371/journal.pone.0169299},
  number               = {1},
  pages                = {e0169299+},
  volume               = {12},
  abstract             = {The problem of portfolio optimization is one of the most important issues in asset management. We here propose a new dynamic portfolio strategy based on the time-varying structures of MST networks in Chinese stock markets, where the market condition is further considered when using the optimal portfolios for investment. A portfolio strategy comprises two stages: First, select the portfolios by choosing central and peripheral stocks in the selection horizon using five topological parameters, namely degree, betweenness centrality, distance on degree criterion, distance on correlation criterion and distance on distance criterion. Second, use the portfolios for investment in the investment horizon. The optimal portfolio is chosen by comparing central and peripheral portfolios under different combinations of market conditions in the selection and investment horizons. Market conditions in our paper are identified by the ratios of the number of trading days with rising index to the total number of trading days, or the sum of the amplitudes of the trading days with rising index to the sum of the amplitudes of the total trading days. We find that central portfolios outperform peripheral portfolios when the market is under a drawup condition, or when the market is stable or drawup in the selection horizon and is under a stable condition in the investment horizon. We also find that peripheral portfolios gain more than central portfolios when the market is stable in the selection horizon and is drawdown in the investment horizon. Empirical tests are carried out based on the optimal portfolio strategy. Among all possible optimal portfolio strategies based on different parameters to select portfolios and different criteria to identify market conditions, 65 percent of our optimal portfolio strategies outperform the random strategy for the Shanghai A-Share market while the proportion is 70 percent for the Shenzhen A-Share market.},
  citeulike-article-id = {14291490},
  citeulike-linkout-0  = {http://dx.doi.org/10.1371/journal.pone.0169299},
  day                  = {27},
  groups               = {Networks and investment management, Network_Invest, PortfOptim_Dynamic, PortfOptim_Network, Invest_Network},
  posted-at            = {2017-03-03 18:42:12},
  publisher            = {Public Library of Science},
  timestamp            = {2020-02-27 04:04},
}

@Article{Roick-et-al-2019,
  author         = {Roick, Tyler and Karlis, Dimitris and McNicholas, Paul D.},
  date           = {2019-01-26},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Clustering Discrete Valued Time Series},
  url            = {https://arxiv.org/abs/1901.09249},
  urldate        = {2019-03-07},
  abstract       = {There is a need for the development of models that are able to account for discreteness in data, along with its time series properties and correlation. Our focus falls on INteger-valued AutoRegressive (INAR) type models. The INAR type models can be used in conjunction with existing model-based clustering techniques to cluster discrete valued time series data. With the use of a finite mixture model, several existing techniques such as the selection of the number of clusters, estimation using expectation-maximization and model selection are applicable. The proposed model is then demonstrated on real data to illustrate its clustering applications.},
  day            = {26},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Ronan-et-al-2018,
  author       = {Tom Ronan and Shawn Anastasio and Zhijie Qi and Pedro Henrique S. Vieira Tavares and Roman Sloutsky and Kristen M. Naegle},
  date         = {2018},
  journaltitle = {Journal of Machine Learning Research},
  title        = {OpenEnsembles: A Python Resource for Ensemble Clustering},
  number       = {26},
  pages        = {1--6},
  url          = {http://jmlr.org/papers/v19/18-100.html},
  volume       = {19},
  abstract     = {In this paper we introduce OpenEnsembles, a Python toolkit for performing and analyzing ensemble clustering. Ensemble clustering is the process of creating many clustering solutions for a given dataset and utilizing the relationships observed across the ensemble to identify final solutions, which are more robust, stable or better than the individual solutions within the ensemble. The OpenEnsembles library provides a unified interface for applying transformations to data, clustering data, visualizing individual clustering solutions, visualizing and finishing the ensemble, and calculating validation metrics for a clustering solution for any given partitioning of the data. We have documented examples of using OpenEnsembles to create, analyze, and visualize a number of different types of ensemble approaches on toy and example datasets. OpenEnsembles is released under the GNU General Public License version 3, can be installed via Conda or the Python Package Index (pip), and is available at https://github.com/NaegleLab/OpenEnsembles.},
  timestamp    = {2020-02-27 04:04},
}

@Article{Ross-2015a,
  author               = {Ross, Gordon J.},
  date                 = {2015-05},
  journaltitle         = {Physical Review E},
  title                = {Dynamic Multi-Factor Clustering of Financial Networks},
  doi                  = {10.1103/physreve.89.022809},
  eprint               = {1505.01550},
  eprinttype           = {arXiv},
  issn                 = {1539-3755},
  number               = {2},
  volume               = {89},
  abstract             = {We investigate the tendency for financial instruments to form clusters when there are multiple factors influencing the correlation structure. Specifically, we consider a stock portfolio which contains companies from different industrial sectors, located in several different countries. Both sector membership and geography combine to create a complex clustering structure where companies seem to first be divided based on sector, with geographical subclusters emerging within each industrial sector. We argue that standard techniques for detecting overlapping clusters and communities are not able to capture this type of structure, and show how robust regression techniques can instead be used to remove the influence of both sector and geography from the correlation matrix separately. Our analysis reveals that prior to the 2008 financial crisis, companies did not tend to form clusters based on geography. This changed immediately following the crisis, with geography becoming a more important determinant of clustering.},
  citeulike-article-id = {14150018},
  citeulike-linkout-0  = {http://arxiv.org/abs/1505.01550},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1505.01550},
  citeulike-linkout-2  = {http://dx.doi.org/10.1103/physreve.89.022809},
  day                  = {7},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-10-01 17:00:42},
  timestamp            = {2020-02-27 04:04},
}

@Article{Roy-et-al-2017,
  author               = {Roy, Aurko and Pokutta, Sebastian},
  date                 = {2017},
  journaltitle         = {Journal of Machine Learning Research},
  title                = {Hierarchical Clustering via Spreading Metrics},
  number               = {88},
  pages                = {1--35},
  url                  = {http://jmlr.org/papers/v18/17-081.html},
  volume               = {18},
  abstract             = {We study the cost function for hierarchical clusterings introduced by (Dasgupta, 2016) where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters. It was also shown in (Dasgupta, 2016) that a top-down algorithm based on the uniform Sparsest Cut problem returns a hierarchical clustering of cost at most O(alpha n log n)O(alpha n log n) times the cost of the optimal hierarchical clustering, where alpha is the approximation ratio of the Sparsest Cut subroutine used. Thus using the best known approximation algorithm for Sparsest Cut due to Arora-Rao- Vazirani, the top-down algorithm returns a hierarchical clustering of cost at most O(log 3/2n)O(log 3/2n) times the cost of the optimal solution. We improve this by giving an O(log n)O(log n))-approximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by using the idea of sphere growing which has been extensively used in the context of graph partitioning. We also prove that our algorithm returns an O(log n)O(log n)- approximate hierarchical clustering for a generalization of this cost function also studied in (Dasgupta, 2016). Experiments show that the hierarchies found by using the ILP formulation as well as our rounding algorithm often have better projections into flat clusters than the standard linkage based algorithms. We conclude with constant factor inapproximability results for this problem: 1) no polynomial size LP or SDP can achieve a constant factor approximation for this problem and 2) no polynomial time algorithm can achieve a constant factor approximation under the Small Set Expansion hypothesis.},
  citeulike-article-id = {14509660},
  citeulike-linkout-0  = {http://jmlr.org/papers/v18/17-081.html},
  keywords             = {*file-import-17-12-29},
  posted-at            = {2017-12-29 01:34:17},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Roy-Mandal-2015,
  author               = {Roy, Parthajit and Mandal, J. K.},
  booktitle            = {Computational Intelligence in Data Mining - Volume 3},
  date                 = {2015},
  title                = {Performance Evaluation of Some Clustering Indices},
  doi                  = {10.1007/978-81-322-2202-6\_46},
  editor               = {Jain, Lakhmi C. and Behera, Himansu S. and Mandal, Jyotsna K. and Mohapatra, Durga P.},
  pages                = {509--517},
  publisher            = {Springer India},
  series               = {Smart Innovation, Systems and Technologies},
  url                  = {http://dx.doi.org/10.1007/978-81-322-2202-6\_46},
  volume               = {33},
  abstract             = {This paper analyzes the performances of four internal and five external cluster validity indices. The internal indices are Banfeld-Raftery index, Davies-Bouldin index, Ray-Turi index and Scott-Symons index. Jaccard index, Folkes-Mallows index, Rand index, Rogers-Tanimoto index and Kulczynski index are the external indices considered. The standard K-Means algorithm and CLARA algorithm has been considered as testing models. Four standard data sets, namely Iris, Seeds, Wine and Flame data sets has been chosen for testing the performance of the indices. The performance of the indices with the increasing number of parameters of the data set is measured. The results are compared and analyzed.},
  citeulike-article-id = {14435141},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-81-322-2202-646},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-81-322-2202-646},
  posted-at            = {2017-09-21 00:37:03},
  timestamp            = {2020-02-27 04:04},
}

@Article{Ryazanov-2016,
  author               = {Ryazanov, Vladimir},
  date                 = {2016-07},
  journaltitle         = {Intelligent Data Analysis},
  title                = {About estimation of quality of clustering results via its stability},
  doi                  = {10.3233/ida-160842},
  issn                 = {1088-467X},
  number               = {s1},
  pages                = {S5--S15},
  volume               = {20},
  abstract             = {We know that there are many clustering methods for the case of a known/unknown number of clusters. Clustering is a result of fulfillment of some stopping criterion. Usually, optimisation of some quality criterion is performed or iterative processes are accomplished. How to estimate the quality of clustering obtained by some method? Is the obtained clustering result corresponding to the objective reality or some stopping criterion of the algorithm is made and we have obtained only some partition? Here, a practical approach and the common general criteria based on an estimation of the stability of clustering are submitted. The criterion does not use any probabilistic assumptions or distances in feature space. For some well-known clustering algorithms, efficient methods for computing the introduced stability criteria according to the training set are obtained. Some illustrative real and artificial examples for various situations are shown.},
  citeulike-article-id = {14357928},
  citeulike-linkout-0  = {http://dx.doi.org/10.3233/ida-160842},
  day                  = {13},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-05-16 14:07:47},
  timestamp            = {2020-02-27 04:04},
}

@Article{Saha-et-al-2015,
  author               = {Saha, Biswajit and Mandal, Amitabha and Tripathy, Soumendu B. and Mukherjee, Debaprasad},
  date                 = {2015-03},
  journaltitle         = {arXiv Electronic Journal},
  title                = {Complex Networks, Communities and Clustering: A survey},
  eprint               = {1503.06277},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1503.06277},
  abstract             = {This paper is an extensive survey of literature on complex network communities and clustering. Complex networks describe a widespread variety of systems in nature and society especially systems composed by a large number of highly interconnected dynamical entities. Complex networks like real networks can also have community structure. There are several types of methods and algorithms for detection and identification of communities in complex networks. Several complex networks have the property of clustering or network transitivity. Some of the important concepts in the field of complex networks are small-world and scale-robustness, degree distributions, clustering, network correlations, random graph models, models of network growth, dynamical processes on networks, etc. Some current areas of research on complex network communities are those on community evolution, overlapping communities, communities in directed networks, community characterization and interpretation, etc. Many of the algorithms or methods proposed for network community detection through clustering are modified versions of or inspired from the concepts of minimum-cut based algorithms, hierarchical connectivity based algorithms, the original GirvanNewman algorithm, concepts of modularity maximization, algorithms utilizing metrics from information and coding theory, and clique based algorithms.},
  citeulike-article-id = {14150071},
  citeulike-linkout-0  = {http://arxiv.org/abs/1503.06277},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1503.06277},
  day                  = {21},
  groups               = {Clustering and network analysis},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:32:16},
  timestamp            = {2020-02-27 04:04},
}

@Article{Sakakibara-et-al-2015,
  author               = {Sakakibara, Takumasa and Matsui, Tohgoroh and Mutoh, Atsuko and Inuzuka, Nobuhiro},
  date                 = {2015},
  journaltitle         = {Procedia Computer Science},
  title                = {Clustering Mutual Funds Based on Investment Similarity},
  doi                  = {10.1016/j.procs.2015.08.251},
  issn                 = {1877-0509},
  pages                = {881--890},
  volume               = {60},
  abstract             = {It is risky to invest to single or similar mutual funds because the variance of the return becomes large. Mutual funds are categorized based on the investment strategy by a company that rated funds based on performance, but the fund categories are different from its actual operations. While some previous studies have proposed methods to cluster mutual funds based on the historical performances, we cannot apply these methods to new mutual funds. In this paper, we clusters mutual funds based on the investment similarity instead of the historical performances. The contributions of this paper are: 1. To propose two new methods for classifying mutual funds based on the investment similarity, 2. To evaluate the proposed methods based on actual 551 Japanese mutual funds.},
  citeulike-article-id = {14212405},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.procs.2015.08.251},
  groups               = {Invest_Network},
  owner                = {cristi},
  posted-at            = {2016-11-21 22:10:38},
  timestamp            = {2020-02-27 04:04},
}

@Article{SardaEspinosa-2017,
  author       = {Alexis Sarda-Espinosa},
  date         = {2019},
  journaltitle = {The R Journal},
  title        = {Comparing Time-Series Clustering Algorithms in R Using the dtwclust Package},
  url          = {https://journal.r-project.org/archive/2019/RJ-2019-023/index.html},
  abstract     = {Most clustering strategies have not changed considerably since their initial definition. Most of the improvements are either related to the distance measure used to assess dissimilarity, or the function used to calculate prototypes or centroids. Time-series clustering is no exception, with the Dynamic Time Warping distance being particularly popular in that context. This distance is computationally expensive, so many related optimizations have been developed over the years. Since no single clustering algorithm can be said to perform best on all datasets, different strategies must be tested and compared, so a common infrastructure can be advantageous. In this manuscript, a general overview of time-series clustering is given, including many specifics related to Dynamic Time Warping and other recently proposed techniques. At the same time, a description of the dtwclust package for the R statistical software is provided, showcasing how it can be used to evaluate many different time-series clustering procedures.},
  howpublished = {Available at https://cran.r-project.org/web/packages/dtwclust/vignettes/dtwclust.pdf},
  timestamp    = {2020-02-27 04:04},
}

@Article{Schmitt-Westerhoff-2017,
  author               = {Schmitt, Noemi and Westerhoff, Frank},
  date                 = {2017-02},
  journaltitle         = {Quantitative Finance},
  title                = {Herding behaviour and volatility clustering in financial markets},
  doi                  = {10.1080/14697688.2016.1267391},
  pages                = {1--17},
  abstract             = {We propose a financial market model in which speculators follow a linear mix of technical and fundamental trading rules to determine their orders. Volatility clustering arises in our model due to speculators? herding behaviour. In case of heightened uncertainty, speculators observe other speculators? actions more closely. Since speculators? trading behaviour then becomes less heterogeneous, the market maker faces a less balanced excess demand and consequently adjusts prices more strongly. Estimating our model using the method of simulated moments reveals that it is able to explain a number of stylized facts of financial markets quite well. Various robustness checks with respect to the model setup reveal that our results are quite stable.},
  citeulike-article-id = {14316708},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2016.1267391},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2016.1267391},
  day                  = {1},
  groups               = {Networks and investment management, Clustering and network analysis, Vol_Cluster},
  posted-at            = {2017-03-23 08:51:27},
  publisher            = {Routledge},
  timestamp            = {2020-02-27 04:04},
}

@Article{Sekula-et-al-2017,
  author               = {Sekula, Michael and Datta, Somnath and Datta, Susmita},
  date                 = {2017-03},
  journaltitle         = {Bioinformation},
  title                = {optCluster: An R Package for Determining the Optimal Clustering Algorithm},
  doi                  = {10.6026/97320630013101},
  issn                 = {0973-8894},
  number               = {03},
  pages                = {101--103},
  volume               = {13},
  abstract             = {There exist numerous programs and packages that perform validation for a given clustering solution; however, clustering algorithms fare differently as judged by different validation measures. If more than one performance measure is used to evaluate multiple clustering partitions, an optimal result is often difficult to determine by visual inspection alone. This paper introduces optCluster, an R package that uses a single function to simultaneously compare numerous clustering partitions (created by different algorithms and/or numbers of clusters) and obtain a "best" option for a given dataset. The method of weighted rank aggregation is utilized by this package to objectively aggregate various performance measure scores, thereby taking away the guesswork that often follows a visual inspection of cluster results. The optCluster package contains biological validation measures as well as clustering algorithms developed specifically for RNA sequencing data, making it a useful tool for clustering genomic data.},
  citeulike-article-id = {14435134},
  citeulike-linkout-0  = {http://dx.doi.org/10.6026/97320630013101},
  day                  = {31},
  posted-at            = {2017-09-20 23:40:37},
  timestamp            = {2020-02-27 04:04},
}

@Article{Serviss-et-al-2017,
  author         = {Serviss, Jason T. and Gaadin, Jesper R. and Eriksson, Per and Folkersen, Lasse and Grander, Dan},
  date           = {2017-10-01},
  journaltitle   = {Bioinformatics},
  title          = {ClusterSignificance: a bioconductor package facilitating statistical analysis of class cluster separations in dimensionality reduced data},
  doi            = {10.1093/bioinformatics/btx393},
  number         = {19},
  pages          = {3126--3128},
  volume         = {33},
  abstract       = {Summary: Multi-dimensional data generated via high-throughput experiments is increasingly used in conjunction with dimensionality reduction methods to ascertain if resulting separations of the data correspond with known classes. This is particularly useful to determine if a subset of the variables, e.g. genes in a specific pathway, alone can separate samples into these established classes. Despite this, the evaluation of class separations is often subjective and performed via visualization. Here we present the ClusterSignificance package; a set of tools designed to assess the statistical significance of class separations downstream of dimensionality reduction algorithms. In addition, we demonstrate the design and utility of the ClusterSignificance package and utilize it to determine the importance of long non-coding RNA expression in the identity of multiple hematological malignancies. Availability and implementation: ClusterSignificance is an R package available via Bioconductor (https://bioconductor.org/packages/ClusterSignificance) under GPL-3. Contact: dan.grander@ki.se. Supplementary information: Supplementary data are available at Bioinformatics online.},
  day            = {1},
  f1000-projects = {QuantInvest},
  pmid           = {28957498},
  timestamp      = {2020-02-27 04:04},
}

@InCollection{Sherkat-et-al-2018,
  author         = {Sherkat, Ehsan and Nourashrafeddin, Seyednaser and Milios, Evangelos E. and Minghim, Rosane},
  booktitle      = {Proceedings of the 2018 Conference on Human Information Interaction\&Retrieval - IUI '18},
  date           = {2018-03-07},
  title          = {Interactive document clustering revisited: A visual analytics approach},
  doi            = {10.1145/3172944.3172964},
  isbn           = {9781450349451},
  location       = {New York, New York, USA},
  pages          = {281--292},
  publisher      = {ACM Press},
  url            = {http://dl.acm.org/citation.cfm?doid=3172944.3172964},
  abstract       = {Document clustering is an efficient way to get insight into large text collections. Due to the personalized nature of document clustering, even the best fully automatic algorithms cannot create clusters that accurately reflect the user's perspectives. To incorporate the users perspective in the clustering process and, at the same time, effectively visualize document collections to enhance user's sense-making of data, we propose a novel visual analytics system for interactive document clustering. We built our system on top of clustering algorithms that can adapt to user's feedback. First, the initial clustering is created based on the user-defined number of clusters and the selected clustering algorithm. Second, the clustering result is visualized to the user. A collection of coordinated visualization modules and document projection is designed to guide the user towards a better insight into the document collection and clusters. The user changes clusters and key-terms iteratively as a feedback to the clustering algorithm until the result is satisfactory. In key-term based interaction, the user assigns a set of key-terms to each target cluster to guide the clustering algorithm. A set of quantitative experiments, a use case, and a user study have been conducted to show the advantages of the approach for document analytics based on clustering.},
  day            = {7},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Song-et-al-2012b,
  author               = {Song, Won-Min and Di Matteo, T. and Aste, Tomaso},
  date                 = {2012-03},
  journaltitle         = {PLoS ONE},
  title                = {Hierarchical Information Clustering by Means of Topologically Embedded Graphs},
  doi                  = {10.1371/journal.pone.0031929},
  number               = {3},
  pages                = {e31929+},
  volume               = {7},
  abstract             = {We introduce a graph-theoretic approach to extract clusters and hierarchies in complex data-sets in an unsupervised and deterministic manner, without the use of any prior information. This is achieved by building topologically embedded networks containing the subset of most significant links and analyzing the network structure. For a planar embedding, this method provides both the intra-cluster hierarchy, which describes the way clusters are composed, and the inter-cluster hierarchy which describes how clusters gather together. We discuss performance, robustness and reliability of this method by first investigating several artificial data-sets, finding that it can outperform significantly other established approaches. Then we show that our method can successfully differentiate meaningful clusters and hierarchies in a variety of real data-sets. In particular, we find that the application to gene expression patterns of lymphoma samples uncovers biologically significant groups of genes which play key-roles in diagnosis, prognosis and treatment of some of the most relevant human lymphoid malignancies.},
  citeulike-article-id = {10441261},
  citeulike-linkout-0  = {http://dx.doi.org/10.1371/journal.pone.0031929},
  day                  = {9},
  owner                = {cristi},
  posted-at            = {2016-09-27 15:16:15},
  publisher            = {Public Library of Science},
  timestamp            = {2020-02-27 04:04},
}

@Article{So-Yip-2012,
  author               = {So, Mike K. P. and Yip, Iris W. H.},
  date                 = {2012-08},
  journaltitle         = {Journal of Forecasting},
  title                = {Multivariate GARCH Models with Correlation Clustering},
  doi                  = {10.1002/for.1234},
  number               = {5},
  pages                = {443--468},
  volume               = {31},
  abstract             = {A new clustered correlation multivariate generalized autoregressive conditional heteroskedasticity (CC-MGARCH) model that allows conditional correlations to form clusters is proposed. This model generalizes the time-varying correlation structure of Tse and Tsui (2002, Journal of Business and Economic Statistics 20: 351-361) by classifying the correlations among the series into groups.

To estimate the proposed model, Markov chain Monte Carlo methods are adopted. Two efficient sampling schemes for drawing discrete indicators are also developed. Simulations show that these efficient sampling schemes can lead to substantial savings in computation time in Monte Carlo procedures involving discrete indicators.

Empirical examples using stock market and exchange rate data are presented in which two-cluster and three-cluster models are selected using posterior probabilities. This implies that the conditional correlation equation is likely to be governed by more than one set of decaying parameters.},
  citeulike-article-id = {13935025},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.1234},
  day                  = {1},
  owner                = {cristi},
  posted-at            = {2016-02-18 07:21:10},
  publisher            = {John Wiley and Sons, Ltd},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Strapasson-et-al-2016,
  author               = {Strapasson, Joao E. and Pinele, Julianna and Costa, Sueli I. R.},
  booktitle            = {IEEE Sensor Array and Multichannel Signal Processing Workshop (SAM)},
  date                 = {2016-07},
  title                = {Clustering using the Fisher-Rao distance},
  doi                  = {10.1109/sam.2016.7569717},
  isbn                 = {978-1-5090-2103-1},
  location             = {Rio de Janerio, Brazil},
  pages                = {1--5},
  publisher            = {IEEE},
  abstract             = {In this paper we consider the Fisher-Rao distance in the space of the multivariate diagonal Gaussian distributions for clustering methods. Centroids in this space are derived and used to introduce two clustering algorithms for diagonal Gaussian mixture models associated to this metric: the k-means and the hierarchical clustering. These algorithms allow to reduce the number of components of such mixture models in the context of image segmentation. The algorithms presented here are compared with the Bregman hard and hierarchical clustering algorithms regarding the advantages of each method in different situations.},
  citeulike-article-id = {14335039},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/sam.2016.7569717},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-04-10 12:16:22},
  timestamp            = {2020-02-27 04:04},
}

@Article{Tang-et-al-2017,
  author         = {Tang, Yang and Browne, Ryan P. and McNicholas, Paul D.},
  date           = {2017-05-09},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Flexible Clustering for High-Dimensional Data via Mixtures of Joint Generalized Hyperbolic Models},
  url            = {https://arxiv.org/abs/1705.03130},
  abstract       = {A mixture of joint generalized hyperbolic distributions (MJGHD) is introduced for asymmetric clustering for high-dimensional data. The MJGHD approach takes into account the cluster-specific subspace, thereby limiting the number of parameters to estimate while also facilitating visualization of results. Identifiability is discussed, and a multi-cycle ECM algorithm is outlined for parameter estimation. The MJGHD approach is illustrated on two real data sets, where the Bayesian information criterion is used for model selection.},
  day            = {9},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Taylor-2018,
  author         = {Taylor, Stephen Michael},
  date           = {2018},
  journaltitle   = {SSRN Electronic Journal},
  title          = {Clustering financial return distributions using the fisher information metric},
  doi            = {10.2139/ssrn.3182914},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3182914},
  abstract       = {Information Geometry provides a correspondence between differential geometry and statistics through the Fisher Information matrix. In particular, given two models from the same parametric family of distributions, one can define the distance between these models as the length of the shortest geodesic connecting them in a Riemannian manifold whose metric is given by the model Fisher Information matrix. One limitation that had hinder the adoption of this similarity measure in practical applications is that this distance is typically difficult to compute in a robust manner. We review such complications and provide a general form for the distance function for one parameter models. We next focus on two higher dimensional extreme value models including the Generalized Pareto and Generalized Extreme Value distributions that will be used in financial risk applications. Specifically, we first develop a technique to identify the nearest neighbors of a target security in the sense that their best fit model distributions have minimal Fisher distance to that of target. Second, we develop a hierarchical clustering technique that compares Generalized Extreme Value distributions fit to block maxima of a set of equity loss distributions to group together securities whose worst single day yearly loss distributions exhibit commonalities.},
  f1000-projects = {QuantInvest},
  groups         = {Invest_Network},
  timestamp      = {2020-02-27 04:04},
}

@Article{Teklehaymanot-et-al-2018,
  author         = {Teklehaymanot, Freweyni K. and Muma, Michael and Zoubir, Abdelhak M.},
  date           = {2018-11-29},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Robust Bayesian Cluster Enumeration},
  url            = {https://arxiv.org/abs/1811.12337},
  urldate        = {2019-04-25},
  abstract       = {A major challenge in cluster analysis is that the number of data clusters is mostly unknown and it must be estimated prior to clustering the observed data. In real-world applications, the observed data is often subject to heavy tailed noise and outliers which obscure the true underlying structure of the data. Consequently, estimating the number of clusters becomes challenging. To this end, we derive a robust cluster enumeration criterion by formulating the problem of estimating the number of clusters as maximization of the posterior probability of multivariate t candidate models. We utilize Bayes' theorem and asymptotic approximations to come up with a robust criterion that possesses a closed-form expression. Further, we refine the derivation and provide a robust cluster enumeration criterion for the finite sample regime. The robust criteria require an estimate of cluster parameters for each candidate model as an input. Hence, we propose a two-step cluster enumeration algorithm that uses the expectation maximization algorithm to partition the data and estimate cluster parameters prior to the calculation of one of the robust criteria. The performance of the proposed algorithm is tested and compared to existing cluster enumeration methods using numerical and real data experiments.},
  day            = {29},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Tellaroli-et-al-2016,
  author               = {Tellaroli, Paola and Bazzi, Marco and Donato, Michele and Brazzale, Alessandra R. and Draghici, Sorin},
  date                 = {2016-03},
  journaltitle         = {PLOS ONE},
  title                = {Cross-Clustering: A Partial Clustering Algorithm with Automatic Estimation of the Number of Clusters},
  doi                  = {10.1371/journal.pone.0152333},
  number               = {3},
  pages                = {e0152333+},
  volume               = {11},
  abstract             = {Four of the most common limitations of the many available clustering methods are: i) the lack of a proper strategy to deal with outliers; ii) the need for a good a priori estimate of the number of clusters to obtain reasonable results; iii) the lack of a method able to detect when partitioning of a specific data set is not appropriate; and iv) the dependence of the result on the initialization. Here we propose Cross-clustering (CC), a partial clustering algorithm that overcomes these four limitations by combining the principles of two well established hierarchical clustering algorithms: Ward's minimum variance and Complete-linkage. We validated CC by comparing it with a number of existing clustering methods, including Ward's and Complete-linkage. We show on both simulated and real datasets, that CC performs better than the other methods in terms of: the identification of the correct number of clusters, the identification of outliers, and the determination of real cluster memberships. We used CC to cluster samples in order to identify disease subtypes, and on gene profiles, in order to determine groups of genes with the same behavior. Results obtained on a non-biological dataset show that the method is general enough to be successfully used in such diverse applications. The algorithm has been implemented in the statistical language R and is freely available from the CRAN contributed packages repository.},
  citeulike-article-id = {14005861},
  citeulike-linkout-0  = {http://dx.doi.org/10.1371/journal.pone.0152333},
  day                  = {25},
  posted-at            = {2017-09-21 00:25:43},
  publisher            = {Public Library of Science},
  timestamp            = {2020-02-27 04:04},
}

@Book{Thrun-2018,
  author         = {Thrun, Michael Christoph},
  date           = {2018},
  title          = {Projection-Based Clustering through Self-Organization and Swarm Intelligence},
  doi            = {10.1007/978-3-658-20540-9},
  isbn           = {978-3-658-20539-3},
  location       = {Wiesbaden},
  publisher      = {Springer Fachmedien Wiesbaden},
  abstract       = {It covers aspects of unsupervised machine learning used for knowledge discovery in data science and introduces a data-driven approach to cluster analysis, the Databionic swarm(DBS). DBS consists of the 3D landscape visualization and clustering of data. The 3D landscape enables 3D printing of high-dimensional data structures.The clustering and number of clusters or an absence of cluster structure are verified by the 3D landscape at a glance. DBS is the first swarm-based technique that shows emergent properties while exploiting concepts of swarm intelligence, self-organization and the Nash equilibrium concept from game theory. It results in the elimination of a global objective function and the setting of parameters. By downloading the R package DBS can be applied to data drawn from diverse research fields and used even by non-professionals in the field of data mining.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Tola-et-al-2008,
  author               = {Tola, Vincenzo and Lillo, Fabrizio and Gallegati, Mauro and Mantegna, Rosario N.},
  date                 = {2008-01},
  journaltitle         = {Journal of Economic Dynamics and Control},
  title                = {Cluster analysis for portfolio optimization},
  doi                  = {10.1016/j.jedc.2007.01.034},
  issn                 = {0165-1889},
  number               = {1},
  pages                = {235--258},
  volume               = {32},
  abstract             = {We consider the problem of the statistical uncertainty of the correlation matrix in the optimization of a financial portfolio. By assuming idealized conditions of perfect forecast ability for the future return and volatility of stocks and short selling, we show that the use of clustering algorithms can improve the reliability of the portfolio in terms of the ratio between predicted and realized risk. Bootstrap analysis indicates that this improvement is obtained in a wide range of the parameters N (number of assets) and T (investment horizon). The predicted and realized risk level and the relative portfolio composition of the selected portfolio for a given value of the portfolio return are also investigated for each considered filtering method. We also show that several of the results obtained by assuming idealized conditions are still observed under the more realistic assumptions of no short selling and mean return and volatility forecasting based on historical data.},
  citeulike-article-id = {14148037},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jedc.2007.01.034},
  groups               = {Networks and investment management, Clustering and network analysis, Network_Invest, PortfOptim_Network, FrcstQWIM_ShortTerm, Vol_Cluster},
  owner                = {cristi},
  posted-at            = {2016-09-28 18:09:06},
  timestamp            = {2020-02-27 04:04},
}

@Article{Torrente-Brazma-2017,
  author               = {Torrente, Aurora and Brazma, Alvis},
  date                 = {2017-08},
  journaltitle         = {Bioinformatics},
  title                = {clustComp, a bioconductor package for the comparison of clustering results},
  doi                  = {10.1093/bioinformatics/btx532},
  issn                 = {1367-4803},
  abstract             = {clustComp is an open source Bioconductor package that implements different techniques for the comparison of two gene expression clustering results. These include flat versus flat and hierarchical versus flat comparisons. The visualization of the similarities is provided by means of a bipartite graph, whose layout is heuristically optimized. Its flexibility allows a suitable visualization for both small and large datasets.},
  citeulike-article-id = {14468380},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/bioinformatics/btx532},
  day                  = {23},
  posted-at            = {2017-10-28 18:52:23},
  timestamp            = {2020-02-27 04:04},
}

@Article{Turkmen-2015,
  author               = {Turkmen, Ali C.},
  date                 = {2015-08},
  journaltitle         = {arXiv Electronic Journal},
  title                = {A Review of Nonnegative Matrix Factorization Methods for Clustering},
  eprint               = {1507.03194},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1507.03194},
  abstract             = {Nonnegative Matrix Factorization (NMF) was first introduced as a low-rank matrix approximation technique, and has enjoyed a wide area of applications. Although NMF does not seem related to the clustering problem at first, it was shown that they are closely linked. In this report, we provide a gentle introduction to clustering and NMF before reviewing the theoretical relationship between them. We then explore several NMF variants, namely Sparse NMF, Projective NMF, Nonnegative Spectral Clustering and Cluster-NMF, along with their clustering interpretations.},
  citeulike-article-id = {13671514},
  citeulike-linkout-0  = {http://arxiv.org/abs/1507.03194},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1507.03194},
  day                  = {28},
  owner                = {cristi},
  posted-at            = {2016-03-24 14:49:31},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Vaquez-et-al-2019,
  author         = {Vaquez, Iago and Villar, Jose R. and Sedano, Javier and Simic, Svetlana},
  booktitle      = {14th international conference on soft computing models in industrial and environmental applications (SOCO 2019)},
  date           = {2019},
  title          = {A preliminary study on multivariate time series clustering},
  doi            = {10.1007/978-3-030-20055-8\_45},
  editor         = {Martinez Alvarez, Francisco and Troncoso Lora, Alicia and Saez Munoz, Jose Antonio and Quintian, Hector and Corchado, Emilio},
  isbn           = {978-3-030-20054-1},
  pages          = {473--480},
  publisher      = {Springer International Publishing},
  series         = {Advances in intelligent systems and computing},
  url            = {http://link.springer.com/10.1007/978-3-030-20055-8\_45},
  urldate        = {2019-10-05},
  volume         = {950},
  abstract       = {Time Series (TS) clustering is one of the most effervescent research fields due to the Big Data and the IoT explosion. The problem gets more challenging if we consider the multivariate TS. In the field of Business and Management, multivariate TS are becoming more and more interesting as they allow to match events the co-occur in time but that is hardly noticeable. In this study, Recurrent Neural Networks and transfer learning have been used to analyze each example, measuring similarities between variables. All the results are finally aggregated to create an adjacency matrix that allows extracting the groups. Proof-of-concept experimentation has been included, showing that the solution might be valid after several improvements.},
  f1000-projects = {QuantInvest},
  groups         = {ML_ClustTimeSrs},
  issn           = {2194-5357},
  timestamp      = {2020-02-27 04:04},
}

@InCollection{Veenstra-et-al-2016a,
  author               = {Veenstra, Patrick and Cooper, Colin and Phelps, Steve},
  booktitle            = {8th Computer Science and Electronic Engineering (CEEC)},
  date                 = {2016-09},
  title                = {Spectral clustering using the kNN-MST similarity graph},
  doi                  = {10.1109/ceec.2016.7835917},
  isbn                 = {978-1-5090-2050-8},
  location             = {Colchester, United Kingdom},
  pages                = {222--227},
  publisher            = {IEEE},
  abstract             = {Spectral clustering is a technique that uses the spectrum of a similarity graph to cluster data. Part of this procedure involves calculating the similarity between data points and creating a similarity graph from the resulting similarity matrix. This is ordinarily achieved by creating a k-nearest neighbour (kNN) graph. In this paper, we show the benefits of using a different similarity graph, namely the union of the kNN graph and the minimum spanning tree of the negated similarity matrix (kNN-MST). We show that this has some distinct advantages on both synthetic and real datasets. Specifically, the clustering accuracy of kNN-MST is less dependent on the choice of k than kNN is.},
  citeulike-article-id = {14444472},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/ceec.2016.7835917},
  posted-at            = {2017-10-03 08:33:03},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Veldt-et-al-2017,
  author               = {Veldt, Nate and Wirth, Anthony I. and Gleich, David F.},
  booktitle            = {Proceedings of the 26th International Conference on World Wide Web},
  date                 = {2017},
  title                = {Correlation Clustering with Low-Rank Matrices},
  doi                  = {10.1145/3038912.3052586},
  isbn                 = {978-1-4503-4913-0},
  location             = {Perth, Australia},
  pages                = {1025--1034},
  publisher            = {International World Wide Web Conferences Steering Committee},
  series               = {WWW '17},
  abstract             = {Correlation clustering is a technique for aggregating data based on qualitative information about which pairs of objects are labeled `similar' or `dissimilar.' Because the optimization problem is NP-hard, much of the previous literature focuses on finding approximation algorithms. In this paper we explore how to solve the correlation clustering objective exactly when the data to be clustered can be represented by a low-rank matrix. We prove in particular that correlation clustering can be solved in polynomial time when the underlying matrix is positive semidefinite with small constant rank, but that the task remains NP-hard in the presence of even one negative eigenvalue. Based on our theoretical results, we develop an algorithm for efficiently ``solving'' low-rank positive semidefinite correlation clustering by employing a procedure for zonotope vertex enumeration. We demonstrate the effectiveness and speed of our algorithm by using it to solve several clustering problems on both synthetic and real-world data.},
  address              = {Republic and Canton of Geneva, Switzerland},
  citeulike-article-id = {14398863},
  citeulike-linkout-0  = {http://portal.acm.org/citation.cfm?id=3038912.3052586},
  citeulike-linkout-1  = {http://dx.doi.org/10.1145/3038912.3052586},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-07-24 17:07:45},
  timestamp            = {2020-02-27 04:04},
}

@Article{Verma-et-al-2018,
  author         = {Verma, A. and Buonocore, R. J. and Di Matteo, T.},
  date           = {2018-11-14},
  journaltitle   = {Quantitative Finance},
  title          = {A cluster driven log-volatility factor model: a deepening on the source of the volatility clustering},
  doi            = {10.1080/14697688.2018.1535183},
  issn           = {1469-7688},
  pages          = {1--16},
  abstract       = {We introduce a new factor model for log volatilities that considers contributions, and performs dimensionality reduction, at a global level through the market, and at a local level through clusters and their interactions. We do not assume a-priori the number of clusters in the data, instead using the Directed Bubble Hierarchical Tree algorithm to fix the number of factors. We use the factor model to study how the log volatility contributes to volatility clustering, quantifying the strength of the volatility clustering using a new nonparametric integrated proxy. Indeed finding a link between volatility and volatility clustering, we find that a global analysis reveals that only the market contributes to the volatility clustering. A local analysis reveals that for some clusters, the cluster itself contributes statistically to the volatility clustering effect. This is significantly advantageous over other factor models, since it offers a way of selecting factors in a statistical way, whilst also keeping economically relevant factors. Finally, we show that the log volatility factor model explains a similar amount of memory to a principal components analysis factor model and an exploratory factor model.},
  day            = {14},
  f1000-projects = {QuantInvest},
  groups         = {Vol_Cluster},
  timestamp      = {2020-02-27 04:04},
}

@Article{Vinod-Viole-2017,
  author               = {Vinod, Hrishikesh D. and Viole, Fred},
  date                 = {2017},
  journaltitle         = {Computational Economics},
  title                = {Nonparametric Regression Using Clusters},
  doi                  = {10.1007/s10614-017-9713-5},
  pages                = {1--18},
  abstract             = {We present a fundamentally unique method of nonparametric regression using clusters and test it against classically established methods. We compare two nonlinear regression estimation packages called 'NNS', Viole (NNS: nonlinear nonparametric statistics, 2016), and 'np', Hayfield and Racine (J Stat Softw 27(5):1-32, 2008), with the help of a simulation using deterministic (DT) and stochastic (ST) regressor models. We find the respective coefficients of determination (R2)(R2) are close for DT models, while finding an advantage to NNS in ST and large sample cases. Regression coefficients are sometimes regarded as approximations to partial derivatives, especially in social sciences. Then, NNS alone has the ability to compute a range of partials evaluated at points within the sample and also out-of-sample. Thus NNS can provide a viable alternative to kernel based nonparametric regressions without using bandwidths for smoothing.},
  citeulike-article-id = {14398901},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10614-017-9713-5},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10614-017-9713-5},
  groups               = {Clustering and network analysis, Regression_Nonlinear},
  posted-at            = {2017-07-24 20:27:05},
  publisher            = {Springer US},
  timestamp            = {2020-02-27 04:04},
}

@Article{Viole-2017a,
  author               = {Viole, Fred},
  date                 = {2017},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Classification Using NNS Clustering Analysis},
  url                  = {https://ssrn.com/abstract=2864711},
  abstract             = {NNS stands for Nonlinear Nonparametric Statistics, henceforth "NNS". What is NNS clustering analysis? NNS clustering is a method of partitioning the joint distribution into partial moment quadrants (clustering), and assigning identifiers to observations (classification). NNS clustering is very similar to k-means clustering, and we direct the reader to Vinod and Viole [2016] for a proof and comparison between the methods. This article is intended to present working examples of several classification problems using NNS clustering analysis.

We demonstrate how NNS clustering is quite effective, as well as an alternative method NNS employs for classification tasks. We compare predictions of test sets with NNS, k-means using the "cl.predict" routine offered in R to "predict class ids or memberships from R objects representing partitions", K nearest neighbors classification using the "knn" routine in R-package "class", and a naive Bayes classification using the "e1071" package.

The methods and results presented immediately raise suspicions on the pervasive notion of dimension reduction given the consistent performance of the NNS Multivariate Regression.},
  citeulike-article-id = {14398900},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2864711},
  groups               = {Clustering and network analysis},
  posted-at            = {2017-07-24 20:26:00},
  timestamp            = {2020-02-27 04:04},
}

@Article{Wang-et-al-2013,
  author               = {Wang, Yongning and Tsay, Ruey S. and Ledolter, Johannes and Shrestha, Keshab M.},
  date                 = {2013-12},
  journaltitle         = {Journal of Forecasting},
  title                = {Forecasting Simultaneously High-Dimensional Time Series: A Robust Model-Based Clustering Approach},
  doi                  = {10.1002/for.2264},
  number               = {8},
  pages                = {673--684},
  volume               = {32},
  abstract             = {This paper considers the problem of forecasting high-dimensional time series. It employs a robust clustering approach to perform classification of the component series. Each series within a cluster is assumed to follow the same model and the data are then pooled for estimation. The classification is model-based and robust to outlier contamination. The robustness is achieved by using the intrinsic mode functions of the Hilbert-Huang transform at lower frequencies.

These functions are found to be robust to outlier contamination. The paper also compares out-of-sample forecast performance of the proposed method with several methods available in the literature. The other forecasting methods considered include vector autoregressive models with or without LASSO, group LASSO, principal component regression, and partial least squares.

The proposed method is found to perform well in out-of-sample forecasting of the monthly unemployment rates of 50 US states.},
  citeulike-article-id = {12519623},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2264},
  day                  = {1},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-02-18 07:10:43},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Wang-et-al-2017,
  author               = {Wang, Hao and Pappada, Roberta and Durante, Fabrizio and Foscolo, Enrico},
  booktitle            = {Soft Methods for Data Science},
  date                 = {2017},
  title                = {A Portfolio Diversification Strategy via Tail Dependence Clustering},
  doi                  = {10.1007/978-3-319-42972-4\_63},
  editor               = {Ferraro, Maria B. and Giordani, Paolo and Vantaggi, Barbara and Gagolewski, Marek and Angeles Gil, Mara and Grzegorzewski, Przemyslaw and Hryniewicz, Olgierd},
  pages                = {511--518},
  publisher            = {Springer International Publishing},
  series               = {Advances in Intelligent Systems and Computing},
  volume               = {456},
  abstract             = {We provide a two-stage portfolio selection procedure in order to increase the diversification benefits in a bear market. By exploiting tail dependence-based risky measures, a cluster analysis is carried out for discerning between assets with the same performance in risky scenarios. Then, the portfolio composition is determined by fixing a number of assets and by selecting only one item from each cluster. Empirical calculations on the EURO STOXX 50 prove that investing on selected assets in trouble periods may improve the performance of risk-averse investors.},
  citeulike-article-id = {14150080},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-42972-463},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-42972-463},
  groups               = {Networks and investment management, Clustering and network analysis, Diversification_Measure, Diversified_Invest, Network_Invest, Invest_Network, Invest_Diversif},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:49:54},
  timestamp            = {2020-02-27 04:04},
}

@Article{Wang-et-al-2018a,
  author         = {Wang, Min and Abrams, Zachary B and Kornblau, Steven M and Coombes, Kevin R},
  date           = {2018-01-08},
  journaltitle   = {BMC Bioinformatics},
  title          = {Thresher: determining the number of clusters while removing outliers.},
  doi            = {10.1186/s12859-017-1998-9},
  number         = {1},
  pages          = {9},
  volume         = {19},
  abstract       = {BACKGROUND: Cluster analysis is the most common unsupervised method for finding hidden groups in data. Clustering presents two main challenges: (1) finding the optimal number of clusters, and (2) removing "outliers" among the objects being clustered. Few clustering algorithms currently deal directly with the outlier problem. Furthermore, existing methods for identifying the number of clusters still have some drawbacks. Thus, there is a need for a better algorithm to tackle both challenges. RESULTS: We present a new approach, implemented in an R package called Thresher, to cluster objects in general datasets. Thresher combines ideas from principal component analysis, outlier filtering, and von Mises-Fisher mixture models in order to select the optimal number of clusters. We performed a large Monte Carlo simulation study to compare Thresher with other methods for detecting outliers and determining the number of clusters. We found that Thresher had good sensitivity and specificity for detecting and removing outliers. We also found that Thresher is the best method for estimating the optimal number of clusters when the number of objects being clustered is smaller than the number of variables used for clustering. Finally, we applied Thresher and eleven other methods to 25 sets of breast cancer data downloaded from the Gene Expression Omnibus; only Thresher consistently estimated the number of clusters to lie in the range of 4-7 that is consistent with the literature. CONCLUSIONS: Thresher is effective at automatically detecting and removing outliers. By thus cleaning the data, it produces better estimates of the optimal number of clusters when there are more variables than objects. When we applied Thresher to a variety of breast cancer datasets, it produced estimates that were both self-consistent and consistent with the literature. We expect Thresher to be useful for studying a wide variety of biological datasets.},
  day            = {8},
  f1000-projects = {QuantInvest},
  pmcid          = {PMC5759208},
  pmid           = {29310570},
  timestamp      = {2020-02-27 04:04},
}

@Article{Wenskovitch-et-al-2018,
  author         = {Wenskovitch, John and Crandell, Ian and Ramakrishnan, Naren and House, Leanna and Leman, Scotland and North, Chris},
  date           = {2018-01},
  journaltitle   = {IEEE Transactions on Visualization and Computer Graphics},
  title          = {Towards a systematic combination of dimension reduction and clustering in visual analytics.},
  doi            = {10.1109/{TVCG}.2017.2745258},
  number         = {1},
  pages          = {131--141},
  url            = {http://dx.doi.org/10.1109/{TVCG}.2017.2745258},
  volume         = {24},
  abstract       = {Dimension reduction algorithms and clustering algorithms are both frequently used techniques in visual analytics. Both families of algorithms assist analysts in performing related tasks regarding the similarity of observations and finding groups in datasets. Though initially used independently, recent works have incorporated algorithms from each family into the same visualization systems. However, these algorithmic combinations are often ad hoc or disconnected, working independently and in parallel rather than integrating some degree of interdependence. A number of design decisions must be addressed when employing dimension reduction and clustering algorithms concurrently in a visualization system, including the selection of each algorithm, the order in which they are processed, and how to present and interact with the resulting projection. This paper contributes an overview of combining dimension reduction and clustering into a visualization system, discussing the challenges inherent in developing a visualization system that makes use of both families of algorithms.},
  f1000-projects = {QuantInvest},
  groups         = {Dimens_Reduc},
  pmid           = {28866581},
  timestamp      = {2020-02-27 04:04},
}

@Article{Weylandt-et-al-2019,
  author         = {Weylandt, Michael and Nagorski, John and Allen, Genevera I.},
  date           = {2019-01-06},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Dynamic Visualization and Fast Computation for Convex Clustering via Algorithmic Regularization},
  url            = {https://arxiv.org/abs/1901.01477},
  urldate        = {2019-05-04},
  abstract       = {Convex clustering is a promising new approach to the classical problem of clustering, combining strong performance in empirical studies with rigorous theoretical foundations. Despite these advantages, convex clustering has not been widely adopted, due to its computationally intensive nature and its lack of compelling visualizations. To address these impediments, we introduce Algorithmic Regularization, an innovative technique for obtaining high-quality estimates of regularization paths using an iterative one-step approximation scheme. We justify our approach with a novel theoretical result, guaranteeing global convergence of the approximate path to the exact solution under easily-checked non-data-dependent assumptions. The application of algorithmic regularization to convex clustering yields the Convex Clustering via Algorithmic Regularization Paths (CARP) algorithm for computing the clustering solution path. On example data sets from genomics and text analysis, CARP delivers over a 100-fold speed-up over existing methods, while attaining a finer approximation grid than standard methods. Furthermore, CARP enables improved visualization of clustering solutions: the fine solution grid returned by CARP can be used to construct a convex clustering-based dendrogram, as well as forming the basis of a dynamic path-wise visualization based on modern web technologies. Our methods are implemented in the open-source R package clustRviz, available at this https URL.},
  day            = {6},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Whiteley-2019,
  author         = {Whiteley, Nick},
  date           = {2019-06-25},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Dynamic time series clustering via volatility change-points},
  url            = {https://arxiv.org/abs/1906.10372},
  urldate        = {2019-08-20},
  abstract       = {This note outlines a method for clustering time series based on a statistical model in which volatility shifts at unobserved change-points. The model accommodates some classical stylized features of returns and its relation to GARCH is discussed. Clustering is performed using a probability metric evaluated between posterior distributions of the most recent change-point associated with each series. This implies series are grouped together at a given time if there is evidence the most recent shifts in their respective volatilities were coincident or closely timed. The clustering method is dynamic, in that groupings may be updated in an online manner as data arrive. Numerical results are given analyzing daily returns of constituents of the S\&P 500.},
  day            = {25},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Wong-2013,
  author               = {Wong, Man H.},
  date                 = {2013-06},
  journaltitle         = {European Journal of Operational Research},
  title                = {Investment models based on clustered scenario trees},
  doi                  = {10.1016/j.ejor.2012.11.051},
  issn                 = {0377-2217},
  number               = {2},
  pages                = {314--324},
  volume               = {227},
  abstract             = {Stochastic programming is widely applied in financial decision problems. In particular, when we need to carry out the actual calculations for portfolio selection problems, we have to assign a value for each expected return and the associated conditional probability in advance. These estimated random parameters often rely on a scenario tree representing the distribution of the underlying asset returns. One of the drawbacks is that the estimated parameters may be deviated from the actual ones. Therefore, robustness is considered so as to cope with the issue of parameter inaccuracy. In view of this, we propose a clustered scenario-tree approach, which accommodates the parameter inaccuracy problem in the context of a scenario tree. Proposed a new kind of scenario tree, called ? cluster tree ?. It accommodates the parameter inaccuracy in the context of a scenario tree. The idea is illustrated with portfolio selection problems. Three risk measures are considered: probability, downside risk and CVaR. OR techniques include fractional programming, interior point methods and SOCP.},
  citeulike-article-id = {13989083},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2012.11.051},
  groups               = {Network_Invest, Scenario_Portfolio},
  owner                = {cristi},
  posted-at            = {2016-03-27 18:42:57},
  timestamp            = {2020-02-27 04:04},
}

@Article{Xia-et-al-2018,
  author         = {Xia, Jiazhi and Gao, Le and Kong, Kezhi and Zhao, Ying and Chen, Yi and Kui, Xiaoyan and Liang, Yixiong},
  date           = {2018-10},
  journaltitle   = {Journal of Visual Languages \& Computing},
  title          = {Exploring linear projections for revealing clusters, outliers, and trends in subsets of multi-dimensional datasets},
  doi            = {10.1016/j.jvlc.2018.08.003},
  issn           = {1045-926X},
  pages          = {52--60},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/{S1045926X18301289}},
  volume         = {48},
  abstract       = {Identifying patterns in 2D linear projections is important in understanding multi-dimensional datasets. However, local patterns, which are composed of partial data points, are usually obscured by noises and missed in traditional quality measure approaches that measure the whole dataset. In this paper, we propose an interactive interface to explore 2D linear projections with visual patterns on subsets. First, we propose a voting-based algorithm to recommend optimal projection, in which the identified pattern looks the most salient. Specifically, we propose three kinds of point-wise quality metrics of 2D linear projections for outliers, clusterings, and trends, respectively. For each sampled projection, we measure its importance by accumulating the metrics of selected points. The projection with the highest importance is recommended. Second, we design an exploring interface with a scatterplot, a projection trail map, and a control panel. Our interface allows users to explore projections by specifying interested data subsets. At last, we employ three datasets and demonstrate the effectiveness of our approach through three case studies of exploring clusters, outliers, and trends.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Xiong-et-al-2017,
  author               = {Xiong, Caiming and Johnson, David M. and Corso, Jason J.},
  date                 = {2017-01},
  journaltitle         = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title                = {Active Clustering with Model-Based Uncertainty Reduction},
  doi                  = {10.1109/tpami.2016.2539965},
  issn                 = {0162-8828},
  number               = {1},
  pages                = {5--17},
  volume               = {39},
  abstract             = {Semi-supervised clustering seeks to augment traditional clustering methods by incorporating side information provided via human expertise in order to increase the semantic meaningfulness of the resulting clusters. However, most current methods are passive in the sense that the side information is provided beforehand and selected randomly. This may require a large number of constraints, some of which could be redundant, unnecessary, or even detrimental to the clustering results. Thus in order to scale such semi-supervised algorithms to larger problems it is desirable to pursue an active clustering method, i.e., an algorithm that maximizes the effectiveness of the available human labor by only requesting human input where it will have the greatest impact. Here, we propose a novel online framework for active semi-supervised spectral clustering that selects pairwise constraints as clustering proceeds, based on the principle of uncertainty reduction. Using a first-order Taylor expansion, we decompose the expected uncertainty reduction problem into a gradient and a step-scale, computed via an application of matrix perturbation theory and cluster-assignment entropy, respectively. The resulting model is used to estimate the uncertainty reduction potential of each sample in the dataset. We then present the human user with pairwise queries with respect to only the best candidate sample. We evaluate our method using three different image datasets (faces, leaves and dogs), a set of common UCI machine learning datasets and a gene dataset. The results validate our decomposition formulation and show that our method is consistently superior to existing state-of-the-art techniques, as well as being robust to noise and to unknown numbers of clusters.},
  citeulike-article-id = {14334833},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/tpami.2016.2539965},
  day                  = {1},
  posted-at            = {2017-04-10 01:56:02},
  timestamp            = {2020-02-27 04:04},
}

@InCollection{Xue-et-al-2018,
  author         = {Xue, Jingming and Zhu, En and Liu, Qiang and Wang, Chuanli and Yin, Jianping},
  booktitle      = {Cloud Computing and Security: 4th International Conference, ICCCS 2018},
  date           = {2018},
  title          = {A Joint Approach to Data Clustering and Robo-Advisor},
  doi            = {10.1007/978-3-030-00006-6\_9},
  editor         = {Sun, Xingming and Pan, Zhaoqing and Bertino, Elisa},
  isbn           = {978-3-030-00005-9},
  pages          = {97--109},
  publisher      = {Springer International Publishing},
  series         = {Lecture notes in computer science},
  url            = {http://link.springer.com/10.1007/978-3-030-00006-6\_9},
  urldate        = {2019-10-12},
  volume         = {11063},
  abstract       = {Robo-advisor is a type of financial recommendation that can provide investors with financial advice or investment management online. Data clustering and item recommendation are both important and challenging in Robo-advisor. These two tasks are often considered independently and most efforts have been made to tackle them separately. However, users in data clustering and group relationship in item recommendation are inherently related. For example, a large number of financial transactions include not only the user asset information, but also the user social information. The existence of relations between users and groups motivates us to jointly perform clustering and item recommendation for Robo-advisor in this paper. In particular, we provide a principle way to capture the relations between users and groups, and propose a novel framework CLURE, which fuses data CLUstering and item REcommendation into a coherent model. With experiments on benchmark and real-world datasets, we demonstrate that the proposed framework CLURE achieves superior performance on both tasks compared to the state-of-the-art methods.},
  f1000-projects = {QuantInvest},
  issn           = {0302-9743},
  timestamp      = {2020-02-27 04:04},
}

@Article{Xu-Lange-2019,
  author         = {Xu, Jason and Lange, Kenneth},
  date           = {2019-05-24},
  journaltitle   = {Proceedings of Machine Learning Research},
  title          = {Power k-Means Clustering},
  url            = {http://proceedings.mlr.press/v97/xu19a.html},
  urldate        = {2019-09-15},
  abstract       = {Clustering is a fundamental task in unsupervised machine learning. Lloyd 1957 algorithm for k-means clustering remains one of the most widely used due to its speed and simplicity, but the greedy approach is sensitive to initialization and often falls short at a poor solution. This paper explores an alternative to Lloyd algorithm that retains its simplicity and mitigates its tendency to get trapped by local minima. Called power k-means, our method embeds the k-means problem in a continuous class of similar, better behaved problems with fewer local minima. Power k-means anneals its way toward the solution of ordinary k-means by way of majorization-minimization (MM), sharing the appealing descent property and low complexity of Lloyd algorithm. Further, our method complements widely used seeding strategies, reaping marked improvements when used together as demonstrated on a suite of simulated and real data examples.},
  day            = {24},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@InCollection{Yang-et-al-2016a,
  author    = {Hoseong Yang and Hye Jin Lee and Eugene Cho and Sungzoon Cho},
  booktitle = {IEEE International Conference on Big Data},
  date      = {2016},
  title     = {Automatic classification of securities using hierarchical clustering of the 10-Ks},
  url       = {https://ieeexplore.ieee.org/abstract/document/7841069},
  abstract  = {Industry classification has been rigorously utilized in academic research and business analytics. The existing classification schemes, however, have been constructed and maintained manually by domain experts, which require exhaustive time and human effort while vulnerable to subjectivity. Hence, the existing classification systems do not properly reflect the fast-changing trends of the firms and the capital market. As a remedy to such shortcomings, this paper proposes a new classification scheme, Business Text Industry Classification (BTIC), namely, that automatically clusters securities based on the textual information from the corporate disclosures. BTIC exploits the business section of the Form 10-Ks, in which firms provide their self-identities in a rich context. We employ doc2vec for document embedding and apply Ward's hierarchical clustering method to categorize securities into BTIC groups. Evaluation results using 12 financial ratios commonly found in financial research show that BTIC performs just as good as SIC and GICS in terms of inter- and intra-industry homogeneity, especially for the higher level of clustering. Given that, we claim that BTIC outperforms SIC and GICS in four aspects: process automation, objectivity, clustering flexibility, and result interpretability.},
  timestamp = {2020-02-27 04:04},
}

@InCollection{You-et-al-2016a,
  author               = {You, Shi Y. and Dan Wang, Yu and Luo, Lin K. and Peng, Hong},
  booktitle            = {11th International Conference on Computer Science and Education (ICCSE)},
  date                 = {2016-08},
  title                = {Finding the clusters with potential value in financial time series based on agglomerative hierarchical clustering},
  doi                  = {10.1109/iccse.2016.7581558},
  isbn                 = {978-1-5090-2218-2},
  location             = {Nagoya, Japan},
  pages                = {77--81},
  publisher            = {IEEE},
  abstract             = {It is interesting to find the clustering with potential value in financial time series. In this paper, we focus on this topic. The owned features of the clusters with potential value are provided firstly. Then, the agglomerative hierarchical clustering (AHC) is used to find those clusters automatically. There are two innovations in this paper. The first one is that the features of the clusters with potential value are embedded into the process of AHC, which reduces the time cost of clustering process. The second one is that we propose two indicators, whole similarity and trend similarity, to measure the persistence of the cluster. The experiment on ten time segments shows the obtained clusters is effective, in which both the whole similarity and the trend similarity on training data are markedly higher than that of randomized clustering. In addition, the persistence of these clusters on test data is also better that the result of randomized guess. We think that the strategy provided in this paper is helpful to find for the clustering with potential value.},
  citeulike-article-id = {14320267},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/iccse.2016.7581558},
  posted-at            = {2017-03-26 18:26:06},
  timestamp            = {2020-02-27 04:04},
}

@Article{Yuan-2005,
  author               = {Yuan, Baosheng},
  date                 = {2006-12},
  journaltitle         = {SSRN Electronic Journal},
  title                = {Scaling, Clustering and Dynamics of Volatility in Financial Time Series},
  url                  = {https://ssrn.com/abstract=950960},
  abstract             = {This thesis investigates volatility clustering, scaling and dynamics in financial series of asset returns and studies the underlying mechanism. We propose a direct measure of volatility clustering based on the conditional probability distribution (CPD) of the returns given the return in the previous time interval. We found that the CPDs of returns in real financial time series exhibits universal scaling, characterized by a collapse of the CPDs (of different time lags and of different returns in the previous interval) into to a universal curve exhibiting a power-law tail with an exponent of 4. We construct a simple phenomenological model to explain the emergence of VC and the associated volatility scaling. We also study agent-based models of financial markets, and explore the impact of dynamical risk aversion (DRA) of heterogeneous agents on the price fluctuations. We found that the DRA is the primary driving force responsible for excess price fluctuations and the associated volatility clustering. Both our models (phenomenological model and agent-based model) are able to generate time series that reproduces stylized facts of the market data on different time scales. We have also studied general herding behavior often exhibited in financial markets in the context of an evolutionary Minority Game. We discovered a general mechanism for the transition from segregation into opposing groups to clustering towards cautious behavior.},
  citeulike-article-id = {13988076},
  citeulike-linkout-0  = {http://ssrn.com/abstract=950960},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID950960code539232.pdf?abstractid=950960 and mirid=1},
  day                  = {22},
  groups               = {Vol_Cluster},
  owner                = {cristi},
  posted-at            = {2016-03-25 16:56:07},
  timestamp            = {2020-02-27 04:04},
}

@Article{Yu-et-al-2018a,
  author         = {Yu, Han and Chapman, Brian and Di Florio, Arianna and Eischen, Ellen and Gotz, David and Jacob, Mathews and Blair, Rachael Hageman},
  date           = {2018-08-28},
  journaltitle   = {Computational statistics},
  title          = {Bootstrapping estimates of stability for clusters, observations and model selection},
  doi            = {10.1007/s00180-018-0830-y},
  issn           = {0943-4062},
  number         = {1},
  pages          = {349--372},
  urldate        = {2019-04-27},
  volume         = {34},
  abstract       = {Clustering is a challenging problem in unsupervised learning. In lieu of a gold standard, stability has become a valuable surrogate to performance and robustness. In this work, we propose a non-parametric bootstrapping approach to estimating the stability of a clustering method, which also captures stability of the individual clusters and observations. This flexible framework enables different types of comparisons between clusterings and can be used in connection with two possible bootstrap approaches for stability. The first approach, scheme 1, can be used to assess confidence (stability) around clustering from the original dataset based on bootstrap replications. A second approach, scheme 2, searches over the bootstrap clusterings for an optimally stable partitioning of the data. The two schemes accommodate different model assumptions that can be motivated by an investigator trust (or lack thereof) in the original data and additional computational considerations. We propose a hierarchical visualization extrapolated from the stability profiles that give insights into the separation of groups, and projected visualizations for the inspection of the stability of individual operations. Our approaches show good performance in simulation and on real data. These approaches can be implemented using the R package bootcluster that is available on the Comprehensive R Archive Network (CRAN).},
  day            = {28},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Yu-et-al-2105b,
  author               = {Yu, Meichen and Hillebrand, Arjan and Tewarie, Prejaas and Meier, Jil and van Dijk, Bob and Van Mieghem, Piet and Stam, Cornelis Jan J.},
  date                 = {2015-02},
  journaltitle         = {Chaos},
  title                = {Hierarchical clustering in minimum spanning trees.},
  issn                 = {1089-7682},
  number               = {2},
  url                  = {http://view.ncbi.nlm.nih.gov/pubmed/25725643},
  volume               = {25},
  abstract             = {The identification of clusters or communities in complex networks is a reappearing problem. The minimum spanning tree (MST), the tree connecting all nodes with minimum total weight, is regarded as an important transport backbone of the original weighted graph. We hypothesize that the clustering of the MST reveals insight in the hierarchical structure of weighted graphs. However, existing theories and algorithms have difficulties to define and identify clusters in trees. Here, we first define clustering in trees and then propose a tree agglomerative hierarchical clustering (TAHC) method for the detection of clusters in MSTs. We then demonstrate that the TAHC method can detect clusters in artificial trees, and also in MSTs of weighted social networks, for which the clusters are in agreement with the previously reported clusters of the original weighted networks. Our results therefore not only indicate that clusters can be found in MSTs, but also that the MSTs contain information about the underlying clusters of the original weighted network.},
  citeulike-article-id = {14150073},
  citeulike-linkout-0  = {http://view.ncbi.nlm.nih.gov/pubmed/25725643},
  citeulike-linkout-1  = {http://www.hubmed.org/display.cgi?uids=25725643},
  groups               = {Networks and investment management, Clustering and network analysis},
  owner                = {cristi},
  pmid                 = {25725643},
  posted-at            = {2016-10-01 20:43:49},
  timestamp            = {2020-02-27 04:04},
}

@Article{Zambom-et-al-2019,
  author         = {Zambom, Adriano Zanin and Collazos, Julian A. and Dias, Ronaldo},
  date           = {2019-05-02},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Selection of the Number of Clusters in Functional Data Analysis},
  url            = {https://arxiv.org/abs/1905.00977},
  urldate        = {2019-08-10},
  abstract       = {Identifying the number KK of clusters in a dataset is one of the most difficult problems in clustering analysis. A choice of KK that correctly characterizes the features of the data is essential for building meaningful clusters. In this paper we tackle the problem of estimating the number of clusters in functional data analysis by introducing a new measure that can be used with different procedures in selecting the optimal KK. The main idea is to use a combination of two test statistics, which measure the lack of parallelism and the mean distance between curves, to compute criteria such as the within and between cluster sum of squares. Simulations in challenging scenarios suggest that procedures using this measure can detect the correct number of clusters more frequently than existing methods in the literature. The application of the proposed method is illustrated on several real datasets.},
  day            = {2},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@TechReport{Zhang-Maringer-2010,
  author      = {Jin Zhang and Dietmar Maringer},
  date        = {2010},
  institution = {COMISEF Computational Optimization Methods in Statistics, Econometrics and Finance},
  title       = {Asset Allocation under Hierarchical Clustering},
  url         = {https://ideas.repec.org/p/com/wpaper/036.html},
  abstract    = {This paper proposes a clustering asset allocation scheme which provides better risk-adjusted portfolio performance than those obtained from tradi- tional asset allocation approaches such as the equal weight strategy and the Markowitz minimum variance allocation. The clustering criterion used, which involves maximization of the in-sample Sharpe ratio (SR), is different from traditional clustering criteria reported in the literature. Two evolu- tionary methods, namely Differential Evolution and Genetic Algorithm, are employed to search for such an optimal clustering structure given a clus- ter number. To explore the clustering impact on the SR, the in-sample and the out-of-sample SR distributions of the portfolios are studied using bootstrapped data as well as simulated paths from the single index market model. It was found that the SR distributions of the portfolios under the clustering asset allocation structure have higher mean values and skewness but approximately the same standard deviation and kurtosis than those in the non-clustered case. Genetic Algorithm is suggested as a more efficient approach than Differential Evolution for the purpose of solving the cluster-ing problem.},
  groups      = {Networks and investment management, Clustering and network analysis, Network_Invest, PortfOptim_Network},
  timestamp   = {2020-02-27 04:04},
}

@Article{Zhang-Maringer-2011,
  author               = {Zhang, Jin and Maringer, Dietmar},
  date                 = {2011-11},
  journaltitle         = {Expert Systems with Applications},
  title                = {Distributing weights under hierarchical clustering: A way in reducing performance breakdown},
  doi                  = {10.1016/j.eswa.2011.05.052},
  issn                 = {0957-4174},
  number               = {12},
  pages                = {14952--14959},
  volume               = {38},
  abstract             = {This paper proposes a clustering asset allocation scheme which provides better risk-adjusted portfolio performance than those obtained from traditional asset allocation approaches such as the equal weight strategy and the Markowitz minimum variance allocation. The clustering criterion used, which involves maximization of the in-sample Sharpe ratio (SR), is different from traditional clustering criteria reported in the literature. Two evolutionary methods, namely Differential Evolution and Genetic Algorithm, are employed to search for such an optimal clustering structure given a cluster number. To explore the clustering impact on the SR, the in-sample and the out-of-sample SR distributions of the portfolios are studied using bootstrapped data as well as simulated paths from the single index market model. It was found that the SR distributions of the portfolios under the clustering asset allocation structure have higher mean values and skewness but approximately the same standard deviation and kurtosis than those in the non-clustered case. Genetic Algorithm is suggested as a more efficient approach than Differential Evolution for the purpose of solving the clustering problem. We introduce a clustering scheme to improve portfolio Sharpe ratio. Mean and Skewness of Sharpe ratio can be improved by using the clustering scheme. Genetic Algorithm is apt at finding an optimal clustering structure. Clustering asset helps to improve portfolio risk-adjusted performance. Sharpe ratio maximization can be considered as a suitable clustering criterion.},
  citeulike-article-id = {9504815},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.eswa.2011.05.052},
  owner                = {cristi},
  posted-at            = {2016-10-01 16:47:37},
  timestamp            = {2020-02-27 04:04},
}

@InProceedings{Zhang-Zhu-2011,
  author         = {Zhang, Dabin and Zhu, Hou},
  booktitle      = {Fourth International Joint Conference on Computational Sciences and Optimization},
  date           = {2011-04-15},
  title          = {A clustering methodology for industry categorization using business cycle},
  doi            = {10.1109/{CSO}.2011.22},
  isbn           = {978-1-4244-9712-6},
  pages          = {318--321},
  publisher      = {IEEE},
  url            = {http://ieeexplore.ieee.org/document/5957670/},
  urldate        = {2019-12-04},
  day            = {15},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Zhong-Enke-2017a,
  author               = {Zhong, Xiao and Enke, David},
  date                 = {2017-12},
  journaltitle         = {Neurocomputing},
  title                = {A comprehensive cluster and classification mining procedure for daily stock market return forecasting},
  doi                  = {10.1016/j.neucom.2017.06.010},
  issn                 = {0925-2312},
  pages                = {152--168},
  volume               = {267},
  abstract             = {Data mining and big data analytic techniques are playing an important role in many application fields, including the financial markets. However, only few studies have focused on predicting daily stock market returns, and among these studies, the data mining procedures utilized are either incomplete or inefficient. This paper presents a comprehensive data mining process to forecast the daily direction of the S\&P 500 Index ETF (SPY) return based on 60 financial and economical features. The fuzzy c-means method (FCM) is initially used to cluster the preprocessed data. A principal component analysis (PCA) is applied next to the entire data set and each of seven clusters. The dimension of the entire cleaned data set is then reduced according to the combining results from the entire data set and each cluster. Corresponding to different levels of the dimensionality reduction, twelve new data sets are generated from the entire cleaned data. Artificial neural networks (ANNs) and logistic regression models are then used with the twelve transformed data sets for classification in order to forecast the daily direction of future market returns and indicate the efficiency of dimensionality reduction with PCA. A group of hypothesis tests are performed over the classification and simulation results to show that the ANNs give significantly higher classification accuracy than logistic regression, and that the trading strategies guided by the comprehensive cluster and classification mining procedure based on PCA and ANNs gain higher risk-adjusted profits than the comparison benchmarks, as well as those strategies guided by the forecasts based on PCA and logistic regression models.},
  citeulike-article-id = {14500341},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.neucom.2017.06.010},
  groups               = {Data_Cleaning},
  posted-at            = {2017-12-11 05:03:50},
  timestamp            = {2020-02-27 04:04},
}

@Article{Zhou-et-al-2016,
  author               = {Zhou, Shibing and Xu, Zhenyuan and Liu, Fei},
  date                 = {2016},
  journaltitle         = {IEEE Transactions on Neural Networks and Learning Systems},
  title                = {Method for Determining the Optimal Number of Clusters Based on Agglomerative Hierarchical Clustering},
  doi                  = {10.1109/tnnls.2016.2608001},
  issn                 = {2162-237X},
  pages                = {1--11},
  abstract             = {It is crucial to determine the optimal number of clusters for the clustering quality in cluster analysis. From the standpoint of sample geometry, two concepts, i.e., the sample clustering dispersion degree and the sample clustering synthesis degree, are defined, and a new clustering validity index is designed. Moreover, a method for determining the optimal number of clusters based on an agglomerative hierarchical clustering (AHC) algorithm is proposed. The new index and the method can evaluate the clustering results produced by the AHC and determine the optimal number of clusters for multiple types of datasets, such as linear, manifold, annular, and convex structures. Theoretical research and experimental results indicate the validity and good performance of the proposed index and the method.},
  citeulike-article-id = {14435140},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/tnnls.2016.2608001},
  posted-at            = {2017-09-21 00:36:08},
  timestamp            = {2020-02-27 04:04},
}

@Article{Zhu-et-al-2019c,
  author         = {Zhu, Dandan and Han, Tian and Zhou, Linqi and Yang, Xiaokang and Wu, Ying Nian},
  date           = {2019-11-19},
  journaltitle   = {arXiv Electronic Journal},
  title          = {Deep Unsupervised Clustering with Clustered Generator Model},
  url            = {https://arxiv.org/abs/1911.08459v1},
  urldate        = {2019-12-15},
  abstract       = {This paper addresses the problem of unsupervised clustering which remains one of the most fundamental challenges in machine learning and artificial intelligence. We propose the clustered generator model for clustering which contains both continuous and discrete latent variables. Discrete latent variables model the cluster label while the continuous ones model variations within each cluster. The learning of the model proceeds in a unified probabilistic framework and incorporates the unsupervised clustering as an inner step without the need for an extra inference model as in existing variational-based models. The latent variables learned serve as both observed data embedding or latent representation for data distribution. Our experiments show that the proposed model can achieve competitive unsupervised clustering accuracy and can learn disentangled latent representations to generate realistic samples. In addition, the model can be naturally extended to per-pixel unsupervised clustering which remains largely unexplored.},
  day            = {19},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-27 04:04},
}

@Article{Ackerman-et-al-2021,
  author           = {Margareta Ackerman and Shai Ben-David and Simina Br{\^{a}}nzei and David Loker},
  date             = {2021-12},
  journaltitle     = {Pattern Recognition},
  title            = {Weighted clustering: Towards solving the user's dilemma},
  doi              = {10.1016/j.patcog.2021.108152},
  pages            = {108152},
  volume           = {120},
  abstract         = {This paper makes a major step towards addressing a long-standing challenge in cluster analysis, known as the user's dilemma, which is the problem of selecting an appropriate clustering algorithm for a specific task. A formal approach for addressing this challenge relies on the identification of succinct, user-friendly properties that capture formal differences amongst clustering techniques. While helpful for gaining insight into the nature of clustering paradigms, there is a theory-practice gap that has so far limited the utility of this approach: Formal properties typically highlight advantages of classical linkage-based algorithms, while practical experience shows that center-based methods are preferable for many applications. We present simple new properties that delineate core differences between common clustering paradigms and overcome this theory-practice gap. The properties we present give a formal understanding of the advantages of center-based approaches for some applications and insight into when different clustering paradigms should be used. These properties address how sensitive algorithms are to changes in element frequencies, which we capture in a generalized setting where every element is associated with a real-valued weight. To complement extensive formal analysis, we discuss how these properties can be applied in practice.},
  creationdate     = {2023-06-24T18:51:00},
  modificationdate = {2023-06-24T18:51:00},
  publisher        = {Elsevier {BV}},
}

@Article{Alfarra-et-al-2021,
  author           = {Motasem Alfarra and Juan C. Perez and Adel Bibi and Ali Thabet and Pablo Arbelaez and Bernard Ghanem},
  date             = {2021},
  journaltitle     = {arXiv e-Print},
  title            = {Rethinking Clustering for Robustness},
  eprint           = {2006.07682},
  eprintclass      = {cs.LG},
  eprinttype       = {arXiv},
  abstract         = {This paper studies how encouraging semantically-aligned features during deep neural network training can increase network robustness. Recent works observed that Adversarial Training leads to robust models, whose learnt features appear to correlate with human perception. Inspired by this connection from robustness to semantics, we study the complementary connection: from semantics to robustness. To do so, we provide a robustness certificate for distance-based classification models (clustering-based classifiers). Moreover, we show that this certificate is tight, and we leverage it to propose ClusTR (Clustering Training for Robustness), a clustering-based and adversary-free training framework to learn robust models. Interestingly, \textit{ClusTR} outperforms adversarially-trained networks by up to $4\%$ under strong PGD attacks.},
  creationdate     = {2023-06-24T18:54:01},
  file             = {:http\://arxiv.org/pdf/2006.07682v3:PDF},
  keywords         = {cs.LG, stat.ML},
  modificationdate = {2023-06-24T18:54:01},
}

@Article{Akansu-et-al-2021,
  author           = {Ali Akansu and Marco Avellaneda and Anqi Xiong},
  date             = {2021},
  journaltitle     = {The Journal of Investment Strategies},
  title            = {Quant investing in cluster portfolios},
  doi              = {10.21314/jois.2021.006},
  number           = {4},
  pages            = {61-78},
  volume           = {9},
  abstract         = {This paper discusses portfolio construction for investing in N given assets, eg, constituents of the Dow Jones Industrial Average (DJIA) or large cap stocks, based on partitioning the investment universe into clusters. The clusters are determined from the trailing correlation matrix via an information theoretic algorithm that uses thresholding of high-correlation pairs. We calculate the principal eigenvector of each cluster from its correlation matrix and the corresponding eigenportfolio. The cluster portfolios are combined into a single N-asset portfolio based on a weighting scheme for the clusters. Various tests conducted on components of the DJIA and a 30-stock basket of large cap stocks indicate that the new portfolios are superior to the DJIA and other mean-variance portfolios in terms of their risk-adjusted returns from 2009 to 2019. We also tested the cluster portfolios for a larger basket of 373 Standard \& Poor's 500 components from 2001 to 2019. The test results provide convincing evidence that a cluster-based portfolio can outperform passive investing.},
  creationdate     = {2023-06-24T18:56:51},
  modificationdate = {2023-06-24T18:56:51},
  publisher        = {Infopro Digital Services Limited},
}

@Article{Avellaneda-Serur-2020,
  author           = {Marco Avellaneda and Juan Andres Serur},
  date             = {2020-10-08},
  journaltitle     = {arXiv e-Print},
  title            = {Hierarchical PCA and Modeling Asset Correlations},
  eprinttype       = {arXiv},
  url              = {https://arxiv.org/abs/2010.04140},
  abstract         = {Modeling cross-sectional correlations between thousands of stocks, across countries and industries, can be challenging. In this paper, we demonstrate the advantages of using Hierarchical Principal Component Analysis (HPCA) over the classic PCA. We also introduce a statistical clustering algorithm for identifying of homogeneous clusters of stocks, or "synthetic sectors". We apply these methods to study cross-sectional correlations in the US, Europe, China, and Emerging Markets.},
  creationdate     = {2023-06-24T18:59:26},
  keywords         = {q-fin.MF},
  modificationdate = {2023-06-24T18:59:26},
  timestamp        = {2020-10-13 18:57},
}

@Article{Baitinger-et-al-2022,
  author           = {Eduard Baitinger and Marcus Rumler and Anastasia Topalova},
  date             = {2022},
  journaltitle     = {SSRN e-Print},
  title            = {Forecasting Adverse Financial Markets Regimes with Graph Theory},
  doi              = {10.2139/ssrn.4289721},
  abstract         = {Turbulent financial market periods are characterized by low mean returns, high volatility and often by the disappearance of the diversification effect. Therefore, accurate predictions of financial turbulence can significantly improve the risk-reward-ratio of investment strategies and is therefore a lively field of academic and professional research. This paper augments the work of Baitinger and Flegel (2021a) that focused on persistent homology for the detection of turbulent financial market regimes. Precisely, we introduce an alternative turbulence indication that is based on graph theory applied to financial networks, i.e. network-based analysis. With the help of out-of-sample studies, we compare the performance of the alternative turbulence indicator with "classical" regime indicators and the persistent homology-based indicator. The results indicate that turbulence indication relying on network-based analysis is superior to existing approaches. Furthermore, the performance analysis reveals that the novel indicator successfully navigates the Covid-19 selloff and the stock market turbulence of 2022.},
  creationdate     = {2023-06-24T19:00:07},
  modificationdate = {2023-06-24T19:00:07},
  publisher        = {Elsevier {BV}},
}

@Article{Baitinger-Flegel-2021,
  author           = {Eduard Baitinger and Samuel Flegel},
  date             = {2021-02},
  journaltitle     = {Financial Markets and Portfolio Management},
  title            = {The better turbulence index? Forecasting adverse financial markets regimes with persistent homology},
  doi              = {10.1007/s11408-020-00377-x},
  abstract         = {Persistent homology is the workhorse of modern topological data analysis, which in recent years becomes increasingly powerful due to methodological and computing power advances. In this paper, after equipping the reader with the relevant background on persistent homology, we show how this tool can be harnessed for investment purposes. Specifically, we propose a persistent homology-based turbulence index for the detection of adverse market regimes. With the help of an out-of-sample study, we demonstrate that investment strategies relying on a persistent homology-based turbulence detection outperform investment strategies based on other popular turbulence indices. Additionally, we conduct a stability analysis of our findings. This analysis confirms the results from the previous out-of-sample study, as the outperformance prevails for most configurations of the respective investment strategy and thereby mitigating possible data mining concerns.},
  creationdate     = {2023-06-24T19:00:07},
  modificationdate = {2023-06-24T19:00:07},
  publisher        = {Springer Science and Business Media {LLC}},
  timestamp        = {2021-03-08 16:36},
}

@Article{Baitinger-Flegel-2021a,
  author           = {Eduard Baitinger and Samuel Flegel},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {New Concepts in Financial Forecasting: Network-Based Information, Topological Data Analysis and their Combination},
  doi              = {10.2139/ssrn.3962148},
  abstract         = {This paper introduces novel financial predictors that are derived from the interaction profile of financial markets. These predictors utilize network-based and topological information. Since these predictors are derived from the inner dynamics (microstructure) of financial markets, they can be best described as microstructural predictors. After equipping the reader with the methodological background of the novel predictors, we perform an extensive in-sample and out-of-sample performance analyses. The in-sample studies demonstrate that microstructural predictors and their combinations are informative with regard to future asset returns. In the out-of-sample studies, we combine microstructual predictors with state of the art machine learning and statistical factor extraction methods. The resulting active forecasting models dominate the benchmark mean model in terms of profitability, but not it terms of statistical precision. Since an investor is usually more concerned with profitability of active investment strategies, the out-of-sample results confirm the value-added of the novel predictors.},
  creationdate     = {2023-06-24T19:00:07},
  modificationdate = {2023-06-24T19:00:07},
  publisher        = {Elsevier {BV}},
}

@Article{Baitinger-2021,
  author           = {Eduard Baitinger},
  date             = {2021-03},
  journaltitle     = {Journal of Forecasting},
  title            = {Forecasting asset returns with network-based metrics: A statistical and economic analysis},
  doi              = {10.1002/for.2772},
  abstract         = {One of the main challenges facing researchers and industry professionals for decades is the successful prediction of asset returns. This paper enriches this endeavor by applying topological metrics of correlation networks to the challenge of financial forecasting. These network-based metrics are retrieved with the help of graph theory and quantify the interconnectedness of financial assets. In this paper, we show that this network-based information statistically significantly predicts future asset returns. Because industry professionals are more interested in the economic value-added of competing forecasting approaches, we also devote our attention to an economic analysis. Considering economic performance metrics, network-based predictors generate a clear value-added, which also applies to the multi-asset allocation case.},
  creationdate     = {2023-06-24T19:00:07},
  modificationdate = {2023-06-24T19:00:07},
  publisher        = {Wiley},
}

@Article{Bnouachir-Mkhadri-2021,
  author           = {Bnouachir, Najla and Mkhadri, Abdallah},
  date             = {2021},
  journaltitle     = {Communications in Statistics - Simulation and Computation},
  title            = {Efficient cluster-based portfolio optimization},
  doi              = {10.1080/03610918.2019.1621341},
  issn             = {0361-0918},
  pages            = {3241-3255},
  urldate          = {2020-01-13},
  volume           = {50},
  abstract         = {The sample mean and covariance matrix of historical data provide a disappointing out-of-sample performance in mean-variance portfolio rules. This poor performance is certainly due to the high estimation error incurred in the optimization model. Our purpose in this article is to find a method that enhances the out-of-sample performance of the portfolio weights. Using hierarchical clustering, we propose an alternative cluster-based portfolio to obtain a sequence of cluster assets. On the basis of Gram-Schmidt orthogonalization, the estimation risk of the data set becomes the sum of the estimations of the clusters in the sequence. The performance of our method and its competitors is compared empirically and via some simulations in high dimension.},
  creationdate     = {2023-06-24T19:02:49},
  day              = {3},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T19:02:49},
  timestamp        = {2020-01-24 08:15},
}

@Article{Clemente-et-al-2022,
  author           = {Gian Paolo Clemente and Rosanna Grassi and Asmerilda Hitaj},
  date             = {2022-04},
  journaltitle     = {Annals of Operations Research},
  title            = {Smart network based portfolios},
  doi              = {10.1007/s10479-022-04675-7},
  abstract         = {In this article we deal with the problem of portfolio allocation by enhancing network theory tools. We propose the use of the correlation network dependence structure in constructing some well-known risk-based models in which the estimation of the correlation matrix is a building block in the portfolio optimization. We formulate and solve all these portfolio allocation problems using both the standard approach and the network-based approach. Moreover, in constructing the network-based portfolios we propose the use of three different estimators for the covariance matrix: the sample, the shrinkage toward constant correlation and the depth-based estimators . All the strategies under analysis are implemented on three high-dimensional portfolios having different characteristics. We find that the network-based portfolio consistently performs better and has lower risk compared to the corresponding standard portfolio in an out-of-sample perspective.},
  creationdate     = {2023-06-24T19:05:25},
  modificationdate = {2023-06-24T19:05:25},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Article{Clemente-et-al-2021,
  author           = {Clemente, Gian Paolo and Grassi, Rosanna and Hitaj, Asmerilda},
  date             = {2021},
  journaltitle     = {Annals of Operations Research},
  title            = {Asset allocation: new evidence through network approaches},
  doi              = {10.1007/s10479-019-03136-y},
  pages            = {61-80},
  volume           = {299},
  abstract         = {The main contribution of the paper is to unveil the role of the network structure in the financial markets to improve the portfolio selection process, where nodes indicate securities and edges capture the dependence structure of the system. Three different methods are proposed in order to extract the dependence structure between assets in a network context. Starting from this modified structure, we formulate and then we solve the asset allocation problem. We find that the optimal portfolios obtained through a network-based approach are composed mainly of peripheral assets, which are poorly connected with the others. These portfolios, in the majority of cases, are characterized by an higher trade-off between performance and risk with respect to the traditional global minimum variance portfolio. Additionally, this methodology benefits of a graphical visualization of the selected portfolio directly over the graphic layout of the network, which helps in improving our understanding of the optimal strategy.},
  creationdate     = {2023-06-24T19:05:25},
  day              = {20},
  f1000-projects   = {QuantInvest},
  groups           = {Invest_Network},
  modificationdate = {2023-06-24T19:05:25},
  timestamp        = {2019-11-28 13:50},
}

@Article{Clemente-Cornaro-2020,
  author           = {Gian Paolo Clemente and Alessandra Cornaro},
  date             = {2020-11-17},
  journaltitle     = {arXiv e-Print},
  title            = {Assessing Systemic Risk in the Insurance Sector via Network Theory},
  eprinttype       = {arXiv},
  url              = {https://arxiv.org/abs/2011.11394},
  abstract         = {We provide a framework for detecting relevant insurance companies in a systemic risk perspective. Among the alternative methodologies for measuring systemic risk, we propose a complex network approach where insurers are linked to form a global interconnected system. We model the reciprocal influence between insurers calibrating edge weights on the basis of specific risk measures. Therefore, we provide a suitable network indicator, the Weighted Effective Resistance Centrality, able to catch which is the effect of a specific vertex on the network robustness. By means of this indicator, we assess the prominence of a company in spreading and receiving risk from the others.},
  creationdate     = {2023-06-24T19:05:25},
  file             = {:http://arxiv.org/pdf/2011.11394v1},
  keywords         = {q-fin.RM},
  modificationdate = {2023-06-24T19:05:25},
  timestamp        = {2021-01-16 21:03},
}

@Article{Clemente-et-al-2018,
  author           = {Clemente, Gian Paolo and Grassi, Rosanna and Hitaj, Asmerilda},
  date             = {2018-10-20},
  journaltitle     = {arXiv e-Print},
  title            = {Asset allocation: new evidence through network approaches},
  url              = {https://arxiv.org/abs/1810.09825},
  urldate          = {2018-11-11},
  abstract         = {The main contribution of the paper is to employ the financial market network as a useful tool to improve the portfolio selection process, where nodes indicate securities and edges capture the dependence structure of the system. Three different methods are proposed in order to extract the dependence structure between assets in a network context. Starting from this modified structure, we formulate and then we solve the asset allocation problem. We find that the portfolios obtained through a network-based approach are composed mainly of peripheral assets, which are poorly connected with the others. These portfolios, in the majority of cases, are characterized by an higher trade-off between performance and risk with respect to the traditional Global Minimum Variance (GMV) portfolio. Additionally, this methodology benefits of a graphical visualization of the selected portfolio directly over the graphic layout of the network, which helps in improving our understanding of the optimal strategy.},
  creationdate     = {2023-06-24T19:05:29},
  day              = {20},
  f1000-projects   = {QuantInvest},
  groups           = {Invest_Network},
  modificationdate = {2023-06-24T19:05:29},
  timestamp        = {2020-07-23 13:31},
}

@Article{Clemente-et-al-2019,
  author           = {Clemente, Gian Paolo and Grassi, Rosanna and Hitaj, Asmerilda},
  date             = {2019-07-02},
  journaltitle     = {arXiv e-Print},
  title            = {Smart network based portfolios},
  url              = {https://arxiv.org/abs/1907.01274},
  urldate          = {2019-08-20},
  abstract         = {In this article we deal with the problem of portfolio allocation by enhancing network theory tools. We use the dependence structure of the correlations network in constructing some well-known risk-based models in which the estimation of correlation matrix is a building block in the portfolio optimization. We formulate and solve all these portfolio allocation problems using both the standard approach and the network-based approach. Moreover, in constructing the network-based portfolios we propose the use of two different estimators for the covariance matrix: the sample estimator and the shrinkage toward constant correlation one. All the strategies under analysis are implemented on two high-dimensional portfolios having different characteristics, covering the period from January 2001 to December 2017. We find that the network-based portfolio consistently better performs and has lower risk compared to the corresponding standard portfolio in an out-of-sample perspective.},
  creationdate     = {2023-06-24T19:05:30},
  day              = {2},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T19:05:30},
  timestamp        = {2020-07-23 13:31},
}

@Article{Clemente-2020,
  author           = {Fabiana Clemente},
  date             = {2020},
  journaltitle     = {Towards Data Science},
  title            = {Deep Learning-- not only for the big ones},
  url              = {https://towardsdatascience.com/deep-learning-not-only-for-the-big-ones-bd16b019d5e8},
  abstract         = {How you can use Deep Learning even for small datasets},
  creationdate     = {2023-06-24T19:05:30},
  modificationdate = {2023-06-24T19:05:30},
  timestamp        = {2020-09-19 20:45},
}

@Article{Coraggio-Coretto-2021,
  author           = {Luca Coraggio and Pietro Coretto},
  date             = {2021-11-03},
  journaltitle     = {arXiv e-Print},
  title            = {Selecting the number of clusters, clustering models, and algorithms. A unifying approach based on the quadratic discriminant score},
  eprint           = {2111.02302},
  eprintclass      = {stat.ML},
  eprinttype       = {arXiv},
  abstract         = {Cluster analysis requires many decisions: the clustering method and the implied reference model, the number of clusters and, often, several hyper-parameters and algorithms' tunings. In practice, one produces several partitions, and a final one is chosen based on validation or selection criteria. There exist an abundance of validation methods that, implicitly or explicitly, assume a certain clustering notion. Moreover, they are often restricted to operate on partitions obtained from a specific method. In this paper, we focus on groups that can be well separated by quadratic or linear boundaries. The reference cluster concept is defined through the quadratic discriminant score function and parameters describing clusters' size, center and scatter. We develop two cluster-quality criteria called quadratic scores. We show that these criteria are consistent with groups generated from a general class of elliptically-symmetric distributions. The quest for this type of groups is common in applications. The connection with likelihood theory for mixture models and model-based clustering is investigated. Based on bootstrap resampling of the quadratic scores, we propose a selection rule that allows choosing among many clustering solutions. The proposed method has the distinctive advantage that it can compare partitions that cannot be compared with other state-of-the-art methods. Extensive numerical experiments and the analysis of real data show that, even if some competing methods turn out to be superior in some setups, the proposed methodology achieves a better overall performance.},
  creationdate     = {2023-06-24T19:07:30},
  file             = {:http\://arxiv.org/pdf/2111.02302v1:PDF},
  keywords         = {stat.ML, cs.LG},
  modificationdate = {2023-06-24T19:07:30},
}

@Article{Dees-et-al-2020,
  author           = {Dees, Bruno Scalzo and Stankovic, Ljubisa and Constantinides, Anthony G. and Mandic, Danilo P.},
  date             = {2020},
  journaltitle     = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title            = {Portfolio Cuts: A Graph-Theoretic Framework to Diversification},
  doi              = {10.1109/ICASSP40776.2020.9054371},
  abstract         = {Investment returns naturally reside on irregular domains, however, standard multivariate portfolio optimization methods are agnostic to data structure. To this end, we investigate ways for domain knowledge to be conveniently incorporated into the analysis, by means of graphs. Next, to relax the assumption of the completeness of graph topology and to equip the graph model with practically relevant physical intuition, we introduce the portfolio cut paradigm. Such a graph-theoretic portfolio partitioning technique is shown to allow the investor to devise robust and tractable asset allocation schemes, by virtue of a rigorous graph framework for considering smaller, computationally feasible, and economically meaningful clusters of assets, based on graph cuts. In turn, this makes it possible to fully utilize the asset returns covariance matrix for constructing the portfolio, even without the requirement for its inversion. The advantages of the proposed framework over traditional methods are demonstrated through numerical simulations based on real-world price data.},
  creationdate     = {2023-06-24T19:09:12},
  day              = {12},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T19:09:12},
  timestamp        = {2020-07-23 13:32},
}

@InProceedings{deMirandaCardoso-Palomar-2020a,
  author           = {Jose Vinicius {de Miranda Cardoso} and Daniel P. Palomar},
  booktitle        = {54th Asilomar Conference on Signals, Systems, and Computers},
  date             = {2020-11},
  title            = {Learning Undirected Graphs in Financial Markets},
  doi              = {10.1109/ieeeconf51394.2020.9443573},
  publisher        = {{IEEE}},
  abstract         = {We investigate the problem of learning undirected graphical models under Laplacian structural constraints from the point of view of financial market data. We show that Laplacian constraints have meaningful physical interpretations related to the market index factor and to conditional correlations between stocks. Those interpretations lead to a set of guidelines that users should be aware of when estimating graphs in financial markets. In addition, we propose algorithms to learn undirected graphs that account for stylized facts and tasks intrinsic to financial data such as non-stationarity and stock clustering.},
  creationdate     = {2023-06-24T19:10:29},
  modificationdate = {2023-06-24T19:10:29},
}

@Article{deMirandaCardoso-et-al-2020,
  author           = {{de Miranda Cardoso}, Jose Vinicius and Jiaxi Ying and Daniel Perez Palomar},
  date             = {2020-12-31},
  journaltitle     = {arXiv e-Print},
  title            = {Algorithms for Learning Graphs in Financial Markets},
  eprint           = {2012.15410},
  abstract         = {In the past two decades, the field of applied finance has tremendously benefited from graph theory. As a result, novel methods ranging from asset network estimation to hierarchical asset selection and portfolio allocation are now part of practitioners' toolboxes. In this paper, we investigate the fundamental problem of learning undirected graphical models under Laplacian structural constraints from the point of view of financial market times series data. In particular, we present natural justifications, supported by empirical evidence, for the usage of the Laplacian matrix as a model for the precision matrix of financial assets, while also establishing a direct link that reveals how Laplacian constraints are coupled to meaningful physical interpretations related to the market index factor and to conditional correlations between stocks. Those interpretations lead to a set of guidelines that practitioners should be aware of when estimating graphs in financial markets. In addition, we design numerical algorithms based on the alternating direction method of multipliers to learn undirected, weighted graphs that take into account stylized facts that are intrinsic to financial data such as heavy tails and modularity. We illustrate how to leverage the learned graphs into practical scenarios such as stock time series clustering and foreign exchange network estimation. The proposed graph learning algorithms outperform the state-of-the-art methods in an extensive set of practical experiments. Furthermore, we obtain theoretical and empirical convergence results for the proposed algorithms. Along with the developed methodologies for graph learning in financial markets, we release an R package, called fingraph, accommodating the code and data to obtain all the experimental results.},
  creationdate     = {2023-06-24T19:10:29},
  file             = {:http://arxiv.org/pdf/2012.15410v1},
  keywords         = {cs.LG, eess.SP, q-fin.ST},
  modificationdate = {2023-06-24T19:10:29},
  timestamp        = {2021-01-04 02:40},
}

@Article{deMirandaCardoso-Palomar-2020,
  author           = {{de Miranda Cardoso}, {Jose Vinicius} and Daniel P. Palomar},
  date             = {2020-05-20},
  journaltitle     = {arXiv e-Print},
  title            = {Learning Undirected Graphs in Financial Markets},
  eprinttype       = {arXiv},
  url              = {https://arxiv.org/abs/2005.09958},
  abstract         = {We investigate the problem of learning undirected graphical models under Laplacian structural constraints from the point of view of financial market data. We show that Laplacian constraints have meaningful physical interpretations related to the market index factor and to the conditional correlations between stocks. Those interpretations lead to a set of guidelines that users should be aware of when estimating graphs in financial markets. In addition, we propose algorithms to learn undirected graphs that account for stylized facts and tasks intrinsic to financial data such as non-stationarity and stock clustering.},
  creationdate     = {2023-06-24T19:10:30},
  file             = {:http://arxiv.org/pdf/2005.09958v4},
  keywords         = {stat.ML, q-fin.CP, q-fin.ST},
  modificationdate = {2023-06-24T19:10:30},
  timestamp        = {2021-01-02 19:43},
}

@Article{Duarte-DeCastro-2020,
  author           = {Flavio Gabriel Duarte and {De Castro}, Leandro Nunes},
  date             = {2020},
  journaltitle     = {IEEE Access},
  title            = {A Framework to Perform Asset Allocation Based on Partitional Clustering},
  doi              = {10.1109/access.2020.3001944},
  pages            = {110775--110788},
  url              = {https://ieeexplore.ieee.org/abstract/document/9115637},
  volume           = {8},
  abstract         = {Over the past years, many approaches to perform asset allocation have been proposed in the literature. Most of them tackle this problem as an optimization task, where the goal is to maximize return, whilst minimizing the risk. However, such approaches require the inversion of a positive-definite covariance matrix, usually resulting in the concentration of allocation, instability and low performance. Some methods have been recently introduced to solve this problem by facing it as a clustering problem. This paper introduces a framework for asset allocation based on partitional clustering algorithms. The idea is to segment the assets into clusters of correlated assets, allocate resources for each cluster and then within each cluster. The framework allows the use of different partitional clustering algorithms, intragroup and intergroup allocation methods. Also, various assessment criteria are considered, and a specialized initialization method is proposed for the clustering algorithm. The framework is evaluated with the Brazilian Stock Exchange (B3) data from the period 12/2005 to 04/2020. Different initialization methods are used for the clustering algorithm together with two intergroup and two intragroup techniques, resulting in five experimental scenarios. The results are compared with the Ibovespa index, the mean-variance model of Markowitz, and the risk-parity model recently proposed by Lopez de Prado.},
  creationdate     = {2023-06-24T19:11:30},
  modificationdate = {2023-06-24T19:11:30},
  timestamp        = {2020-08-11 14:29},
}

@Article{Dugue-et-al-2021,
  author           = {Nicolas Dugu{\'{e}} and Jean-Charles Lamirel and Yue Chen},
  date             = {2021-05},
  journaltitle     = {Neural Computing and Applications},
  title            = {Evaluating clustering quality using features salience: a promising approach},
  doi              = {10.1007/s00521-021-05942-7},
  number           = {19},
  pages            = {12939--12956},
  volume           = {33},
  abstract         = {This paper focuses on using feature salience to evaluate the quality of a partition when dealing with hard clustering. It is based on the hypothesis that a good partition is an easy to label partition, i.e. a partition for which each cluster is made of salient features. This approach is mostly compared to usual approaches relying on distances between data, but also to more recent approaches based on entropy or stability. We show that our feature-based approach outperforms the compared indexes for optimal model selection: they are more efficient from low- to high-dimensional range as well as they are more robust to noise. To show the efficiency of our indexes on a real-life application, we consider the task of diachronic analysis on a textual dataset. We demonstrate that our approach allows to get some interesting and relevant results in that context, while other approaches mostly lead to unusable results.},
  creationdate     = {2023-06-24T19:13:58},
  modificationdate = {2023-06-24T19:13:58},
  publisher        = {Springer Science and Business Media {LLC}},
}

@MastersThesis{Eidenvall-2021,
  author           = {Eidenvall, Adam},
  date             = {2021},
  institution      = {Lund University},
  title            = {Hierarchical Clustering To Improve Portfolio Tail Risk Characteristics},
  url              = {https://lup.lub.lu.se/student-papers/search/publication/9042910},
  abstract         = {Many agree that estimating portfolio risks has better estimation possibilities, than estimations on returns. Therefore investors attempts to construct better, more efficient riskmanaged portfolios by diversifying portfolios through factors rather than traditional asset classes. This entails very often in estimations of correlation matrices so complex it cannot be fully analyzed. Hierarchical clustering reduces the complexity, by only focusing on the correlations that matters.

\leavevmode\newline 

Hierarchical clustering uses graph theory, linked to unsupervised machine learning techniques. Hierarchical clustering is obtained by the suggested data and is a formation of a recursive clustering. Several hierarchical clustering methods are presented and evaluated against traditional riskbased portfolios with focus on left hand tail risk. A regime shift, based on momentum is applied to minimize drawdowns. The portfolios are tested on simulated data derived from Bootstrapping simulations and on historical data in a Walk forward optimization process.

The results indicate that hierarchical clustering based portfolios are truly diversified and achieve statistically better riskadjusted performances than commonly used portfolio optimization techniques.},
  creationdate     = {2023-06-24T19:15:17},
  modificationdate = {2023-06-24T19:15:17},
}

@Article{Flint-et-al-2021a,
  author           = {Emlyn Flint and Florence Chikurunhe and Ndinae Masutha},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Managing Tail Risk Part I: Option-Based Hedging},
  doi              = {10.2139/ssrn.3817440},
  abstract         = {There is nothing like a market crash to focus the mind on the importance of risk management and, more specifically, tail risk management. Because tail events are generally systemic in nature and are characterised by elevated correlations and liquidity squeezes, effective tail risk management is not just a diversification exercise but rather requires explicit and specialised strategies. The natural question that arises then is how do you create an optimal tail risk management strategy? Most studies identify four categories of tail risk management strategy: option-based hedging, asset allocation, dynamic trading and defensive equity. Within each category, we then find a number of strategies available to investors for managing tail risk. Unfortunately, it remains an open question as to which strategy works best for a given investment scenario, or if there even exists a universally optimal strategy to begin with. We attempt to answer this question by analysing a range of commonly used tail risk management strategies in a South African market setting. However, given the breadth of the four available strategy categories, we split our research on this topic into two parts. In this Part I, we firstly examine and quantify the tail risk inherent in South African markets. This is done through a review the long-term history of equity and bond market drawdowns. Thereafter, we discuss nine core principles that are applicable to all candidate strategies and that define good tail risk management. Finally, we provide a comprehensive analysis of option-based tail hedging strategies. The concepts of defensive, offensive, active and indirect tail hedging are discussed at length and examples of each are implemented on historical market data.},
  creationdate     = {2023-06-24T19:18:51},
  modificationdate = {2023-06-24T19:18:51},
  publisher        = {Elsevier {BV}},
}

@Article{Flint-et-al-2021,
  author           = {E. Flint and A. Seymour and F. Chikurunhe},
  date             = {2021-01},
  journaltitle     = {South African Actuarial Journal},
  title            = {Defining and measuring portfolio diversification},
  doi              = {10.4314/saaj.v20i1.2},
  number           = {1},
  pages            = {17--48},
  volume           = {20},
  abstract         = {It is often said that diversification is the only 'free lunch' available to investors; meaning that a properly diversified portfolio reduces total risk without necessarily sacrificing expected return. However, achieving true diversification is easier said than done, especially when we do not fully know what we mean when we are talking about diversification. While the qualitative purpose of diversification is well known, a satisfactory quantitative definition of portfolio diversification remains elusive. In this research, we summarise a wide range of diversification measures, focusing our efforts on those most commonly used in practice. We categorise each measure based on which portfolio aspect it focuses on: cardinality, weights, returns, risk or higher moments. We then apply these measures to a range of South African equity indices, thus giving a diagnostic review of historical local equity diversification and, perhaps more importantly, providing a description of the investable opportunity set available tofund managers in this space. Finally, we introduce the idea of diversification profiles. These regime dependent profiles give a much richer description of portfolio diversification than their single-value counterparts and also allow one to manage diversification proactively based on one's view of future market conditions.},
  creationdate     = {2023-06-24T19:18:51},
  modificationdate = {2023-06-24T19:18:51},
  publisher        = {African Journals Online ({AJOL})},
}

@Article{Fu-Perry-2020,
  author           = {Wei Fu and Patrick O. Perry},
  date             = {2020},
  journaltitle     = {Journal of Computational and Graphical Statistics},
  title            = {Estimating the Number of Clusters Using Cross-Validation},
  doi              = {10.1080/10618600.2019.1647846},
  number           = {1},
  pages            = {162--173},
  volume           = {29},
  abstract         = {Many clustering methods, including k-means, require the user to specify the number of clusters as an input parameter. A variety of methods have been devised to choose the number of clusters automatically, but they often rely on strong modeling assumptions. This article proposes a data-driven approach to estimate the number of clusters based on a novel form of cross-validation. The proposed method differs from ordinary cross-validation, because clustering is fundamentally an unsupervised learning problem. Simulation and real data analysis results show that the proposed method outperforms existing methods, especially in high-dimensional settings with heterogeneous or heavy-tailed noise. In a yeast cell cycle dataset, the proposed method finds a parsimonious clustering with interpretable gene groupings. Supplementary materials for this article are available online.},
  creationdate     = {2023-06-24T19:19:34},
  modificationdate = {2023-06-24T19:19:34},
  publisher        = {Informa {UK} Limited},
}

@Article{Fusai-et-al-2020,
  author           = {Gianluca Fusai and Domenico Mignacca and Andrea Nardon and Ben Human},
  date             = {2020},
  journaltitle     = {Risk (Cutting Edge)},
  title            = {Equally Diversified or Equally Weighted?},
  url              = {https://www.risk.net/cutting-edge/investments/7675551/equally-diversified-or-equally-weighted},
  abstract         = {Gianluca Fusai, Domenico Mignacca, Andrea Nardon and Ben Human show how to decompose portfolio volatility into undiversified volatility and a diversification component. The authors' decomposition has a clear statistical interpretation because it relates the diversification component to partial covariances. On this basis, they advocate the construction of an equally diversified portfolio. An empirical analysis illustrates the superior out-of-sample performance of the equally diversified portfolio with respect to an equally weighted portfolio},
  creationdate     = {2023-06-24T19:21:18},
  modificationdate = {2023-06-24T19:21:18},
  timestamp        = {2020-09-08 19:45},
}

@Article{Giudici-et-al-2022,
  author           = {Paolo Giudici and Gloria Polinesi and Alessandro Spelta},
  date             = {2022},
  journaltitle     = {Annals of Operations Research},
  title            = {Network models to improve robot advisory portfolios},
  doi              = {10.1007/s10479-021-04312-9},
  pages            = {965-989},
  volume           = {313},
  abstract         = {Robot advisory services are rapidly expanding, responding to a growing interest people have in directly managing their savings. Robot-advisors may reduce costs and improve the quality of asset allocation services, making user's involvement more transparent. Against this background, there exists the possibility that robot advisors underestimate market risks, especially during crisis times, when high order interconnections arise. This may lead to a mismatch between investors' expected and actual risk. The aim of this paper is to overcome this issue, taking into account not only investors' risk preference but also their attitude towards interconnectdness. To achieve this aim, we combine random matrix theory with correlation networks and extend the Markowitz' optimisation problem to a third dimension. To demonstrate the practical advantage of our proposed approach we employ daily returns of a large set of Exchange Traded Funds, which are representative of the financial products employed by robot-advisors.},
  creationdate     = {2023-06-24T19:22:41},
  modificationdate = {2023-06-24T19:22:41},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Article{Guan-Loew-2021,
  author           = {Shuyue Guan and Murray Loew},
  date             = {2021-06-17},
  journaltitle     = {arXiv e-Print},
  title            = {A Distance-based Separability Measure for Internal Cluster Validation},
  eprint           = {2106.09794},
  eprintclass      = {cs.LG},
  eprinttype       = {arXiv},
  abstract         = {To evaluate clustering results is a significant part of cluster analysis. Since there are no true class labels for clustering in typical unsupervised learning, many internal cluster validity indices (CVIs), which use predicted labels and data, have been created. Without true labels, to design an effective CVI is as difficult as to create a clustering method. And it is crucial to have more CVIs because there are no universal CVIs that can be used to measure all datasets and no specific methods of selecting a proper CVI for clusters without true labels. Therefore, to apply a variety of CVIs to evaluate clustering results is necessary. In this paper, we propose a novel internal CVI -- the Distance-based Separability Index (DSI), based on a data separability measure. We compared the DSI with eight internal CVIs including studies from early Dunn (1974) to most recent CVDD (2019) and an external CVI as ground truth, by using clustering results of five clustering algorithms on 12 real and 97 synthetic datasets. Results show DSI is an effective, unique, and competitive CVI to other compared CVIs. We also summarized the general process to evaluate CVIs and created the rank-difference metric for comparison of CVIs' results.},
  creationdate     = {2023-06-24T19:24:14},
  file             = {:http\://arxiv.org/pdf/2106.09794v1:PDF},
  keywords         = {cs.LG, cs.CV, cs.DM},
  modificationdate = {2023-06-24T19:24:14},
}

@Article{Heckens-Guhr-2022b,
  author           = {Anton J. Heckens and Thomas Guhr},
  date             = {2022-10},
  journaltitle     = {Physica A: Statistical Mechanics and its Applications},
  title            = {New collectivity measures for financial covariances and correlations},
  doi              = {10.1016/j.physa.2022.127704},
  pages            = {127704},
  volume           = {604},
  abstract         = {Complex systems are usually non-stationary and their dynamics is often dominated by collective effects. Collectivity, defined as coherent motion of the whole system or of some of its parts, manifests itself in the time-dependent structures of covariance and correlation matrices. The largest eigenvalue corresponds to the collective motion of the system as a whole, while the other large, isolated, eigenvalues indicate collectivity in parts of the system. In the case of finance, these are industrial sectors. By removing the collective motion of the system as a whole, the latter effects are much better revealed. We measure a remaining collectivity to which we refer as average sector collectivity. We identify collective signals around the Lehman Brothers crash and after the dot-com bubble burst. For the Lehman Brothers crash, we find a potential precursor. We analyze 213 US stocks over a period of more than 30 years from 1990 to 2021. We plot the average sector collectivity versus the collectivity corresponding to the largest eigenvalue to study the whole market trajectory in a two dimensional space spanned by both collectivities. Therefore, we capture the average sector collectivity in a much more precise way. Additionally, we observe that larger values in the average sector collectivity are often accompanied by trend shifts in the mean covariances and mean correlations. As of 2015/2016 the collectivity in the US stock markets changed fundamentally.},
  creationdate     = {2023-06-24T19:26:30},
  modificationdate = {2023-06-24T19:26:30},
  publisher        = {Elsevier {BV}},
}

@Article{Herteliu-et-al-2021,
  author           = {Claudiu Herteliu and Susanna Levantesi and Giulia Rotundo},
  date             = {2021-10},
  journaltitle     = {Physica A: Statistical Mechanics and its Applications},
  title            = {Network analysis of pension funds investments},
  doi              = {10.1016/j.physa.2021.126139},
  pages            = {126139},
  volume           = {579},
  abstract         = {In this paper, we analyze the Italian pension funds and their declared benchmarks, which are market indexes. Within this perspective, the amounts invested in accord to the declared benchmarks can be analyzed like as a portfolio of benchmarks. We aim at understanding whether the pension funds investments are in line with the optimal portfolios which can be built through the declared benchmarks. To achieve the results, we set up a portfolio optimization problem building two networks of pension funds: one based on the (Pearson) correlation, and the other measuring the tail correlation. For each network, we use the local clustering coefficients to describe the level of connectivity, and we insert it in the risk function. This approach allows us to consider the network measures directly in the portfolio optimization model. We compare the results with the classical Markowitz setting, and we find a new efficient frontier overperforming the Markowitz one. A comparison among the performances of pension funds and their declared portfolio of benchmarks is also reported.},
  creationdate     = {2023-06-24T19:27:20},
  modificationdate = {2023-06-24T19:27:20},
  publisher        = {Elsevier {BV}},
}

@Article{Horvath-et-al-2022b,
  author           = {Lajos Horv{\'{a}}th and Zhenya Liu and Gregory Rice and Yuqian Zhao},
  date             = {2022},
  journaltitle     = {The Econometrics Journal},
  title            = {Detecting common breaks in the means of high dimensional cross-dependent panels},
  doi              = {10.1093/ectj/utab028},
  number           = {2},
  pages            = {362--383},
  volume           = {25},
  abstract         = {The problem of detecting change points in the mean of high dimensional panel data with potentially strong cross-sectional dependence is considered. Under the assumption that the cross-sectional dependence is captured by an unknown number of common factors, a new CUSUM-type statistic is proposed. We derive its asymptotic properties under three scenarios depending on to what extent the common factors are asymptotically dominant. With panel data consisting of N cross sectional time series of length T, the asymptotic results hold under the mild assumption that min(T,N) goes to infinity, with an otherwise arbitrary relationship between N and T, allowing the results to apply to most panel data examples. Bootstrap procedures are proposed to approximate the sampling distribution of the test statistics. A Monte Carlo simulation study showed that our test outperforms several other existing tests in finite samples in a number of cases, particularly when N is much larger than T. The practical application of the proposed results are demonstrated with real data applications to detecting and estimating change points in the high dimensional FRED-MD macroeconomic data set.},
  creationdate     = {2023-06-24T19:28:05},
  modificationdate = {2023-06-24T19:28:05},
  publisher        = {Oxford University Press ({OUP})},
}

@Article{Horvath-et-al-2022a,
  author           = {Miklos Z. Horvath and Mark Niklas Muller and Marc Fischer and Martin Vechev},
  date             = {2022-04-01},
  journaltitle     = {arXiv e-Print},
  title            = {Robust and Accurate -- Compositional Architectures for Randomized Smoothing},
  eprint           = {2204.00487},
  eprintclass      = {cs.LG},
  eprinttype       = {arXiv},
  abstract         = {Randomized Smoothing (RS) is considered the state-of-the-art approach to obtain certifiably robust models for challenging tasks. However, current RS approaches drastically decrease standard accuracy on unperturbed data, severely limiting their real-world utility. To address this limitation, we propose a compositional architecture, ACES, which certifiably decides on a per-sample basis whether to use a smoothed model yielding predictions with guarantees or a more accurate standard model without guarantees. This, in contrast to prior approaches, enables both high standard accuracies and significant provable robustness. On challenging tasks such as ImageNet, we obtain, e.g., $80.0\%$ natural accuracy and $28.2\%$ certifiable accuracy against $\ell_2$ perturbations with $r=1.0$. We release our code and models at \url{https://github.com/eth-sri/aces}},
  creationdate     = {2023-06-24T19:28:05},
  file             = {:http\://arxiv.org/pdf/2204.00487v1:PDF},
  keywords         = {cs.LG, cs.AI, cs.CR},
  modificationdate = {2023-06-24T19:28:05},
}

@Article{Horvath-et-al-2022,
  author           = {Lajos Horv{\'{a}}th and Hemei Li and Zhenya Liu},
  date             = {2022},
  journaltitle     = {Finance Research Letters},
  title            = {How to identify the different phases of stock market bubbles statistically?},
  doi              = {10.1016/j.frl.2021.102366},
  issue            = {102366},
  volume           = {46 (Part A)},
  abstract         = {Eugene Fama once mentioned in 2016 that people have not come up with ways of identifying bubbles statistically. This paper presents the nonparametric change-point method to identify different stages of stock bubbles, and we derive its asymptotic distribution under the null hypothesis. By simulation, we obtain the corresponding critical value. In the empirical analysis, we employ this test and binary segmentation method to the 1990s Nasdaq bubble and get the same result as Phillips et al. (2011). We also apply this test to the S\&P 500 index, the Shanghai stock index, the Nikkei 225 index, the FTSE 100 index, and the CAC 40 index respectively, and successfully identify the bubbles' different phases in each stock market.},
  creationdate     = {2023-06-24T19:28:05},
  modificationdate = {2023-06-24T19:28:05},
  publisher        = {Elsevier {BV}},
}

@Article{Horvath-et-al-2021b,
  author           = {Blanka Horvath and Zacharia Issa and Aitor Muguruza},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Clustering Market Regimes Using the Wasserstein Distance},
  doi              = {10.2139/ssrn.3947905},
  abstract         = {he problem of rapid and automated detection of distinct market regimes is a topic of great interest to financial mathematicians and practitioners alike. In this paper, we outline an unsupervised learning algorithm for clustering financial time-series into a suitable number of temporal segments (market regimes).
As a special case of the above, we develop a robust algorithm that automates the process of classifying market regimes. The method is robust in the sense that it does not depend on modelling assumptions of the underlying time series as our experiments with real datasets show. This method -- dubbed the Wasserstein $k$-means algorithm -- frames such a problem as one on the space of probability measures with finite $p^{th}$ moment, in terms of the $p$-Wasserstein distance between (empirical) distributions. We compare our WK-means approach with a more traditional clustering algorithms by studying the so-called maximum mean discrepancy scores between, and within clusters. In both cases it is shown that the WK-means algorithm vastly outperforms all considered competitor approaches. We demonstrate the performance of all approaches both in a controlled environment on synthetic data, and on real data.},
  creationdate     = {2023-06-24T19:28:05},
  modificationdate = {2023-06-24T19:28:05},
  publisher        = {Elsevier {BV}},
}

@Article{Horvath-et-al-2021a,
  author           = {Blanka Horvath and Zacharia Issa and Aitor Muguruza},
  date             = {2021-10-22},
  journaltitle     = {arXiv e-Print},
  title            = {Clustering Market Regimes using the Wasserstein Distance},
  eprint           = {2110.11848},
  eprintclass      = {q-fin.CP},
  eprinttype       = {arXiv},
  abstract         = {The problem of rapid and automated detection of distinct market regimes is a topic of great interest to financial mathematicians and practitioners alike. In this paper, we outline an unsupervised learning algorithm for clustering financial time-series into a suitable number of temporal segments (market regimes). As a special case of the above, we develop a robust algorithm that automates the process of classifying market regimes. The method is robust in the sense that it does not depend on modelling assumptions of the underlying time series as our experiments with real datasets show. This method -- dubbed the Wasserstein $k$-means algorithm -- frames such a problem as one on the space of probability measures with finite $p^{th}$ moment, in terms of the $p$-Wasserstein distance between (empirical) distributions. We compare our WK-means approach with a more traditional clustering algorithms by studying the so-called maximum mean discrepancy scores between, and within clusters. In both cases it is shown that the WK-means algorithm vastly outperforms all considered competitor approaches. We demonstrate the performance of all approaches both in a controlled environment on synthetic data, and on real data.},
  creationdate     = {2023-06-24T19:28:05},
  file             = {:http\://arxiv.org/pdf/2110.11848v1:PDF},
  keywords         = {q-fin.CP, cs.LG, q-fin.MF, 91-08 (Primary), 91G60 (Secondary)},
  modificationdate = {2023-06-24T19:28:05},
}

@Article{Horvath-et-al-2021,
  author           = {Lajos Horv{\'{a}}th and Piotr Kokoszka and Shixuan Wang},
  date             = {2021-08},
  journaltitle     = {Annals of Statistics},
  title            = {Monitoring for a change point in a sequence of distributions},
  doi              = {10.1214/20-aos2036},
  number           = {4},
  volume           = {49},
  abstract         = {We propose a method for the detection of a change point in a sequence Fi of distributions, which are available through a large number of observations at each i>=1. Under the null hypothesis, the distributions Fi are equal. Under the alternative hypothesis, there is a change point i*>1, such that Fi=G for i>=i* and some unknown distribution G, which is not equal to F1. The change point, if it exists, is unknown, and the distributions before and after the potential change point are unknown. The decision about the existence of a change point is made sequentially, as new data arrive. At each time i, the count of observations, N, can increase to infinity. The detection procedure is based on a weighted version of the Wasserstein distance. Its asymptotic and finite sample validity is established. Its performance is illustrated by an application to returns on stocks in the S\&P 500 index.},
  creationdate     = {2023-06-24T19:28:05},
  modificationdate = {2023-06-24T19:28:05},
  publisher        = {Institute of Mathematical Statistics},
}

@Article{Horvath-2021,
  author           = {Ferenc Horvath},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Arbitrage-Based Recovery},
  doi              = {10.2139/ssrn.3880921},
  abstract         = {We develop a novel recovery theorem based on no-arbitrage principles. Our Arbitrage-Based Recovery Theorem does not require assuming time homogeneity of either the physical probabilities, the Arrow-Debreu prices, or the stochastic discount factor; and it requires the observation of Arrow-Debreu prices only for one single maturity. We perform several different density tests and mean prediction tests using 25 years of S\&P 500 options data, and we find evidence that our method can correctly recover the probability distribution of the S\&P 500 index level on a monthly horizon.},
  creationdate     = {2023-06-24T19:28:05},
  modificationdate = {2023-06-24T19:28:05},
  publisher        = {Elsevier {BV}},
}

@Article{Horvath-et-al-2020a,
  author           = {Lajos Horv{\'{a}}th and Piotr Kokoszka and Shixuan Wang},
  date             = {2020-09},
  journaltitle     = {Journal of Multivariate Analysis},
  title            = {Testing normality of data on a multivariate grid},
  doi              = {10.1016/j.jmva.2020.104640},
  pages            = {104640},
  volume           = {179},
  abstract         = {We propose a significance test to determine if data on a regular -dimensional grid can be assumed to be a realization of Gaussian process. By accounting for the spatial dependence of the observations, we derive statistics analogous to sample skewness and kurtosis. We show that the sum of squares of these two statistics converges to a chi-square distribution with two degrees of freedom. This leads to a readily applicable test. We examine two variants of the test, which are specified by two ways the spatial dependence is estimated. We provide a careful theoretical analysis, which justifies the validity of the test for a broad class of stationary random fields. A simulation study compares several implementations. While some implementations perform slightly better than others, all of them exhibit very good size control and high power, even in relatively small samples. An application to a comprehensive data set of sea surface temperatures further illustrates the usefulness of the test.},
  creationdate     = {2023-06-24T19:28:05},
  modificationdate = {2023-06-24T19:28:05},
  publisher        = {Elsevier {BV}},
  timestamp        = {2021-01-19 07:22},
}

@Article{Horvath-et-al-2019,
  author           = {Lajos Horvath and Curtis Miller and Gregory Rice},
  date             = {2019-04},
  journaltitle     = {Journal of Business \& Economic Statistics},
  title            = {A new class of change point test statistics of Renyi type},
  doi              = {10.1080/07350015.2018.1537923},
  abstract         = {A new class of change point test statistics is proposed that utilizes a weighting and trimming scheme for the cumulative sum (CUSUM) process inspired by Renyi (1953). A thorough asymptotic analysis and simulations both demonstrate that this new class of statistics possess superior power compared to traditional change point statistics based on the CUSUM process when the change point is near the beginning or end of the sample. Generalizations of these "Renyi" statistics are also developed to test for changes in the parameters in linear and non-linear regression models, and in generalized method of moments estimation. In these contexts we applied the proposed statistics, as well as several others, to test for changes in the coefficients of Fama-French factor models. We observed that the Renyi statistic was the most effective in terms of retrospectively detecting change points that occur near the endpoints of the sample.},
  creationdate     = {2023-06-24T19:28:05},
  modificationdate = {2023-06-24T19:28:05},
  timestamp        = {2019-09-26 10:15},
}

@Article{Huang-et-al-2022a,
  author           = {Qi-An Huang and Jun-Chan Zhao and Xiao-Qun Wu},
  date             = {2022-01},
  journaltitle     = {Physica A: Statistical Mechanics and its Applications},
  title            = {Financial risk propagation between Chinese and American stock markets based on multilayer networks},
  doi              = {10.1016/j.physa.2021.126445},
  pages            = {126445},
  volume           = {586},
  abstract         = {Stock networks, which are constructed from stock price time series, are useful tools for analyzing complex behaviors in stock markets. Following former researches, the epidemic model has been usually used to detect dynamic characteristics in a stock price complex systems. Recently, multilayer networks have been demonstrated well when working on heterogeneous nodes rather than integrated networks. In this paper, we proposed a two-layer SIR propagation model with an infective medium to analyze the spread of financial shocks. In consideration of strict financial regulation in the A shares, the model assumed that capital cannot flow directly between layers but through the Hong Kong stock market. By applying the model to constituent stocks included in three prominent indices, Standard \& Poor 500, Shanghai and Shenzhen 300, and Hang Seng(medium), we established a two-layer Granger networks. Betweenness showed that the Hong Kong stock market had a promoting transition function of financial shocks between the US stock markets and the mainland China stock markets. In addition, with a big basic reproduction number, stock markets system appeared to be vulnerable during extreme financial shock such as the outbreak of COVID-19 epidemic and the meltdown of stock markets. Furthermore, sensitivity analysis and the spreading simulation indicated that the US stock markets were much more robust to financial shocks than the mainland China stock markets.},
  creationdate     = {2023-06-24T19:29:02},
  modificationdate = {2023-06-24T19:29:02},
  publisher        = {Elsevier {BV}},
}

@Article{Jaeger-et-al-2021,
  author           = {Jaeger, Markus and Krugel, Stephan and Marinelli, Dimitri and Papenbrock, Jochen and Schwendner, Peter},
  date             = {2021},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {Interpretable Machine Learning for Diversified Portfolio Construction},
  doi              = {10.3905/jfds.2021.1.066},
  number           = {3},
  pages            = {31-51},
  url              = {https://jfds.pm-research.com/content/early/2021/06/14/jfds.2021.1.066},
  volume           = {3},
  abstract         = {In this paper, the authors construct a pipeline to benchmark Hierarchical Risk Parity (HRP) relative to Equal Risk Contribution (ERC) as examples of diversification strategies allocating to liquid multi-asset futures markets with dynamic leverage ("volatility target"). The authors use interpretable machine learning concepts ("explainable AI") to compare the robustness of the strategies and to back out implicit rules for decision making. The empirical dataset consists of 17 equity index, government bond and commodity futures markets across 20 years. The two strategies are backtested for the empirical dataset and for about 100 000 bootstrapped datasets. XGBoost is used to regress the Calmar ratio spread between the two strategies against features of the bootstrapped datasets. Compared to ERC, HRP shows higher Calmar ratios and better matches the volatility target. Using Shapley values, the Calmar ratio spread can be attributed especially to univariate drawdown measures of the asset classes.},
  creationdate     = {2023-06-24T19:31:51},
  modificationdate = {2023-06-24T19:31:51},
  timestamp        = {2021-01-09 15:53},
}

@Article{Jaeger-et-al-2021b,
  author           = {Markus Jaeger and Stephan Krugel and Jochen Papenbrock and Peter Schwendner},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Adaptive Seriational Risk Parity and other Extensions for Heuristic Portfolio Construction using Machine Learning and Graph Theory},
  doi              = {10.2139/ssrn.3806714},
  abstract         = {In this article, the authors present a conceptual framework named Adaptive Seriational Risk Parity (ASRP) to extend Hierarchical Risk Parity (HRP) as an asset allocation heuristic. The first step of HRP (quasi-diagonalization) determining the hierarchy of assets is required for the actual allocation in the second step of HRP (recursive bisectioning). In the original HRP scheme, this hierarchy is found using the single-linkage hierarchical clustering of the correlation matrix, which is a static tree-based method. The authors of this paper compare the performance of the standard HRP with other static and also adaptive tree-based methods, but also seriation-based methods that do not rely on trees. Seriation is a broader concept allowing to reorder the rows or columns of a matrix to best express similarities between the elements. Each discussed variation leads to a different time series reflecting portfolio performance using a 20-year backtest of a multi-asset futures universe. An unsupervised representation learning based on this time series data creates a taxonomy that groups the strategies in high correspondence to the structure of the various types of ASRP. The performance analysis of the variations shows that most of the static tree-based alternatives of HRP outperform the single linkage clustering used in HRP on a risk-adjusted basis. Adaptive tree methods show mixed results and most generic seriation-based approaches underperform.},
  creationdate     = {2023-06-24T19:31:51},
  modificationdate = {2023-06-24T19:31:51},
  publisher        = {Elsevier {BV}},
}

@Article{Jaeger-et-al-2021a,
  author           = {Jaeger, Markus and Krugel, Stephan and Marinelli, Dimitri and Papenbrock, Jochen and Schwendner, Peter},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Interpretable Machine Learning for Diversified Portfolio Construction},
  url              = {https://ssrn.com/abstract=3730144},
  abstract         = {In this paper, the authors construct a pipeline to benchmark Hierarchical Risk Parity (HRP) relative to Equal Risk Contribution (ERC) as examples of diversification strategies allocating to liquid multi-asset futures markets with dynamic leverage ("volatility target"). The authors use interpretable machine learning concepts ("explainable AI") to compare the robustness of the strategies and to back out implicit rules for decision making. The empirical dataset consists of 17 equity index, government bond and commodity futures markets across 20 years. The two strategies are backtested for the empirical dataset and for about 100 000 bootstrapped datasets. XGBoost is used to regress the Calmar ratio spread between the two strategies against features of the bootstrapped datasets. Compared to ERC, HRP shows higher Calmar ratios and better matches the volatility target. Using Shapley values, the Calmar ratio spread can be attributed especially to univariate drawdown measures of the asset classes.},
  creationdate     = {2023-06-24T19:31:55},
  modificationdate = {2023-06-24T19:31:55},
  timestamp        = {2021-01-09 15:53},
}

@Article{Katsouris-2022b,
  author           = {Christis Katsouris},
  date             = {2022},
  journaltitle     = {SSRN e-Print},
  title            = {Forecast Evaluation in Large Cross-Sections of Realized Volatility},
  doi              = {10.2139/ssrn.3981635},
  abstract         = {In this paper, we consider the forecast evaluation of realized volatility measures under cross-section dependence using equal predictive accuracy testing procedures. We evaluate the predictive accuracy of the model based on the augmented cross-section when forecasting Realized Volatility. Under the null hypothesis of equal predictive accuracy the benchmark model employed is a standard HAR model while under the alternative of non-equal predictive accuracy the forecast model is an augmented HAR model estimated via the LASSO shrinkage. We study the sensitivity of forecasts to the model specification by incorporating a measurement error correction as well as cross-sectional jump component measures. The out-of-sample forecast evaluation of the models is assessed with numerical implementations.},
  creationdate     = {2023-06-24T19:33:12},
  modificationdate = {2023-06-24T19:33:12},
  publisher        = {Elsevier {BV}},
}

@Article{Katsouris-2022a,
  author           = {Christis Katsouris},
  date             = {2022},
  journaltitle     = {SSRN e-Print},
  title            = {Sequential Break-Point Detection in Stationary Time Series: An Application to Monitoring Economic Indicators},
  doi              = {10.2139/ssrn.3983627},
  abstract         = {Monitoring economic conditions and financial stability with an early warning system serves as a prevention mechanism for unexpected economic events. In this paper, we investigate the statistical performance of sequential break-point detectors for stationary time series regression models with extensive simulation experiments. We employ an online sequential scheme for monitoring economic indicators from the European as well as the American financial markets that span the period during the 2008 financial crisis. Our results show that the performance of these tests applied to stationary time series regressions such as the AR(1) as well as the AR(1)-GARCH(1,1) depend on the severity of the break as well as the location of the break-point within the out-of-sample period. Consequently, our study provides some useful insights to practitioners for sequential break-point detection in economic and financial conditions.},
  creationdate     = {2023-06-24T19:33:12},
  modificationdate = {2023-06-24T19:33:12},
  publisher        = {Elsevier {BV}},
}

@Article{Katsouris-2022,
  author           = {Christis Katsouris},
  date             = {2022},
  journaltitle     = {SSRN e-Print},
  title            = {Optimal Portfolio Choice and Stock Centrality for Tail Risk Events},
  doi              = {10.2139/ssrn.3971170},
  abstract         = {We propose a novel risk matrix to characterize the optimal portfolio choice of an investor with tail concerns. The diagonal of the matrix contains the Value-at-Risk of each asset in the portfolio and the off-diagonal the pairwise Delta-CoVaR measures reflecting tail connections between assets. First, we derive the conditions under which the associated quadratic risk function has a closed-form solution. Second, we examine the relationship between portfolio risk and eigenvector centrality. Third, we show that portfolio risk is not necessarily increasing with respect to stock centrality. Forth, we demonstrate under certain conditions that asset centrality increases the optimal weight allocation of the asset to the portfolio. Overall, our empirical study indicates that a network topology which exhibits low connectivity is outperformed by high connectivity based on a Sharpe ratio test.},
  creationdate     = {2023-06-24T19:33:12},
  modificationdate = {2023-06-24T19:33:12},
  publisher        = {Elsevier {BV}},
}

@Article{Katsouris-2021b,
  author           = {Christis Katsouris},
  date             = {2021-12-09},
  journaltitle     = {arXiv e-Print},
  title            = {Forecast Evaluation in Large Cross-Sections of Realized Volatility},
  eprint           = {2112.04887},
  eprintclass      = {stat.ML},
  eprinttype       = {arXiv},
  abstract         = {In this paper, we consider the forecast evaluation of realized volatility measures under cross-section dependence using equal predictive accuracy testing procedures. We evaluate the predictive accuracy of the model based on the augmented cross-section when forecasting Realized Volatility. Under the null hypothesis of equal predictive accuracy the benchmark model employed is a standard HAR model while under the alternative of non-equal predictive accuracy the forecast model is an augmented HAR model estimated via the LASSO shrinkage. We study the sensitivity of forecasts to the model specification by incorporating a measurement error correction as well as cross-sectional jump component measures. The out-of-sample forecast evaluation of the models is assessed with numerical implementations.},
  creationdate     = {2023-06-24T19:33:12},
  file             = {:http\://arxiv.org/pdf/2112.04887v1:PDF},
  keywords         = {stat.ML, cs.LG},
  modificationdate = {2023-06-24T19:33:12},
}

@Article{Katsouris-2021a,
  author           = {Christis Katsouris},
  date             = {2021-12-01},
  journaltitle     = {arXiv e-Print},
  title            = {Optimal Portfolio Choice and Stock Centrality for Tail Risk Events},
  eprint           = {2112.12031},
  eprintclass      = {q-fin.PM},
  eprinttype       = {arXiv},
  abstract         = {We propose a novel risk matrix to characterize the optimal portfolio choice of an investor with tail concerns. The diagonal of the matrix contains the Value-at-Risk of each asset in the portfolio and the off-diagonal the pairwise Delta-CoVaR measures reflecting tail connections between assets. First, we derive the conditions under which the associated quadratic risk function has a closed-form solution. Second, we examine the relationship between portfolio risk and eigenvector centrality. Third, we show that portfolio risk is not necessarily increasing with respect to stock centrality. Forth, we demonstrate under certain conditions that asset centrality increases the optimal weight allocation of the asset to the portfolio. Overall, our empirical study indicates that a network topology which exhibits low connectivity is outperformed by high connectivity based on a Sharpe ratio test.},
  creationdate     = {2023-06-24T19:33:12},
  file             = {:http\://arxiv.org/pdf/2112.12031v1:PDF},
  keywords         = {q-fin.PM},
  modificationdate = {2023-06-24T19:33:12},
}

@Article{Katsouris-2021,
  author           = {Christis Katsouris},
  date             = {2021-12-13},
  journaltitle     = {arXiv e-Print},
  title            = {Sequential Break-Point Detection in Stationary Time Series: An Application to Monitoring Economic Indicators},
  eprint           = {2112.06889},
  eprintclass      = {stat.AP},
  eprinttype       = {arXiv},
  abstract         = {Monitoring economic conditions and financial stability with an early warning system serves as a prevention mechanism for unexpected economic events. In this paper, we investigate the statistical performance of sequential break-point detectors for stationary time series regression models with extensive simulation experiments. We employ an online sequential scheme for monitoring economic indicators from the European as well as the American financial markets that span the period during the 2008 financial crisis. Our results show that the performance of these tests applied to stationary time series regressions such as the AR(1) as well as the AR(1)-GARCH(1,1) depend on the severity of the break as well as the location of the break-point within the out-of-sample period. Consequently, our study provides some useful insights to practitioners for sequential break-point detection in economic and financial conditions.},
  creationdate     = {2023-06-24T19:33:12},
  file             = {:http\://arxiv.org/pdf/2112.06889v1:PDF},
  keywords         = {stat.AP},
  modificationdate = {2023-06-24T19:33:12},
}

@Article{Kaya-2017,
  author               = {Kaya, Hakan},
  date                 = {2017},
  journaltitle         = {Journal of Asset Management},
  title                = {Managing ambiguity in asset allocation},
  doi                  = {10.1057/s41260-016-0029-0},
  number               = {3},
  pages                = {163--187},
  volume               = {18},
  abstract             = {This paper is about the issue of input parameter uncertainty in portfolio optimization in a discrete setting with finite states (such as the case in a world with different macroeconomic regimes). In such a setting, being unable to assign reliable point estimates to the probabilities (or frequencies) of the states creates the ambiguity. We first describe how this ambiguity can be modeled probabilistically. Then, we show how this added uncertainty can be dealt with in optimal asset allocation problems. In simple-yet-realistic example applications we demonstrate that without sacrificing much of the upside, ambiguity managed portfolios may enhance the uniformity of returns across different states when compared to portfolios constructed by traditional methods. We stress that a key conclusion to be taken from these methods builds the case for insurance-like and potentially negative-yielding investments such as bonds and commodities so as to hedge the unforeseeable macrouncertainties for a smoother portfolio performance. Finally, we offer a variety of problem domains in which ambiguity management can be nested including macroeconomic scenario-based asset allocation, investing with regime-switching models, momentum investing, and risk-based investing.},
  citeulike-article-id = {14217776},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/s41260-016-0029-0},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1057/s41260-016-0029-0},
  creationdate         = {2023-06-24T19:34:11},
  groups               = {PortfOptim_Scenario, Portf_Insurance},
  journal              = {Journal of Asset Management},
  modificationdate     = {2023-06-24T19:34:11},
  owner                = {cristi},
  posted-at            = {2016-11-30 19:41:45},
  publisher            = {Palgrave Macmillan UK},
  timestamp            = {2020-07-01 00:13},
}

@Article{Kaya-2015,
  author           = {Kaya, Hakan},
  date             = {2015},
  journaltitle     = {Journal of Network Theory in Finance},
  title            = {Eccentricity in Asset Management},
  number           = {1},
  pages            = {1--32},
  url              = {https://ssrn.com/abstract=2350429},
  volume           = {1},
  abstract         = {We describe how networks based on information theory can help measure and visualize systemic risk, enhance diversification, and help price assets. To do this, we first define a distance measure based on the mutual information between asset pairs and use this measure in the construction of minimum spanning trees. The dynamics of the shape and the descriptive statistics of these trees are analyzed in various investment domains. The method provides evidence of regime changes in dependency structures prior to market sell-offs, and as such, it is a potential candidate for monitoring systemic risk. We also provide empirical evidence that the assets that are located towards the center of the network tend to have higher returns. Finally, an investment strategy that utilizes network centrality information is shown to add value historically.},
  creationdate     = {2023-06-24T19:34:37},
  day              = {7},
  groups           = {Networks and investment management},
  modificationdate = {2023-06-24T19:34:37},
  owner            = {cristi},
  posted-at        = {2016-05-18 22:44:58},
  timestamp        = {2019-08-30 15:19},
}

@Article{Kinlaw-et-al-2023,
  author           = {William B. Kinlaw and Mark Kritzman and David Turkington},
  date             = {2023},
  journaltitle     = {{SSRN} Electronic Journal},
  title            = {Co-Occurrence: A New Perspective on Portfolio Diversification},
  doi              = {10.2139/ssrn.4450938},
  abstract         = {Investors typically measure an assets potential to diversify a portfolio by its correlations with the portfolios other assets, but correlation is useful only if it provides a good estimate of how an assets returns co-occur cumulatively with the other asset returns over the investors prospective horizon. And because correlation is an average of sub-period co-occurrences, it only serves as a good estimate of prospective co-occurrence if the assets returns are multi-variate normal, which requires them to be independent and identically distributed. The authors provide evidence that correlations differ depending on the return interval used to estimate them, which indicates they are not serially independent. Moreover, the authors show that asset co-movement differs between regimes of high and low interest rates and between turbulent and quiescent markets, and that they are asymmetric around return thresholds, which indicates that returns are not identically distributed. These departures from multi-variate normality cast serious doubt on the usefulness of full-sample correlations to measure an assets potential to diversify a portfolio. The authors propose an alternative technique for diversifying a portfolio that explicitly considers the empirical prevalence of co-occurrences and thus the non-normality of returns.},
  creationdate     = {2023-06-24T19:36:08},
  modificationdate = {2023-06-24T19:36:08},
  publisher        = {Elsevier {BV}},
}

@Article{Kinlaw-et-al-2021b,
  author           = {Kinlaw, William B. and Kritzman, Mark and Turkington, David},
  date             = {2020},
  journaltitle     = {Journal of Investment Management},
  title            = {A new index of the business cycle},
  number           = {3},
  pages            = {4-19},
  volume           = {19},
  abstract         = {The authors introduce a new index of the business cycle that uses the Mahalanobis distance to measure the statistical similarity of current economic conditions to past episodes of recession and robust growth. Their index has several important features that distinguish it from the Conference Board leading, coincident, and lagging indicators. It is efficient because as a single index it conveys reliable information about the path of the business cycle. Their index gives an independent assessment of the state of the economy because it is constructed from variables that are different than those used by the NBER to identify recessions. It is strictly data driven; hence, it is unaffected by human bias or persuasion. It gives an objective assessment of the business cycle because it is expressed in units of statistical likelihood. And it explicitly accounts for the interaction, along with the level, of the economic variables from which it is constructed.},
  creationdate     = {2023-06-24T19:36:08},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T19:36:08},
  timestamp        = {2020-07-23 13:37},
}

@Article{Kinlaw-et-al-2022,
  author           = {William B. Kinlaw and Mark Kritzman and Michael Metcalfe and David Turkington},
  date             = {2022},
  journaltitle     = {SSRN e-Print},
  title            = {The Determinants of Inflation},
  doi              = {10.2139/ssrn.4137861},
  abstract         = {The authors apply a Hidden Markov Model to identify regimes of shifting inflation and then employ an attribution technique based on the Mahalanobis distance to identify the economic variables that determine the trajectory of inflation. Their analysis enables policymakers to focus on the most effective tools to manage inflation, and it offers guidance to investors whose strategies might benefit from knowledge of the prevailing determinants of inflation. Their analysis reveals that as of February 2022, the most important determinant of the recent spike in inflation was spending by the federal government.},
  creationdate     = {2023-06-24T19:36:08},
  modificationdate = {2023-06-24T19:36:08},
  publisher        = {Elsevier {BV}},
}

@Article{Kinlaw-et-al-2021,
  author           = {William B. Kinlaw and Mark Kritzman and S{\'{e}}bastien Page and David Turkington},
  date             = {2021},
  journaltitle     = {The Journal Of Portfolio Management},
  title            = {The Myth of Diversification Reconsidered},
  doi              = {10.3905/jpm.2021.1.273},
  number           = {8},
  volume           = {47},
  abstract         = {That investors should diversify their portfolios is a core principle of modern finance. Yet there are some periods where diversification is undesirable. When the portfolio's main growth engine performs well, investors prefer the opposite of diversification. An ideal complement to the growth engine would provide diversification when it performs poorly and unification when it performs well. Numerous studies have presented evidence of asymmetric correlations between assets. Unfortunately, this asymmetry is often of the undesirable variety: it is characterized by downside unification and upside diversification. In other words, diversification often disappears when it is most needed. In this article we highlight a fundamental flaw in the way that some prior studies have measured correlation asymmetry. Because they estimate downside correlations from subsamples where both assets perform poorly, they ignore instances of "successful" diversification; that is, periods where one asset's gains offset the other's losses. We propose instead that investors measure what matters: the degree to which a given asset diversifies the main growth engine when it underperforms. This approach yields starkly different conclusions, particularly for asset pairs with low full sample correlation. In this paper we review correlation mathematics, highlight the flaw in prior studies, motivate the correct approach, and present an empirical analysis of correlation asymmetry across major asset classes.},
  creationdate     = {2023-06-24T19:36:08},
  modificationdate = {2023-06-24T19:36:08},
  publisher        = {Elsevier {BV}},
  timestamp        = {2021-03-06 14:23},
}

@Article{Kinlaw-et-al-2021a,
  author           = {William B. Kinlaw and Mark Kritzman and S{\'{e}}bastien Page and David Turkington},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {The Myth of Diversification Reconsidered},
  doi              = {10.2139/ssrn.3781844},
  abstract         = {That investors should diversify their portfolios is a core principle of modern finance. Yet there are some periods where diversification is undesirable. When the portfolio's main growth engine performs well, investors prefer the opposite of diversification. An ideal complement to the growth engine would provide diversification when it performs poorly and unification when it performs well. Numerous studies have presented evidence of asymmetric correlations between assets. Unfortunately, this asymmetry is often of the undesirable variety: it is characterized by downside unification and upside diversification. In other words, diversification often disappears when it is most needed. In this article we highlight a fundamental flaw in the way that some prior studies have measured correlation asymmetry. Because they estimate downside correlations from subsamples where both assets perform poorly, they ignore instances of "successful" diversification; that is, periods where one asset's gains offset the other's losses. We propose instead that investors measure what matters: the degree to which a given asset diversifies the main growth engine when it underperforms. This approach yields starkly different conclusions, particularly for asset pairs with low full sample correlation. In this paper we review correlation mathematics, highlight the flaw in prior studies, motivate the correct approach, and present an empirical analysis of correlation asymmetry across major asset classes.},
  creationdate     = {2023-06-24T19:36:08},
  modificationdate = {2023-06-24T19:36:08},
  publisher        = {Elsevier {BV}},
  timestamp        = {2021-03-06 14:23},
}

@Conference{Kinlaw-2012,
  author               = {Kinlaw, Will},
  booktitle            = {GARP},
  date                 = {2012},
  title                = {Regimes, Risk Factors and Asset Allocation},
  citeulike-article-id = {13930796},
  creationdate         = {2023-06-24T19:36:08},
  groups               = {Factor_Regime},
  modificationdate     = {2023-06-24T19:36:08},
  owner                = {zkgst0c},
  posted-at            = {2016-02-10 18:19:20},
  timestamp            = {2019-02-02 16:59},
}

@Article{Kinlaw-Kritzman-2009,
  author               = {Kinlaw, Will and Kritzman, Mark},
  date                 = {2009},
  journaltitle         = {Journal of Asset Management},
  title                = {Optimal currency hedging in and out of sample},
  doi                  = {10.1057/jam.2008.36},
  issn                 = {1470-8272},
  number               = {1},
  pages                = {22--36},
  volume               = {10},
  citeulike-article-id = {4313623},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2008.36},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/pal/jam/2009/00000010/00000001/art00003},
  citeulike-linkout-2  = {http://link.springer.com/article/10.1057/jam.2008.36},
  creationdate         = {2023-06-24T19:36:08},
  modificationdate     = {2023-06-24T19:36:08},
  posted-at            = {2017-03-19 19:24:36},
  publisher            = {Palgrave Macmillan UK},
  timestamp            = {2019-01-27 20:06},
}

@Article{Kinlaw-et-al-2013,
  author           = {Kinlaw, W. and Kritzman, M. and Turkington, D.},
  date             = {2013},
  journaltitle     = {The Journal of Portfolio Management},
  title            = {Liquidity and Portfolio Choice: A Unified Approach},
  doi              = {10.3905/jpm.2013.39.2.019},
  number           = {2},
  pages            = {19--27},
  url              = {https://jpm.pm-research.com/content/39/2/19},
  volume           = {36},
  abstract         = {Many investors struggle with how to account for liquidity when forming portfolios. By accounting for liquidity as a shadow allocation to a portfolio, attaching either a shadow asset to tradable assets or a shadow liability to nontradable assets, investors are better able to incorporate liquidity into their portfolio decisions.},
  creationdate     = {2023-06-24T19:36:08},
  groups           = {Invest_Liquidity},
  journal          = {The Journal of Portfolio Management},
  modificationdate = {2023-06-24T19:36:08},
  owner            = {zkgst0c},
  timestamp        = {2019-09-04 18:42},
}

@Article{Kinlaw-et-al-2014,
  author               = {Kinlaw, William and Kritzman, Mark and Turkington, David},
  date                 = {2014-09},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {The Divergence of High- and Low-Frequency Estimation: Causes and Consequences},
  doi                  = {10.3905/jpm.2014.40.5.156},
  issn                 = {0095-4918},
  number               = {5},
  pages                = {156--168},
  volume               = {40},
  abstract             = {Financial analysts typically estimate volatilities and correlations from monthly or higher-frequency returns when determining the optimal composition of a portfolio. Although it is widely acknowledged that these measures are not necessarily stationary across samples, most analysts assume implicitly that, within sample, volatilities scale with the square root of time and correlations estimated from high-frequency returns are similar to correlations estimated from low-frequency returns.

Evidence does not support this view. Instead, evidence shows that relative asset values often evolve through time in ways that are highly inconsistent with their high-frequency volatilities and correlations. As a consequence, portfolios that are optimal based on high-frequency returns often lead to significantly suboptimal results for investors with long horizons. The causes and consequences of this discrepancy are analyzed by the article's authors, as well as presenting a framework for constructing portfolios that balance short-horizon and long-horizon optimality.},
  citeulike-article-id = {13972216},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2014.40.5.156},
  creationdate         = {2023-06-24T19:36:08},
  modificationdate     = {2023-06-24T19:36:08},
  owner                = {cristi},
  posted-at            = {2016-03-08 09:36:00},
  timestamp            = {2019-09-02 10:40},
}

@Article{Kinlaw-Turkington-2013,
  author               = {Kinlaw, Will and Turkington, David},
  date                 = {2014},
  journaltitle         = {Journal of Asset Management},
  title                = {Correlation surprise},
  doi                  = {10.1057/jam.2013.27},
  issn                 = {1470-8272},
  number               = {6},
  pages                = {385--399},
  volume               = {14},
  abstract             = {Soon after Harry Markowitz published his landmark 1952 article on portfolio selection, the correlation coefficient assumed vital significance as a measure of diversification and an input to portfolio construction. However, investors typically overlook the potential for correlation patterns to help predict subsequent return and risk. Kritzman and Li (2010) introduced what is perhaps the first measure to capture the degree of multivariate asset price 'unusualness' through time. Their financial turbulence score spikes when asset prices 'behave in an uncharacteristic fashion, including extreme price moves, decoupling of correlated assets, and convergence of uncorrelated assets.'

We extend Kritzman and Li's study by disentangling the volatility and correlation components of turbulence to derive a measure of correlation surprise. We show how correlation surprise is orthogonal to volatility and present empirical evidence that it contains incremental forward-looking information. On average, after controlling for volatility, we find that periods characterized by correlation surprise lead to higher risk and lower returns to risk premia than periods characterized by typical correlations. This result holds across many markets including US equities, European equities and foreign exchange.

Our results corroborate the predictive capacity of turbulence and suggest that its decomposition may also prove fruitful in forecasting investment performance.},
  citeulike-article-id = {13968911},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2013.27},
  creationdate         = {2023-06-24T19:36:08},
  day                  = {09},
  groups               = {FrcstQWIM_MedLngTerm, FcstQWIM_Equity, Financial_Turbulence},
  modificationdate     = {2023-06-24T19:36:08},
  owner                = {cristi},
  posted-at            = {2016-03-06 06:30:24},
  timestamp            = {2019-09-02 10:40},
}

@Article{Kinlaw-et-al-2015,
  author           = {Kinlaw, William B. and Kritzman, Mark and Mao, Jason},
  date             = {2015},
  journaltitle     = {The Journal of Alternative Investments},
  title            = {The Components of Private Equity Performance: Implications for Portfolio Choice},
  number           = {2},
  pages            = {25--38},
  volume           = {18},
  abstract         = {We use a proprietary database of private equity returns to measure the excess return of private equity over public equity and to partition it into two components: an asset class alpha and compensation for illiquidity.

Our evidence suggests that private equity managers, as a group, generate alpha by anticipating the relative performance of economic sectors. If we assume that manager specific alpha is fully diluted across a broad universe of private equity managers, we can interpret the balance of excess return as a premium for illiquidity.

This result suggests that investors can capture the asset class alpha of private equity by using liquid assets such as ETFs to match the sector weights of private equity investors. This decomposition of private equity performance has important implications for portfolio choice, which we explore in this paper.},
  creationdate     = {2023-06-24T19:36:08},
  groups           = {Private_Equity, Performance_Metrics},
  modificationdate = {2023-06-24T19:36:08},
  owner            = {zkgst0c},
  timestamp        = {2019-09-02 10:40},
}

@Article{Kinlaw-et-al-2018,
  author           = {Kinlaw, William B. and Kritzman, Mark and Turkington, David},
  date             = {2018},
  journaltitle     = {SSRN e-Print},
  title            = {Crowded Trades},
  doi              = {10.2139/ssrn.3182664},
  issn             = {1556-5068},
  url              = {https://ssrn.com/abstract=3182664},
  abstract         = {Crowded trades are often associated with bubbles. If investors can locate a bubble sufficiently early they can profit from the run up in prices. But in order to profit from a bubble investors must exit the bubble before the selloff erodes all of the profits. The authors propose two measures for managing exposure to bubbles. One measure, called asset centrality, locates crowded trading which often leads to the formation of bubbles. The other is a measure of relative value, which helps to separate inflationary crowding from deflationary crowding. Neither measure by itself is sufficient for identifying the full cycle of a bubble, but the authors show that together these measures have the potential to locate bubbles as they begin to emerge and to identify exit points before they fully deflate.},
  creationdate     = {2023-06-24T19:36:08},
  f1000-projects   = {QuantInvest},
  groups           = {Invest_Crowding},
  modificationdate = {2023-06-24T19:36:08},
  timestamp        = {2020-07-23 13:37},
}

@Article{Kinlaw-et-al-2019,
  author           = {Kinlaw, W. and Kritzman, M. and Turkington, D.},
  date             = {2019-06-30},
  journaltitle     = {The Journal of Portfolio Management},
  title            = {Crowded trades: implications for sector rotation and factor timing},
  doi              = {10.3905/jpm.2019.45.5.046},
  issn             = {0095-4918},
  number           = {5},
  pages            = {46--57},
  urldate          = {2019-09-22},
  volume           = {45},
  abstract         = {Crowded trades are often associated with bubbles. If investors can locate a bubble sufficiently early, they can profit from the run-up in prices. But to profit from a bubble, investors must exit the bubble before the sell-off erodes all of the profits. The authors propose two measures for managing exposure to bubbles. One measure, called asset centrality, locates crowded trading, which they show is often associated with the formation of bubbles. The other is a measure of relative value, which helps to separate crowding that occurs during a bubble run-up from crowding that occurs during a bubble sell-off. Neither measure by itself is sufficient for identifying the full cycle of a bubble, but the authors show that together these measures have the potential to locate bubbles in sectors and in factors as they begin to emerge and to identify exit points before they fully deflate.},
  creationdate     = {2023-06-24T19:36:08},
  day              = {30},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T19:36:08},
  timestamp        = {2019-09-26 08:14},
}

@Article{Kinlaw-et-al-2020,
  author           = {Kinlaw, William B. and Kritzman, Mark and Turkington, David},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {A new index of the business cycle},
  doi              = {10.2139/ssrn.3521300},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3521300},
  urldate          = {2020-01-22},
  abstract         = {The authors introduce a new index of the business cycle that uses the Mahalanobis distance to measure the statistical similarity of current economic conditions to past episodes of recession and robust growth. Their index has several important features that distinguish it from the Conference Board leading, coincident, and lagging indicators. It is efficient because as a single index it conveys reliable information about the path of the business cycle. Their index gives an independent assessment of the state of the economy because it is constructed from variables that are different than those used by the NBER to identify recessions. It is strictly data driven; hence, it is unaffected by human bias or persuasion. It gives an objective assessment of the business cycle because it is expressed in units of statistical likelihood. And it explicitly accounts for the interaction, along with the level, of the economic variables from which it is constructed.},
  creationdate     = {2023-06-24T19:36:12},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T19:36:12},
  timestamp        = {2020-07-23 13:37},
}

@Article{Konstantinov-2023,
  author           = {Gueorgui S. Konstantinov},
  date             = {2023-03},
  journaltitle     = {The Journal of Portfolio Management},
  title            = {Errors and Challenges Associated with Investing in {EMU} Government Bonds},
  doi              = {10.3905/jpm.2023.1.481},
  abstract         = {This article highlights some of the main practical errors and challenges associated with investing in Economic and Monetary Union (EMU) government bonds and the difficulties in implementing EMU government bond strategies. These challenges refer to both portfolio allocation and to factor bond portfolios. Different country economic fundamentals, central bank policy, market timing, market risks, model and management risks, liquidity, and idiosyncratic country risks as well as spillover effects should be considered when investing in individual EMU government bonds. These essential properties of EMU bonds are not adequately addressed in portfolio allocation models (e.g., the tracking error or the mean-variance optimization) applied to single bond portfolios from equity investing. The main EMU government bond disparities are associated with country risks, which can be generalized to the main difference between core and periphery country yield spreads.},
  creationdate     = {2023-06-24T19:36:55},
  modificationdate = {2023-06-24T19:36:55},
  publisher        = {Pageant Media {US}},
}

@Article{Konstantinov-2022a,
  author           = {Gueorgui S. Konstantinov},
  date             = {2022-06},
  journaltitle     = {The Journal of Portfolio Management},
  title            = {Emerging Market Bonds: Expected Returns and Currency Impact},
  doi              = {10.3905/jpm.2022.1.385},
  abstract         = {Optimizing in local currency or in currency-adjusted expected returns depends on the portfolio base currency. Currency unhedged portfolios are more suitable for EUR-based investors and much less for CHF-based portfolios. The appreciation of the portfolio base currency represents a serious risk. Emerging market bond portfolios gain from the absence of currency risk when the portfolio base currency is the US dollar. The empirical results reveal that the traditional models like mean-variance-optimization (MVO), Minimum-Variance-Optimization (MinVo), and Sharpe ratio optimizations are suitable for portfolio allocation, using both local currency and currency-adjusted expected bond returns as inputs in the optimizations. However, network- and centrality-based allocations outperform traditional models.},
  creationdate     = {2023-06-24T19:36:55},
  modificationdate = {2023-06-24T19:36:55},
  publisher        = {Pageant Media {US}},
}

@Article{Konstantinov-2022,
  author           = {Gueorgui S. Konstantinov},
  date             = {2022-06},
  journaltitle     = {The Journal of Alternative Investments},
  title            = {Hedge Fund Networks},
  doi              = {10.3905/jai.2022.1.168},
  number           = {2},
  pages            = {14-32},
  volume           = {25},
  abstract         = {Network theory helps to resolve allocation problems and issues of systematic risk propagation in hedge fund networks because it allows for the hedge funds to be shown as interacting entities. Importance scores and cluster analysis support the understanding of risk propagation and causality, and capture the time-varying interconnectedness among hedge fund strategies. Furthermore, considering cluster affiliation, network metrics derived from importance scores help separate active management from active risk monitoring and build diversified portfolios. Hedge fund indexes with large centrality scores are less meaningful as risk indicators because the importance scores are time-varying. Hedge fund indexes with low centrality scores are weakly connected, but their diversification and return enhancement advantages are time-varying. Correlation networks use asset prices, are the most widely used graphs, and are easy to implement. However, the difference between directed and correlation networks is one of the most important factors because there is a direction in risk flow.},
  creationdate     = {2023-06-24T19:36:55},
  modificationdate = {2023-06-24T19:36:55},
  publisher        = {Pageant Media {US}},
  timestamp        = {2023-01-04},
}

@Article{Konstantinov-2021,
  author           = {Gueorgui S. Konstantinov},
  date             = {2021-04},
  journaltitle     = {The Journal of Portfolio Management},
  title            = {What Portfolio in Europe Makes Sense?},
  doi              = {10.3905/jpm.2021.1.243},
  number           = {7},
  pages            = {79-94},
  volume           = {47},
  abstract         = {This article focuses on European bond and equity portfolios, and specifically the relevance of balanced portfolios. European equity and bond markets have undergone tremendous changes since the launch of the European Monetary Union (EMU) in 2000. Describing the risk-return profile of these markets provides valuable information about past structure and historical returns and shifts the focus onto future expectations for institutional portfolios. The great volatility and low returns involved in European equity markets are a possible explanation for why investors prefer bonds that show Sharpe ratios higher than one. Taking into account five-year realized inflation, as well as strong bond market performance in the past 20 years with yields in the negative territory, the evidence indicates that the window for expected returns on balanced portfolios with large bond exposure is firmly shut. There are two challenging issues for European portfolio allocation. The first reflects the future of bond exposure in a balanced mandate. The second involves meaningful allocation to equity markets in a balanced portfolio.},
  creationdate     = {2023-06-24T19:36:55},
  modificationdate = {2023-06-24T19:36:55},
  publisher        = {Pageant Media {US}},
}

@Article{Konstantinov-Utkin-2021a,
  author           = {Andrei V. Konstantinov and Lev V. Utkin},
  date             = {2021-06},
  journaltitle     = {Knowledge-Based Systems},
  title            = {Interpretable machine learning with an ensemble of gradient boosting machines},
  doi              = {10.1016/j.knosys.2021.106993},
  pages            = {106993},
  volume           = {222},
  abstract         = {A method for the local and global interpretation of a black-box model on the basis of the well-known generalized additive models is proposed. It can be viewed as an extension or a modification of the algorithm using the neural additive model. The method is based on using an ensemble of gradient boosting machines (GBMs) such that each GBM is learned on a single feature and produces a shape function of the feature. The ensemble is composed as a weighted sum of separate GBMs resulting a weighted sum of shape functions which form the generalized additive model. GBMs are built in parallel using randomized decision trees of depth 1, which provide a very simple architecture. Weights of GBMs as well as features are computed in each iteration of boosting by using the Lasso method and then updated by means of a specific smoothing procedure. In contrast to the neural additive model, the method provides weights of features in the explicit form, and it is simply trained. A lot of numerical experiments with an algorithm implementing the proposed method on synthetic and real datasets demonstrate its efficiency and properties for local and global interpretation.},
  creationdate     = {2023-06-24T19:36:56},
  modificationdate = {2023-06-24T19:36:56},
  publisher        = {Elsevier {BV}},
}

@Article{Konstantinov-Utkin-2021,
  author           = {Andrei V. Konstantinov and Lev V. Utkin},
  date             = {2021-08-10},
  journaltitle     = {arXiv e-Print},
  title            = {Attention-like feature explanation for tabular data},
  eprint           = {2108.04855},
  eprintclass      = {cs.LG},
  eprinttype       = {arXiv},
  abstract         = {A new method for local and global explanation of the machine learning black-box model predictions by tabular data is proposed. It is implemented as a system called AFEX (Attention-like Feature EXplanation) and consisting of two main parts. The first part is a set of the one-feature neural subnetworks which aim to get a specific representation for every feature in the form of a basis of shape functions. The subnetworks use shortcut connections with trainable parameters to improve the network performance. The second part of AFEX produces shape functions of features as the weighted sum of the basis shape functions where weights are computed by using an attention-like mechanism. AFEX identifies pairwise interactions between features based on pairwise multiplications of shape functions corresponding to different features. A modification of AFEX with incorporating an additional surrogate model which approximates the black-box model is proposed. AFEX is trained end-to-end on a whole dataset only once such that it does not require to train neural networks again in the explanation stage. Numerical experiments with synthetic and real data illustrate AFEX.},
  creationdate     = {2023-06-24T19:36:56},
  file             = {:http\://arxiv.org/pdf/2108.04855v1:PDF},
  keywords         = {cs.LG, cs.AI},
  modificationdate = {2023-06-24T19:36:56},
}

@Article{Konstantinov-Fabozzi-2020,
  author           = {Gueorgui S. Konstantinov and Frank J. Fabozzi},
  date             = {2020},
  journaltitle     = {The Journal of Fixed Income},
  title            = {Carry Strategies and the {US} Dollar Risk of {US} and Global Bonds},
  doi              = {10.3905/jfds.2020.1.031},
  number           = {2},
  pages            = {64-84},
  url              = {https://jfds.pm-research.com/content/2/2/64},
  volume           = {2},
  abstract         = {In this article, the authors investigate the ability of currency carry strategies to mitigate the US-dollar risk of US and global bond benchmark indices. Using the Fama-French and currency factors, the authors analyze the bond benchmarks and find that currency carry strategies can easily be modified to suit bond investors, portfolio managers, and allocators. A central finding of the authors is that currency carry strategies help to mitigate the currency risk of the US dollar. Specifically, broad indices (including credit and government exposure) deserve modified carry strategies to mitigate the impact on the US dollar The results show that the exposure to the equity-based profitability factor, one of the factors in the Fama-French five-factor model, is to some extent a natural hedge against the US-dollar risk of high-yield and corporate indices. Moreover, combined currency carry overlay strategies and fixed-income investments generate Probabilistic Sharpe ratios above a certain threshold level. The results suggest that investors should consider modified currency carry strategies rather than simple carry rules when managing both US and global-high yield, corporate, and government debt. The authors also provide a framework that can be used to diversify the impact of individual equity-based factors and the currency style factors on fixed-income benchmarks.},
  creationdate     = {2023-06-24T19:36:56},
  journal          = {The Journal of Fixed Income},
  modificationdate = {2023-06-24T19:36:56},
  month            = {jul},
  publisher        = {Pageant Media {US}},
  timestamp        = {2020-08-31 23:12},
  year             = {2020},
}

@Article{Konstantinov-2017,
  author               = {Konstantinov, Gueorgui},
  date                 = {2017-01-31},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Currency Crowdedness Generated by Global Bond Funds},
  doi                  = {10.3905/jpm.2017.43.2.123},
  issn                 = {0095-4918},
  number               = {2},
  pages                = {123--135},
  volume               = {43},
  abstract             = {The focus of this article is the management style of global bond funds and their exposure to currencies. The author finds strong evidence that, in the short-term, global bond funds rely heavily on beta strategies and thus generate crowded trades in currency investing. As a result, global fixed-income funds add to the previously detected crowdedness in pure currency investing. There is empirical indication that the currency beta exposure has increased over the years.},
  citeulike-article-id = {14504910},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2017.43.2.123},
  creationdate         = {2023-06-24T19:36:56},
  day                  = {31},
  groups               = {Invest_Crowding},
  modificationdate     = {2023-06-24T19:36:56},
  posted-at            = {2017-12-18 21:35:59},
  timestamp            = {2019-04-19 19:52},
}

@Article{Konstantinov-2017a,
  author           = {Konstantinov, Gueorgui},
  date             = {2017-05-31},
  journaltitle     = {The Journal of Investing},
  title            = {On the Dynamics of EMU Bond Portfolios: Is the Diversification of Risk Factors Driving to Convergence of Fund Exposure?},
  abstract         = {We show that the persistent style of European Monetary Union (EMU) bond portfolios generates significant crowdedness in common factors level and steepness. Despite fund categorization, our results suggest that bond portfolios show low risk-factor diversification. However, we found less-crowded trades that deserve investors attention and could improve diversification. In line with previous research, we argue that a transitory shift from the current levels of crowdedness and risk factors in the EMU is imminent and almost inevitable. Finally, we propose a framework for analysis that ide.tifies crowdedness, helps monitoring of the exposure at risk, and suggests investment process enhancements, which could improve investors diversification.},
  creationdate     = {2023-06-24T19:36:56},
  day              = {31},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T19:36:56},
  publisher        = {Institutional Investor Journals Umbrella},
  timestamp        = {2019-08-30 12:10},
}

@Article{Konstantinov-Rebmann-2019,
  author           = {Gueorgui Konstantinov and Jonas Rebmann},
  date             = {2019-08-01},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {From risk factors to networks: A case study on interconnectedness using currency funds},
  doi              = {10.3905/jfds.2019.1.3.108},
  issn             = {2640-3951},
  url              = {https://jfds.pm-research.com/content/1/3/108},
  urldate          = {2019-09-06},
  abstract         = {In this article, the authors introduce a combined approach for investigating currency funds by using methods from network science and risk factor analysis. They document a positive relationship between currency funds style exposure, fund age, size, and connectedness, providing both economically and statistically significant results. The most important funds in the network can influence the currency market with significant exposure to the risk factors carry, value, and trend. In general, the authors approach helps investors to identify market interconnectedness; shows how risk can be transmitted; and highlights the factors that could represent significant idiosyncratic, systematic, and systemic economic risk. The authors argue that the interconnectedness is asymmetrical and the network is reciprocal. There are funds with significant importance scores. They provide a framework for practical implementation.},
  creationdate     = {2023-06-24T19:36:56},
  day              = {1},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T19:36:56},
  timestamp        = {2019-09-08 02:10},
}

@Article{Konstantinov-et-al-2020,
  author           = {Konstantinov, Gueorgui and Chorus, Andreas and Rebmann, Jonas},
  date             = {2020-03-06},
  journaltitle     = {The Journal of Portfolio Management},
  title            = {A network and machine learning approach to factor, asset, and blended allocation},
  doi              = {10.3905/jpm.2020.1.147},
  issue            = {6},
  pages            = {54-71},
  url              = {https://jpm.pm-research.com/content/46/6/54},
  urldate          = {2020-03-07},
  volume           = {46},
  abstract         = {The main idea of this article is to approach and compare factor and asset allocation portfolios using both traditional and alternative allocation techniques: inverse variance optimization, minimum-variance optimization, and centrality-based techniques from network science. Analysis of the interconnectedness between assets and factors shows that their relationship is strong. The authors compare the allocation techniques, considering centrality and hierarchal-based networks. They demonstrate the advantages of graph theory to explain the advantages to portfolio management and the dynamic nature of assets and factors with their importance score. They find that asset allocation can be efficiently derived using directed networks, dynamically driven by both US Treasuries and currency returns with significant centrality scores. Alternatively, the inverse variance weight estimation and correlation-based networks generate factor allocation with favorable risk-return parameters. Furthermore, factor allocation is driven mostly by the importance scores of the Fama-French-Carhart factors: SMB, HML, CMA, RMW, and MOM. The authors confirm previous results and argue that both factors and assets are interconnected with different value and momentum factors. Therefore, a blended strategy comprising factors and assets can be defensible for investors. As argued in previous research, factors are much more overcrowded than assets. Therefore, the centrality scores help to identify the crowded exposure and build diversified allocation. The authors run LASSO regressions and show how the network-based allocation can be implemented using machine learning.},
  creationdate     = {2023-06-24T19:36:56},
  day              = {6},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T19:36:56},
  timestamp        = {2020-06-30 10:24},
}

@Article{Konstantinov-Rusev-2020,
  author           = {Konstantinov, Gueorgui and Rusev, Mario},
  date             = {2020-01-31},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {The Bond-Equity-Fund Relation Using the Fama-French-Carhart Factors: A Practical Network Approach},
  issue            = {1},
  pages            = {24-44},
  url              = {https://jfds.pm-research.com/content/2/1/24},
  urldate          = {2020-03-07},
  volume           = {2},
  abstract         = {The main goal of this article is to show the relation between global equity and bond funds from a network perspective. The authors demonstrate the advantages of graph theory to explain the collective fund dynamics. The results show that equity and bond funds have a significant exposure to the Fama-French-Carhart factors. The authors argue that the network is dynamically driven by equity funds with their centrality scores and risk factor exposure and can transmit and amplify system-wide stress or inefficiencies in the factor bets. Using graph theory, the authors demonstrate that the return-based relationships between bond and equity funds are asymmetrical and the network is sufficiently clustered. Specifically, equity funds connect the different clusters. The HML factor is significant both on a single-fund level and as a web determinant. Therefore, investors should pay close attention to it when managing funds and deriving asset allocations. Finally, the authors provide a machine learning approach to how fund managers, plan sponsors, and analysts can derive equity-bond allocations, based on centrality scores, factor exposure, and hierarchical clustering of asymmetrically connected assets.},
  creationdate     = {2023-06-24T19:36:56},
  day              = {31},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T19:36:56},
  publisher        = {Institutional Investor Journals Umbrella},
  timestamp        = {2020-06-30 10:25},
}

@Article{Loistl-Konstantinov-2020,
  author           = {Loistl, Otto and Konstantinov, Gueorgui S.},
  date             = {2020-03-06},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {Interactions and Interconnectedness Shape Financial Market Research},
  eid              = {2},
  number           = {2},
  pages            = {51-63},
  url              = {https://jfds.pm-research.com/content/early/2020/03/05/jfds.2020.1.026},
  urldate          = {2020-03-07},
  abstract         = {In this article the authors investigate two fields that might be relevant for financial data sciences. The first issue covers the entire production chain from orders to prices  by realistically modeling stock exchange microstructure (e.g., NASDAQ and Xetra). Specifically, the authors show how data-driven research can model decisions to place orders and to generate prices by matching orders accordingly. The other issue is price interconnectedness at markets by networks. The authors show that interactions shape a market  performance. Emergence comprises the interactions at markets; as such, the collective may not be equal to the sum of individual activities. As a consequence, the assumption that markets are in equilibrium and that arbitrage opportunities do not exist can be replaced by more realistic working hypotheses. The authors show with the two examples that market participants interact, learn, and trade. These individual interactions can be described as organized complexity. Whereas calculus may not support explicit modeling of interactions, the age of big data permits their modeling and application of innovative concepts, such as network solutions for asset allocation, which can be modeled using machine learning. This article illustrates that assertion with concrete examples.},
  creationdate     = {2023-06-24T19:36:57},
  day              = {6},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T19:36:57},
  timestamp        = {2020-08-30 04:47},
}

@Article{Konstantinov-Simonian-2020,
  author           = {Konstantinov, Gueorgui S. and Simonian, Joseph},
  date             = {2020-06-23},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {A Network Approach to Analyzing Hedge Fund Connectivity},
  issue            = {3},
  number           = {3},
  pages            = {55-72},
  url              = {https://jfds.pm-research.com/content/2/3/55},
  urldate          = {2020-06-23},
  volume           = {22},
  abstract         = {In this article, the authors investigate the hedge fund market as a network of interacting individual funds. The authors identify and analyze the most important hedge fund styles that could both affect the market and transmit systemwide shocks to other funds, individual asset classes, and beyond. The authors find that the most connected hedge fund database categories are global macro and equity long-short funds. A central result of the article is a classification of funds using clustering, in which seemingly different funds are shown to cluster based on their shared factor exposures. This finding demonstrates that investors should consider fund connectivity and their attendant importance scores rather than database classifications when measuring hedge fund risk across the business cycle. The authors also provide a forecasting framework that can be used to predict hedge fund network behavior and the impact of individual factors on the network.},
  creationdate     = {2023-06-24T19:36:57},
  day              = {23},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T19:36:57},
  publisher        = {Institutional Investor Journals Umbrella},
  timestamp        = {2020-08-05 09:42},
}

@Article{Konstantinov-Rebmann-2020,
  author           = {Gueorgui Konstantinov and Jonas Rebmann},
  date             = {2020},
  journaltitle     = {The Journal of Alternative Investments},
  title            = {Different in Nature, Common in Style: View Commonality of Single Hedge Funds and Funds of Hedge Funds},
  doi              = {10.3905/jai.2020.1.110},
  number           = {2},
  pages            = {49-66},
  url              = {https://jai.pm-research.com/content/early/2020/08/13/jai.2020.1.110},
  volume           = {23},
  abstract         = {Using style analysis, the authors show the common factor exposures of single hedge funds and funds of hedge funds (FoHFs). Despite the different nature and characteristics of the two categories, there are significant style similarities through the different business cycles. The authors argue that in general, FoHFs use substantially more leverage than single hedge fund strategies and found a positive trade-off between leverage and style exposure for the former and a negative one for the latter. Furthermore, the style in both category changes according to the business cycle, with single hedge fund strategies having more flexibility in allocation and aggressive style drifts. However, the similarities in style exposure increase in times of financial turmoil. Additionally, the exposure in equity and bond-related factors are significant and identify style drifts from the past. Finally, the authors found that both categories engage most in equity markets related factors--Betting Against Beta factors (BAB), High minus Low (HML) portfolios sorted by book-to-market to indicate value, Small minus Big (SMB) portfolios sorted by market-cap to indicate size, and Conservative minus Aggressive (CMA) portfolios.},
  creationdate     = {2023-06-24T19:36:57},
  modificationdate = {2023-06-24T19:36:57},
  timestamp        = {2020-08-31 23:13},
}

@Article{Koumou-2020,
  author           = {Gilles Boevi Koumou},
  date             = {2020},
  journaltitle     = {Financial Markets and Portfolio Management},
  title            = {Diversification and portfolio theory: a review},
  doi              = {10.1007/s11408-020-00352-6},
  pages            = {267-312},
  volume           = {34},
  abstract         = {Diversification is one of the major components of investment decision-making under risk or uncertainty. However, paradoxically, as the 2007-2009 financial crisis revealed, the concept remains misunderstood. Our goal in writing this paper is to correct this issue by reviewing the concept in portfolio theory. The core of our review focuses on the following diversification principles: law of large numbers, correlation, capital asset pricing model and risk contribution or risk parity diversification principles. These four diversification principles are the DNA of the existing portfolio selection rules and asset pricing theories and are instrumental to the understanding of diversification in portfolio theory. We review their definition. We also review their optimality, with respect to expected utility theory, and their usefulness. Finally, we explore their measurement.},
  creationdate     = {2023-06-24T20:47:01},
  modificationdate = {2023-06-24T20:47:01},
  timestamp        = {2020-07-25 11:33},
}

@MastersThesis{Kurtti-2020,
  author           = {Kurtti, Markku},
  date             = {2020},
  institution      = {University of Oulu},
  title            = {How many stocks make a diversified portfolio in a continuous-time world?},
  url              = {http://jultika.oulu.fi/Record/nbnfioulu-202011203162},
  abstract         = {This thesis aims to answer how many stocks make a diversified portfolio in a continuous-time world. The study investigates what are the factors determining diversification effects in a real, continuous-time, world as opposed to thoroughly studied theoretical single period world. Continuous-time world investors care about geometric, instead of arithmetic, rate of return.

\leavevmode\newline 

We show how methodology based on information theory can be utilized in investing context. Geometric risk premium is explained by the Shannon limit and its derivative, fractional Kelly criterion. Investing world counterpart for the Shannon limit, compounding process capacity, is derived. Geometric risk premium is decomposed to single stock risk premium and diversification premium. Method for estimating diversification premium is provided. Concept of realizable risk premium is derived and used in risk averse investor diversification metrics. Diversification effect is measured as a (realizable) risk premium ratio and as a (realizable) gross compound excess wealth ratio. Both ratios are between a randomly selected portfolio of selected size and fully diversified benchmark.

\leavevmode\newline 

We show, both analytically and empirically, that diversification in a continuous-time world is a negative price lunch as opposed to free lunch in a single period world. Investor is paid a diversification premium, implying higher geometric risk premium, for consuming a lunch. The magnitude of diversification premium difference to benchmark, the opportunity cost of foregone diversification, is shown to be equal to one half of portfolio's idiosyncratic variance scaled by squared investment fraction. To maintain a constant wealth ratio, required level of diversification for a long-term risk neutral investor is approximately directly proportional to investment time horizon length.

\leavevmode\newline 

The factors determining required level of diversification in a continuous-time world are number of stocks in the benchmark, Sharpe ratio and variance of the benchmark, idiosyncratic variance of an average stock, investment fraction and time. At investment fraction 1.0, risk averse investor requires more than 100, 200 or 1000 stocks to achieve 90\%, 95\% or 99\% of the maximum diversification benefit, respectively. For short-term risk neutral investor, the corresponding numbers are about 20, 40 or 200 stocks and yet significantly more for long-term risk neutral investor. The numbers increase and decrease as investment fraction increase and decrease, respectively. We find that small firms require substantially more diversification compared to large firms and that there are substantial and consistent differences in diversification premiums between investing styles.},
  creationdate     = {2023-06-24T20:48:26},
  modificationdate = {2023-06-24T20:48:26},
}

@Article{Laur-2020,
  author           = {Bhanu Laur},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Portfolio Optimization - Can Optimizing Portfolio Outperform Naive Diversification?},
  doi              = {10.2139/ssrn.3524277},
  url              = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3524277},
  abstract         = {In this study we examined the performances of mean-variance and tangency portfolio investment strategies in order to determine if optimal diversification has benefits over 1/N strategy.},
  creationdate     = {2023-06-24T20:49:06},
  modificationdate = {2023-06-24T20:49:06},
  timestamp        = {2020-08-11 00:20},
}

@Article{Lim-Ong-2021,
  author           = {Tristan Lim and Chin Sin Ong},
  date             = {2021},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {Portfolio Diversification Using Shape-Based Clustering},
  doi              = {10.3905/jfds.2020.1.054},
  number           = {1},
  pages            = {111-126},
  volume           = {3},
  abstract         = {Portfolio diversification involves lowering the correlation between portfolio assets to achieve improved risk-return exposure. It is reasonable to infer from the classic Anscombe quartet that relying on descriptive statistics, and specifically, correlation, to achieve portfolio diversification may not derive the most optimal multiperiod portfolio risk-adjusted return because stocks in a portfolio can exhibit different price trends over time, even with the same computed pairwise correlation. This research applied a shape-based time-series clustering technique of agglomerative hierarchical clustering using dynamic time-series warping as a distance measure to aggregate stocks into like-trending clusters across time as a portfolio diversification tool. Results support the use of the shape-based clustering technique for (1) portfolio allocation and rebalancing, (2) dynamic predictive portfolio construction, and (3) individual stock selection through outlier identification. The findings will be a useful addition to the existing literature in portfolio management by providing shape-based clustering as an alternative tool for portfolio construction and security selection.},
  creationdate     = {2023-06-24T20:49:54},
  modificationdate = {2023-06-24T20:49:54},
  publisher        = {Pageant Media {US}},
  timestamp        = {2021-01-05 11:51},
}

@Article{Lohre-et-al-2021,
  author           = {Lohre, Harald and Hixon, Robert S and Raol, Jay H and Swade, Alexander and Tao, Hua and Wolle, Scott},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Investing Through a Macro Factor Lens},
  url              = {https://ssrn.com/abstract=3738545},
  abstract         = {A macro factor perspective can help guide portfolio allocation by focusing on salient macroeconomic factors like growth or inflation. We study the link between such macro factors and common multi-asset multi-factor investment building blocks. Specifically, we investigate their macro factor sensitivities and propose a simple, yet effective, route to designing diversified macro factor-mimicking portfolios that prove beneficial in diversifying a given portfolio allocation with respect to its macro factor exposures.},
  creationdate     = {2023-06-24T20:50:39},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:50:39},
  timestamp        = {2020-08-11 00:36},
}

@InCollection{Lohre-et-al-2020,
  author           = {Harald Lohre and Carsten Rother and Kilian Axel Schafer},
  booktitle        = {Machine Learning for Asset Management: New Developments and Financial Applications},
  date             = {2020},
  title            = {Hierarchical Risk Parity: Accounting for Tail Dependencies in Multi-asset Multi-factor Allocations},
  doi              = {10.1002/9781119751182.ch9},
  editor           = {Emmanuel Jurczenko},
  pages            = {329--368},
  publisher        = {Wiley},
  abstract         = {This chapter examines the use and merits of hierarchical clustering techniques in the context of multi-asset multi-factor investing. In particular, it contrasts these techniques with several competing risk-based allocation paradigms, such as 1/N, minimum-variance, standard risk parity and diversified risk parity. The chapter introduces hierarchical risk parity (HRP) strategies based on the Pearson correlation coefficient and also introduces hierarchical clustering based on the lower tail dependence coefficient. The chapter provides an overview of traditional risk-based allocation strategies and outlines a framework to measure and manage portfolio diversification. It examines the performance of the introduced HRP strategies relative to the traditional alternatives. The chapter discusses Meucci's approach to managing diversification, which serves to construct a diversified risk parity strategy based on economic factors.},
  creationdate     = {2023-06-24T20:50:39},
  modificationdate = {2023-06-24T20:50:39},
  timestamp        = {2020-08-08 15:24},
}

@Article{dePrado-2023c,
  author           = {Marcos {L{\'{o}}pez de Prado}},
  date             = {2023},
  journaltitle     = {{SSRN} Electronic Journal},
  title            = {The Hierarchy of Empirical Evidence in Finance},
  doi              = {10.2139/ssrn.4425855},
  abstract         = {Recent progress in causal inference has opened a path, however difficult, for advancing financial economics beyond its current phenomenological stage. The goal of this article is to propose a hierarchy of empirical evidence, recognizing that not all types of observations have the same scientific weight, in the sense of enabling the falsification of causal claims.},
  creationdate     = {2023-06-24T20:51:58},
  modificationdate = {2023-06-24T20:51:58},
  publisher        = {Elsevier {BV}},
}

@Article{dePrado-2023a,
  author           = {Marcos {L{\'{o}}pez de Prado}},
  date             = {2023},
  journaltitle     = {SSRN e-Print},
  title            = {Pseudo-Factors and Factor Investing},
  doi              = {10.2139/ssrn.4336002},
  abstract         = {In this article, I advocate for the use of causal graphs to modernize the field of factor investing, and set it on a logically-coherent foundation. In order to do that, first I must introduce the concepts of association and causations. Second, I explain the use of causal graphs and the real (causal) meaning of the "ceteris paribus" assumption that is so popular among economists. Third, I explain how causal graphs help us estimate causal effects in observational (non-experimental) studies. Fourth, I illustrate all of the earlier concepts with Monte Carlo experiments. Fifth, I conclude that the field of factor investing must embrace causal graphs in order to wake up from its associational slumber.},
  creationdate     = {2023-06-24T20:51:58},
  modificationdate = {2023-06-24T20:51:58},
  publisher        = {Elsevier {BV}},
}

@Article{dePrado-2023,
  author           = {Marcos {L{\'{o}}pez de Prado}},
  date             = {2023-02},
  journaltitle     = {The Journal of Portfolio Management},
  title            = {Where Are the Factors in Factor Investing?},
  doi              = {10.3905/jpm.2023.1.477},
  number           = {5},
  pages            = {6-20},
  volume           = {49},
  abstract         = {In this article, the author advocates for the use of causal graphs to modernize the field of factor investing and set it on a logically coherent foundation. To do this, first he introduces the concepts of association and causation. Second, he explains the use of causal graphs and the real (causal) meaning of the ceteris paribus assumption that is so popular among economists. Third, he explains how causal graphs help us estimate causal effects in observational (nonexperimental) studies. Fourth, he illustrates all of the earlier concepts with Monte Carlo experiments. He concludes that the field of factor investing must embrace causal graphs in order to wake up from its associational slumber.},
  creationdate     = {2023-06-24T20:51:58},
  modificationdate = {2023-06-24T20:51:58},
  publisher        = {Pageant Media {US}},
}

@Article{dePrado-2023b,
  author           = {Marcos {L{\'{o}}pez de Prado}},
  date             = {2023},
  journaltitle     = {SSRN e-Print},
  title            = {Can Factor Investing Become Scientific? (Seminar Slides)},
  doi              = {10.2139/ssrn.4324450},
  abstract         = {Virtually all journal articles in the factor investing literature make associational claims, instead of causal claims. Authors do not identify the causal graph consistent with the observed phenomenon, they justify their chosen model specification in terms of correlations, and they do not propose experiments for falsifying causal mechanisms. Absent a causal theory, their findings are likely false, due to rampant backtest overfitting and incorrect specification choices.

I differentiate between type-A and type-B spurious claims, and explain how both types prevent factor investing from advancing beyond its current pre-scientific stage. This seminar analyzes the current state of causal confusion in the factor investing literature, and proposes solutions with the potential to transform factor investing into a truly scientific discipline.

The full manuscript can be found here: http://ssrn.com/abstract=4205613.},
  creationdate     = {2023-06-24T20:51:58},
  modificationdate = {2023-06-24T20:51:58},
  publisher        = {Elsevier {BV}},
}

@Article{dePrado-2022a,
  author           = {Marcos {L{\'{o}}pez de Prado}},
  date             = {2022-07},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {Machine Learning for Econometricians: The Readme Manual},
  doi              = {10.3905/jfds.2022.1.101},
  number           = {3},
  pages            = {10-30},
  volume           = {4},
  abstract         = {One of the most exciting recent developments in financial research is the availability of new administrative, private sector, and micro-level datasets that did not exist a few years ago. The unstructured nature of many of these observations, along with the complexity of the phenomena they measure, means that many of these datasets are beyond the grasp of econometric analysis. Machine learning (ML) techniques offer the numerical power and functional flexibility needed to identify complex patterns in a high-dimensional space. ML is often perceived as a black box, however, in contrast to the transparency of econometric approaches. In this article, the author demonstrates that each analytical step of the econometric process has a homologous step in ML analyses. By clearly stating this correspondence, the author's goal is to facilitate and reconcile the adoption of ML techniques among econometricians.},
  creationdate     = {2023-06-24T20:51:58},
  modificationdate = {2023-06-24T20:51:58},
  publisher        = {Pageant Media {US}},
}

@Article{dePrado-2022b,
  author           = {{L{\'{o}}pez de Prado}, Marcos},
  date             = {2022-07},
  journaltitle     = {The Journal of Portfolio Management},
  title            = {Type I and Type {II} Errors of the Sharpe Ratio under Multiple Testing},
  doi              = {10.3905/jpm.2022.1.403},
  number           = {1},
  pages            = {39-46},
  volume           = {49},
  abstract         = {Articles in financial literature typically estimate the p-value associated with an investment strategy's performance without reporting the power of the test used to make that discovery. In this article, the author provides analytic estimates to Type I and Type II errors for the Sharpe ratios of investments and derives their familywise counterparts. These estimates allow researchers to carefully design experiments and select investments with high confidence and power.},
  creationdate     = {2023-06-24T20:51:58},
  modificationdate = {2023-06-24T20:51:58},
  publisher        = {Pageant Media {US}},
}

@Article{dePrado-2022,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2022},
  journaltitle     = {SSRN e-Print},
  title            = {Escaping The Sisyphean Trap: How Quants Can Achieve Their Full Potential},
  doi              = {10.2139/ssrn.3916692},
  abstract         = {Investing can be characterized as a data science problem. While investment firms have attracted scientific talent, they have done a poor job at developing it. Firms hire specialists, but entice them to become generalists (e.g., portfolio managers). Under the ubiquitous silo/platform structure, quants succumb to the Sisyphean trap, and do not achieve their full potential.

A research lab structure offers a unique environment for developing scientists, by means of: (a) co-specialization, working in a highly cooperative lab environment; (b) tackling well-defined open investment problems; and (c) applying the scientific method.},
  creationdate     = {2023-06-24T20:51:58},
  modificationdate = {2023-06-24T20:51:58},
  publisher        = {Elsevier {BV}},
}

@Article{dePrado-Lipton-2021,
  author           = {Marcos {L{\'{o}}pez de Prado} and Alex Lipton},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Three Quant Lessons from {COVID}-19},
  doi              = {10.2139/ssrn.3562025},
  abstract         = {Many quantitative firms have suffered substantial losses as a result of the COVID-19 selloff. In this note we highlight three lessons that quantitative researchers could learn.},
  creationdate     = {2023-06-24T20:51:58},
  modificationdate = {2023-06-24T20:51:58},
  publisher        = {Elsevier {BV}},
}

@Article{dePrado-2020g,
  author           = {Marcos {L{\'{o}}pez de Prado}},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Advances in Financial Machine Learning: NumeraI's Tournament (Presentation Slides)},
  doi              = {10.2139/ssrn.3478927},
  abstract         = {Machine learning (ML) is changing virtually every aspect of our lives. Today ML algorithms accomplish tasks that until recently only expert humans could perform. As it relates to finance, this is the most exciting time to adopt a disruptive technology that will transform how everyone invests for generations. In this course, we discuss scientifically sound ML tools that have been successfully applied to the management of large pools of funds.},
  creationdate     = {2023-06-24T20:51:58},
  modificationdate = {2023-06-24T20:51:58},
  publisher        = {Elsevier {BV}},
}

@Article{dePrado-2019g,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2019-11-09},
  journaltitle     = {SSRN e-Print},
  title            = {Estimation of Theory-Implied Correlation Matrices},
  url              = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3484152},
  urldate          = {2019-11-14},
  abstract         = {Correlation matrices are ubiquitous in finance. Some key applications include portfolio construction, risk management, and factor/style analysis. Correlation matrices are usually estimated from historical empirical observations or derived from historically estimated factors. It is widely acknowledged that empirical correlation matrices: (a) have poor numerical properties that lead to unreliable estimators; and (b) have poor predictive power. Additionally, factor-based correlation matrices have their own caveats. In particular, estimated factors are typically non-hierarchical and do not allow for interactions at different levels. This contravenes the fact that financial instruments typically exhibit a nested cluster structure (e.g., MSCI GICS levels 1-4).This paper introduces a machine learning (ML) algorithm to estimate forward-looking correlation matrices implied by economic theory. Given a particular theoretical representation of the hierarchical structure that governs a universe of securities, the method fits the correlation matrix that complies with that theoretical representation of the future. This particular use case demonstrates how, contrary to popular perception, ML solutions are not black-boxes, and can be applied effectively to develop and test economic theories.},
  creationdate     = {2023-06-24T20:51:59},
  day              = {9},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:51:59},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2021a,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Detection of False Investment Strategies through {FWER} and {FDR} (Seminar Slides)},
  doi              = {10.2139/ssrn.3799803},
  abstract         = {Financial systems rarely allow experimentation. For example, we cannot reproduce the flash crash of 2010 while controlling for environmental conditions. As a result, much financial research relies on the statistical analysis of finite (historical) datasets, where: (a) Time series datasets are limited, and (b) The investment universe is limited.

The implication is that a large number of hypotheses are tested on the same observations. In the context of asset management, this situation leads to false investment strategies and losses, particularly among quantitative funds.

This seminar explains how to detect false investment strategies by controlling for the familywise error rate (FWER) and the false discovery rate (FDR) of an organization. It is part of Cornell University's ORIE 5256 course.},
  creationdate     = {2023-06-24T20:51:59},
  modificationdate = {2023-06-24T20:51:59},
  publisher        = {Elsevier {BV}},
}

@Article{dePrado-Lewis-2019,
  author           = {{Lopez de Prado}, Marcos and Lewis, Michael J.},
  date             = {2019},
  journaltitle     = {Quantitative Finance},
  title            = {Detection of false investment strategies using unsupervised learning methods},
  doi              = {10.1080/14697688.2019.1622311},
  issn             = {1556-5068},
  number           = {9},
  pages            = {1555-1565},
  volume           = {19},
  abstract         = {In this paper we address the problem of selection bias under multiple testing in the context of investment strategies. We introduce an unsupervised learning algorithm that determines the number of effectively uncorrelated trials carried out in the context of a discovery. This estimate is critical for computing the familywise false positive probability, and for filtering out false investment strategies.},
  creationdate     = {2023-06-24T20:51:59},
  f1000-projects   = {QuantInvest},
  groups           = {ML_Test_FalsePosNeg},
  modificationdate = {2023-06-24T20:51:59},
  timestamp        = {2020-09-20 13:17},
}

@Article{dePrado-2013,
  author               = {{Lopez de Prado}, Marcos},
  date                 = {2013-04},
  journaltitle         = {SSRN e-Print},
  title                = {How Long Does It Take to Recover from a Drawdown?},
  url                  = {https://ssrn.com/abstract=2254668},
  abstract             = {Investment management firms routinely hire and fire employees based on the performance of their portfolios. Such performance is evaluated through popular metrics that assume IID Normal returns, like Sharpe ratio, Sortino ratio, Treynor ratio, Information ratio, etc.

Investment returns are far from IID Normal.

Conclusion 1: Firms evaluating performance through Sharpe ratio are firing up to three times more skillful managers than originally targeted. This is very costly to firms and investors, and is a direct consequence of wrongly assuming that returns are IID Normal.

Conclusion 2: An accurate performance evaluation methodology is worth a substantial portion of the fees paid to hedge funds. There is a 20 percent loss of the drawdown for every false positive. For a large firm, this amounts to tens of millions of dollars lost annually, as a result of wrongly assuming that returns are IID Normal.},
  citeulike-article-id = {13780321},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2254668},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2460364code434076.pdf?abstractid=2254668 and mirid=1},
  creationdate         = {2023-06-24T20:51:59},
  day                  = {22},
  howpublished         = {Available at SSRN: ssrn.com/abstract=2254668},
  keywords             = {portfolio, portfolio-allocation, sharpe-ratio},
  modificationdate     = {2023-06-24T20:51:59},
  owner                = {zkgst0c},
  posted-at            = {2016-01-09 17:45:42},
  timestamp            = {2020-07-23 13:38},
}

@Article{dePrado-2016a,
  author               = {{Lopez de Prado}, Marcos},
  date                 = {2015-12},
  journaltitle         = {SSRN e-Print},
  title                = {Building Diversified Portfolios that Outperform Out-of-Sample},
  url                  = {https://ssrn.com/abstract=2708678},
  abstract             = {This paper introduces the Hierarchical Risk Parity (HRP) approach. HRP portfolios address three major concerns of quadratic optimizers in general and Markowitz's CLA in particular: Instability, concentration and underperformance.HRP applies modern mathematics (graph theory and machine learning techniques) to build a diversified portfolio based on the information contained in the covariance matrix. However, unlike quadratic optimizers, HRP does not require the invertibility of the covariance matrix. In fact, HRP can compute a portfolio on an ill-degenerated or even a singular covariance matrix, an impossible feat for quadratic optimizers. Monte Carlo experiments show that HRP delivers lower out-of-sample variance than CLA, even though minimum-variance is CLA's optimization objective. HRP also produces less risky portfolios out-of-sample compared to traditional risk parity methods.A presentation can be found at http://ssrn.com/abstract=2713516.},
  citeulike-article-id = {14130573},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2708678},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2783555code434076.pdf?abstractid=2708678 and mirid=1},
  creationdate         = {2023-06-24T20:51:59},
  day                  = {28},
  modificationdate     = {2023-06-24T20:51:59},
  owner                = {zkgst0c},
  posted-at            = {2016-09-05 23:16:46},
  timestamp            = {2020-07-23 13:38},
}

@Article{dePrado-2016,
  author               = {{Lopez de Prado}, Marcos},
  date                 = {2016-07},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {Building Diversified Portfolios that Outperform Out of Sample},
  doi                  = {10.3905/jpm.2016.42.4.059},
  issn                 = {0095-4918},
  number               = {4},
  pages                = {59--69},
  volume               = {42},
  abstract             = {In this article, the author introduces the Hierarchical Risk Parity (HRP) approach to address three major concerns of quadratic optimizers, in general, and Markowitz's critical line algorithm (CLA), in particular: instability, concentration, and underperformance. HRP applies modern mathematics (graph theory and machine-learning techniques) to build a diversified portfolio based on the information contained in the covariance matrix. However, unlike quadratic optimizers, HRP does not require the invertibility of the covariance matrix. In fact, HRP can compute a portfolio on an ill-degenerated or even a singular covariance matrix an impossible feat for quadratic optimizers. Monte Carlo experiments show that HRP delivers lower out-ofsample variance than CLA, even though minimum variance is CLA's optimization objective. HRP also produces less risky portfolios out of sample compared to traditional risk parity methods.},
  citeulike-article-id = {14130574},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2016.42.4.059},
  creationdate         = {2023-06-24T20:51:59},
  groups               = {Networks and investment management, Clustering and network analysis, Machine learning and investment strategies, ML_Network_QWIM, ML_PerfMetrics, Invest_Diversif},
  modificationdate     = {2023-06-24T20:51:59},
  owner                = {zkgst0c},
  posted-at            = {2016-09-05 23:17:29},
  timestamp            = {2019-10-11 18:45},
}

@Article{dePrado-2016b,
  author               = {{Lopez de Prado}, Marcos},
  date                 = {2016-08},
  journaltitle         = {SSRN e-Print},
  title                = {Mathematics and Economics: A Reality Check},
  url                  = {https://ssrn.com/abstract=2819847},
  abstract             = {Economics (and by extension finance) is arguably one of the most mathematical fields of research. However, economists' choice of math may be inadequate to model the complexity of social institutions.In a constructive spirit, this note offers some advice on how students could increase their chances of having a successful career in 21st century finance. Practitioners seeking to enhance their skillset may also draw some ideas.},
  citeulike-article-id = {14119852},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2819847},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2826891code434076.pdf?abstractid=2819847 and mirid=1},
  creationdate         = {2023-06-24T20:51:59},
  day                  = {13},
  modificationdate     = {2023-06-24T20:51:59},
  owner                = {zkgst0c},
  posted-at            = {2016-08-22 12:17:26},
  timestamp            = {2020-07-23 13:38},
}

@Article{dePrado-2017,
  author               = {{Lopez de Prado}, Marcos},
  date                 = {2017-09},
  journaltitle         = {SSRN e-Print},
  title                = {The 7 Reasons Most Machine Learning Funds Fail},
  url                  = {https://ssrn.com/abstract=3031282},
  abstract             = {The rate of failure in quantitative finance is high, and particularly so in financial machine learning. The few managers who succeed amass a large amount of assets, and deliver consistently exceptional performance to their investors. However, that is a rare outcome, for reasons that will become apparent in this presentation. Over the past two decades, I have seen many faces come and go, firms started and shut down. In my experience, there are 7 critical mistakes underlying most of those failures.},
  citeulike-article-id = {14428199},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3031282},
  creationdate         = {2023-06-24T20:51:59},
  modificationdate     = {2023-06-24T20:51:59},
  posted-at            = {2017-09-09 20:49:08},
  timestamp            = {2020-07-23 13:38},
}

@Article{dePrado-2018c,
  author               = {{Lopez de Prado}, Marcos},
  date                 = {2018},
  journaltitle         = {SSRN e-Print},
  title                = {The 10 Reasons Most Machine Learning Funds Fail},
  series               = {dePrado-2018a},
  url                  = {https://ssrn.com/abstract=3104816},
  abstract             = {The rate of failure in quantitative finance is high, and particularly so in financial machine learning. The few managers who succeed amass a large amount of assets, and deliver consistently exceptional performance to their investors. However, that is a rare outcome, for reasons that will become apparent in this article. Over the past two decades, I have seen many faces come and go, firms started and shut down. In my experience, there are ten critical mistakes underlying most of those failures.},
  citeulike-article-id = {14520323},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3104816},
  creationdate         = {2023-06-24T20:52:05},
  modificationdate     = {2023-06-24T20:52:05},
  posted-at            = {2018-01-19 21:50:44},
  timestamp            = {2020-07-23 13:38},
}

@Book{dePrado-2018,
  author               = {{Lopez de Prado}, Marcos},
  date                 = {2018},
  title                = {Advances in Financial Machine Learning},
  pagetotal            = {400},
  publisher            = {Wiley},
  url                  = {https://www.wiley.com/en-us/Advances+in+Financial+Machine+Learning-p-9781119482086},
  abstract             = {Machine learning (ML) is changing virtually every aspect of our lives. Today ML algorithms accomplish tasks that until recently only expert humans could perform. As it relates to finance, this is the most exciting time to adopt a disruptive technology that will transform how everyone invests for generations. Readers will learn how to structure Big data in a way that is amenable to ML algorithms; how to conduct research with ML algorithms on that data; how to use supercomputing methods; how to backtest your discoveries while avoiding false positives. The book addresses real-life problems faced by practitioners on a daily basis, and explains scientifically sound solutions using math, supported by code and examples. Readers become active users who can test the proposed solutions in their particular setting. Written by a recognized expert and portfolio manager, this book will equip investment professionals with the groundbreaking tools needed to succeed in modern finance.},
  citeulike-article-id = {14520319},
  creationdate         = {2023-06-24T20:52:05},
  groups               = {ML_BestPractices, ML_Interpretability, ML_Test_FalsePosNeg},
  modificationdate     = {2023-06-24T20:52:05},
  posted-at            = {2018-01-19 21:46:14},
  timestamp            = {2020-02-03 16:43},
}

@Article{dePrado-2018e,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2018-05-03},
  journaltitle     = {SSRN e-Print},
  title            = {How the Sharpe Ratio Died, and Came Back to Life},
  url              = {https://ssrn.com/abstract=3173146},
  creationdate     = {2023-06-24T20:52:05},
  day              = {3},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:05},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2018a,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2018-05-16},
  journaltitle     = {SSRN e-Print},
  title            = {A Practical Solution to the Multiple-Testing Crisis in Financial Research},
  url              = {https://ssrn.com/abstract=3179826},
  abstract         = {Most discoveries in empirical finance are false, as a consequence of selection bias under multiple testing. This may explain why so many hedge funds fail to perform as advertised or as expected, particularly in the quantitative space. These false discoveries may have been prevented if academic journals and investors demanded that any reported investment performance incorporates the false positive probability, adjusted for selection bias under multiple testing. In this presentation, we demonstrate how this adjusted false positive probability can be computed and reported for public consumption. The full paper can be downloaded at http://ssrn.com/abstract=3177057},
  creationdate     = {2023-06-24T20:52:05},
  day              = {16},
  f1000-projects   = {QuantInvest},
  groups           = {Proba_Test},
  modificationdate = {2023-06-24T20:52:05},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2018b,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2018-05-12},
  journaltitle     = {SSRN e-Print},
  title            = {Financial Machine Learning in 10 Minutes},
  url              = {https://ssrn.com/abstract=3177534},
  creationdate     = {2023-06-24T20:52:05},
  day              = {12},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:05},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2018d,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2018-06-10},
  journaltitle     = {SSRN e-Print},
  title            = {Market Microstructure in the Age of Machine Learning},
  url              = {https://ssrn.com/abstract=3193702},
  abstract         = {In this presentation, we analyze the explanatory (in-sample) and predictive (out-of-sample) importance of some of the best known market microstructural features. Our conclusions are drawn over the entire universe of the 87 most liquid futures worldwide, covering all asset classes, going back through 10 years of tick-data history.},
  creationdate     = {2023-06-24T20:52:05},
  day              = {10},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:05},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2018f,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2018},
  journaltitle     = {SSRN e-Print},
  title            = {Significance thresholds of 5\% are suboptimal in finance},
  doi              = {10.2139/ssrn.3201981},
  issn             = {1556-5068},
  url              = {https://ssrn.com/abstract=3201981},
  abstract         = {Full paper is available at: https://ssrn.com/abstract=3193697 Most papers in the financial literature control for Type I errors (false positive rate), while ignoring Type II errors (false negative rate). This is a mistake, because a low Type I error can only be achieved at the cost of a high Type II error.Contrary to long-held beliefs, a familywise significance level below 15\% is suboptimal (excessively conservative) in the context of most investment strategies.},
  creationdate     = {2023-06-24T20:52:05},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:05},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2018g,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2018-05-16},
  journaltitle     = {SSRN e-Print},
  title            = {A Practical Solution to the Multiple-Testing Crisis in Financial Research},
  url              = {https://ssrn.com/abstract=3177057},
  abstract         = {Most discoveries in empirical finance are false, as a consequence of selection bias under multiple testing. This may explain why so many hedge funds fail to perform as advertised or as expected, particularly in the quantitative space. These false discoveries may have been prevented if academic journals and investors demanded that any reported investment performance incorporates the false positive probability, adjusted for selection bias under multiple testing. In this paper we demonstrate how this adjusted false positive probability can be computed and reported for public consumption. The full paper can be downloaded at http://ssrn.com/abstract=3177057},
  creationdate     = {2023-06-24T20:52:09},
  day              = {16},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:09},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-Foreman-2014,
  author               = {{Lopez de Prado}, Marcos and Foreman, Matthew D.},
  date                 = {2014-05},
  journaltitle         = {Quantitative Finance},
  title                = {A mixture of Gaussians approach to mathematical portfolio oversight: the EF3M algorithm},
  doi                  = {10.1080/14697688.2013.861075},
  number               = {5},
  pages                = {913--930},
  volume               = {14},
  abstract             = {An analogue can be made between: (a) the slow pace at which species adapt to an environment, which often results in the emergence of a new distinct species out of a once homogeneous genetic pool and (b) the slow changes that take place over time within a fund, mutating its investment style. A fund?s track record provides a sort of genetic marker, which we can use to identify mutations. This has motivated our use of a biometric procedure to detect the emergence of a new investment style within a fund?s track record. In doing so, we answer the question: What is the probability that a particular PM?s performance is departing from the reference distribution used to allocate her capital? The EF3M algorithm, inspired by evolutionary biology, may help detect early stages of an evolutionary divergence in an investment style and trigger a decision to review a fund?s capital allocation.},
  citeulike-article-id = {14316697},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2013.861075},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2013.861075},
  creationdate         = {2023-06-24T20:52:09},
  day                  = {4},
  modificationdate     = {2023-06-24T20:52:09},
  posted-at            = {2017-03-23 08:35:54},
  publisher            = {Routledge},
  timestamp            = {2019-09-04 17:28},
}

@Article{dePrado-Lewis-2018,
  author           = {{Lopez de Prado}, Marcos and Lewis, Michael J.},
  date             = {2018},
  journaltitle     = {SSRN e-Print},
  title            = {Detection of false investment strategies using unsupervised learning methods},
  doi              = {10.2139/ssrn.3167017},
  issn             = {1556-5068},
  url              = {https://ssrn.com/abstract=3167017},
  abstract         = {Most investment strategies uncovered by practitioners and academics are false. This partially explains the high rate of failure, especially among quantitative hedge funds (smart beta, factor investing, stat-arb, CTAs, etc.) In this paper we examine why false positives are so prevalent in finance, why researchers fail (in many cases purposely) to detect them, and why firms are able to monetize their scheme. Beyond merely pointing to this industrywide problem, we offer a practical solution. We hope that the machine learning tools presented in this paper will help financial academic journals filter out false positives, and bring up the retraction rate to reasonable levels. The SEC, FINRA and other regulatory agencies worldwide could use these tools to take a more active role in curving this rampant financial fraud. A presentation based on this paper can be found at https://ssrn.com/abstract=3173146},
  creationdate     = {2023-06-24T20:52:09},
  f1000-projects   = {QuantInvest},
  groups           = {ML_Test_FalsePosNeg},
  modificationdate = {2023-06-24T20:52:09},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-Lewis-2018a,
  author           = {{Lopez de Prado}, Marcos and Lewis, Michael J.},
  date             = {2018-06-10},
  journaltitle     = {SSRN e-Print},
  title            = {What is the Optimal Significance Level for Investment Strategies?},
  url              = {https://ssrn.com/abstract=3193697},
  abstract         = {Most papers in the financial literature estimate the p-value associated with an investment strategy, without reporting the power of the test used to make that discovery. This is a mistake, because a particularly low false positive rate (Type I error) may be achieved at the expense of missing a large proportion of the investment opportunities (Type II error). In this paper we provide analytic estimates to Type I and Type II errors in the context of investments, and derive the familywise significance level that optimizes the performance of hypothesis tests under general assumptions. Contrary to long-held beliefs, we conclude that a familywise significance level below 15\% is suboptimal (excessively conservative) in the context of most investment strategies.},
  creationdate     = {2023-06-24T20:52:09},
  day              = {10},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:09},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-Rebonato-2016,
  author               = {{Lopez de Prado}, Marcos and Rebonato, Riccardo},
  date                 = {2016-08},
  journaltitle         = {The Journal of Investing},
  title                = {Kinetic Component Analysis},
  doi                  = {10.3905/joi.2016.25.3.142},
  issn                 = {1068-0896},
  number               = {3},
  pages                = {142--154},
  volume               = {25},
  abstract             = {The authors introduce kinetic component analysis (KCA), a state-space application that extracts the signal from a series of noisy measurements by applying a Kalman filter on a Taylor expansion of a stochastic process. They show that KCA presents several advantages over such popular noise-reduction methods as fast Fourier transform (FFT) or locally weighted scatterplot smoothing (LOWESS). First, KCA provides band estimates in addition to point estimates. Second, KCA further decomposes the signal in terms of three hidden components, which can be intuitively associated with position, velocity, and acceleration. Third, KCA is more robust in forecasting applications. Fourth, KCA is a forward-looking, state-space approach, resilient to structural changes. The authors believe that this type of decomposition is particularly useful in the analysis of trend following, momentum, and mean reversion in financial prices. An instrument exhibits financial inertia when its price acceleration is not significantly greater than zero for long periods of time. This empirical analysis of 19 of the most liquid futures worldwide confirms the presence of strong inertia across all asset classes. The authors also argue that KCA can be useful to market makers, liquidity providers, and faders for the calculation of their trading ranges.},
  citeulike-article-id = {14150162},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/joi.2016.25.3.142},
  creationdate         = {2023-06-24T20:52:09},
  groups               = {ChngPoints_TimeSrs},
  modificationdate     = {2023-06-24T20:52:09},
  owner                = {cristi},
  posted-at            = {2016-10-02 04:27:04},
  timestamp            = {2019-09-04 17:28},
}

@Article{dePrado-Leinweber-2012,
  author           = {{Lopez de Prado}, Marcos and David Leinweber},
  date             = {2012},
  journaltitle     = {The Journal of Investment Strategies},
  title            = {Advances in cointegration and subset correlation hedging methods},
  number           = {2},
  pages            = {67--115},
  volume           = {1},
  abstract         = {We divide hedging methods between single-period and multi-period. After reviewing some well-known hedging algorithms, two new procedures are introduced, called Dickey-Fuller Optimal (DFO), Mini-Max Subset Correlation (MMSC). The former is a multi-period, cointegration-based hedging method that estimates the holdings that are most likely to deliver a hedging error absent of unit root. The latter is a single-period method that studies the geometry of the hedging errors and estimates a hedging vector such that subsets of its components are as orthogonal as possible to the error.

We test each method for stability and robustness of the derived hedged portfolio. Results indicate that DFO produces estimates similar to the Error Correction Method, but more stable. Likewise, MMSC estimates are similar to Principal Component Analysis but more stable. Finally, a generalized Box-Tiao Canonical Decomposition (BTCD) method is proposed, which is of the multi-period class. BTCD estimates are also very stable, and cannot be related to any of the aforementioned methodologies.

Finally, we find that all three advanced hedging methods (MMSC, BTCD, DFO) perform well.},
  creationdate     = {2023-06-24T20:52:09},
  modificationdate = {2023-06-24T20:52:09},
  owner            = {zkgst0c},
  timestamp        = {2019-09-16 12:41},
}

@Article{dePrado-Bailey-2018,
  author           = {{Lopez de Prado}, Marcos and Bailey, David H.},
  date             = {2018-07-29},
  journaltitle     = {SSRN e-Print},
  title            = {The False Strategy Theorem: A Financial Application of Experimental Mathematics},
  url              = {https://ssrn.com/abstract=3221798},
  abstract         = {The False Strategy theorem tells us that the optimal outcome of an unknown number of historical simulations is right-unbounded - with enough trials, there is no Sharpe ratio sufficiently enough to reject the hypothesis that a strategy is false. Given the ease with which one can use a computer to explore many trials or variations of given strategy and only select the optimal variation, it follows that it is very easy to find impressive-looking strategy variations that are nothing more than false positives. This is the essence of selection bias under multiple testing.},
  creationdate     = {2023-06-24T20:52:09},
  day              = {29},
  f1000-projects   = {QuantInvest},
  groups           = {Test_MultiHypotheses},
  modificationdate = {2023-06-24T20:52:09},
  timestamp        = {2020-07-23 13:38},
}

@Article{Fabozzi-dePrado-2018,
  author           = {Fabozzi, Frank J. and {Lopez de Prado}, Marcos},
  date             = {2018-10-31},
  journaltitle     = {The Journal of Portfolio Management},
  title            = {Being Honest in Backtest Reporting: A Template for Disclosing Multiple Tests},
  doi              = {10.3905/jpm.2018.45.1.141},
  number           = {1},
  pages            = {141-147},
  url              = {https://jpm.pm-research.com/content/45/1/141},
  volume           = {45},
  abstract         = {Selection bias under multiple testing is a serious problem. From a practitioner perspective, failure to disclose the impact of multiple tests of a proposed investment strategy to clients and senior management can lead to the adoption of a false discovery. Clients will lose money, senior management will misallocate resources, and the firm may be exposed to reputational, legal, and regulatory risks. From the perspective of academic journals that publish evidence supporting an investment strategy, the failure to address selection bias under multiple testing threatens to invalidate large portions of the literature in empirical finance. In this article, the authors propose a template that practitioners should use to fairly disclose multiple tests involved in an alleged discovery when pitching strategies to clients and senior management. The same template could be used by contributors to academic journals so that referees, and ultimately readers, can assess the strategy. By disclosing this information, those who are charged with making the final decision about a discovery can evaluate the probability that the purported discovery is false.},
  creationdate     = {2023-06-24T20:52:09},
  day              = {31},
  f1000-projects   = {QuantInvest},
  groups           = {Proba_Test},
  modificationdate = {2023-06-24T20:52:09},
  publisher        = {Institutional Investor Journals Umbrella},
  timestamp        = {2020-07-03 01:19},
}

@Article{dePrado-2019,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2019-04-16},
  journaltitle     = {SSRN e-Print},
  title            = {The 7 Reasons Most Econometric Investments Fail},
  url              = {https://ssrn.com/abstract=3373116},
  urldate          = {2019-04-19},
  abstract         = {This presentation reviews the main reasons why investment strategies discovered through econometric methods fail. As a solution, it proposes the modernization of the statistical methods used by financial firms and academic authors.This material is part of Cornell University's ORIE 5256 graduate course at the School of Engineering.},
  creationdate     = {2023-06-24T20:52:09},
  day              = {16},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:09},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2019a,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2019-02-01},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {A Data Science Solution to the Multiple-Testing Crisis in Financial Research},
  doi              = {10.3905/jfds.2019.1.099},
  issn             = {2640-3951},
  number           = {1},
  pages            = {99-110},
  url              = {https://jfds.iijournals.com/content/1/1/99},
  urldate          = {2019-04-29},
  volume           = {1},
  abstract         = {Most discoveries in empirical finance are false, as a consequence of selection bias under multiple testing. Although many researchers are aware of this problem, the solutions proposed in the literature tend to be complex and hard to implement. In this article, the author reduces the problem of selection bias in the context of investment strategy development to two sub-problems: determining the number of essentially independent trials and determining the variance across those trials. The author explains what data researchers need to report to allow others to evaluate the effect that multiple testing has had on reported performance. He applies his method to a real case of strategy development and estimates the probability that a discovered strategy is false.},
  creationdate     = {2023-06-24T20:52:09},
  day              = {1},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:09},
  timestamp        = {2019-09-16 12:41},
}

@Article{dePrado-2019b,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2019},
  journaltitle     = {SSRN e-Printl},
  title            = {The past and future of quantitative research},
  doi              = {10.2139/ssrn.3447561},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3447561},
  urldate          = {2019-10-02},
  abstract         = {Traditionally, the development of investment strategies has required domain-specific knowledge and access to restricted datasets. These two barriers exist by design: (a) Financial knowledge is hoarded by firms, and protected as trade secrets, and (b) Financial data is expensive, making it inaccessible to the broad scientific community.This presentation explores how these two barriers impact the quality of quantitative research, and how investment tournaments can help deliver better investment outcomes by overcoming those two barriers.},
  creationdate     = {2023-06-24T20:52:09},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:09},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2019c,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2019},
  journaltitle     = {SSRN e-Print},
  title            = {Tactical Investment Algorithms},
  doi              = {10.2139/ssrn.3459866},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3459866},
  urldate          = {2019-10-24},
  abstract         = {There are three fundamental ways of testing the validity of an investment algorithm against historical evidence: a) the walk-forward method; b) the resampling method; and c) the Monte Carlo method. By far the most common approach followed among academics and practitioners is the walk-forward method. Implicit in that choice is the assumption that a given investment algorithm should be deployed throughout all market regimes. We denote such assumption the -weather hypothesis, and the algorithms based on that hypothesis investment algorithms (or strategies). The all-weather hypothesis is not necessarily true, as demonstrated by the fact that many investment strategies have floundered in a zero-rate environment. This motivates the problem of identifying investment algorithms that are optimal for specific market regimes, denoted investment algorithms. This paper argues that backtesting against synthetic datasets should be the preferred approach for developing tactical investment algorithms. A new organizational structure for asset managers is proposed, as a tactical algorithmic factory, consistent with the Monte Carlo backtesting paradigm.},
  creationdate     = {2023-06-24T20:52:09},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:09},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2019d,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {A robust estimator of the efficient frontier},
  doi              = {10.2139/ssrn.3469961},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3469961},
  urldate          = {2019-10-24},
  abstract         = {Convex optimization solutions tend to be unstable, to the point of entirely offsetting the benefits of optimization. For example, in the context of financial applications, it is known that portfolios optimized in-sample often underperform the naive (equal weights) allocation out-of-sample. This instability can be traced back to two sources: (i) noise in the input variables; and (ii) signal structure that magnifies the estimation errors in the input variables. A first innovation of this paper is to introduce the nested clustered optimization algorithm ({NCO}), a method that tackles both sources of instability.Over the past 60 years, various approaches have been developed to address these two sources of instability. These approaches are flawed in the sense that different methods may be appropriate for different input variables, and it is unrealistic to expect that one method will dominate all the rest under all circumstances. Accordingly, a second innovation of this paper is to introduce {MCOS}, a Monte Carlo approach that estimates the allocation error produced by various optimization methods on a particular set of input variables. The result is a precise determination of what method is most robust to a particular case. Thus, rather than relying always on one particular approach, {MCOS} allows users to apply opportunistically whatever optimization method is best suited in a particular setting.Presentation materials are available at: https://ssrn.com/abstract=3469964.},
  creationdate     = {2023-06-24T20:52:10},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:10},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2019e,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2019},
  journaltitle     = {SSRN e-Print},
  title            = {Machine Learning Asset Allocation},
  doi              = {10.2139/ssrn.3469964},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3469964},
  urldate          = {2019-10-24},
  abstract         = {Convex optimization solutions tend to be unstable, to the point of entirely offsetting the benefits of optimization. For example, in the context of financial applications, it is known that portfolios optimized in sample often underperform the naive (equal weights) allocation out of sample.This instability can be traced back to two sources: (1) noise in the input variables; and (2) signal structure that magnifies the estimation errors in the input variables.There is abundant literature discussing noise induced instability. In contrast, signal induced instability is often ignored or misunderstood.We introduce a new optimization method that is robust to signal induced instability.For additional details, see the full paper at: https://ssrn.com/abstract=3469961.},
  creationdate     = {2023-06-24T20:52:10},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:10},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2020,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Codependence},
  doi              = {10.2139/ssrn.3512994},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3512994},
  urldate          = {2020-01-19},
  abstract         = {Two random variables are codependent when knowing the value of one helps us determine the value of the other. This should not me confounded with the notion of causality.Correlation is perhaps the best known measure of codependence in econometric studies. Despite its popularity among economists, correlation has many known limitations in the contexts of financial studies.In this seminar we will explore more modern measures of codependence, based on information theory, which overcome some of the limitations of correlations.},
  creationdate     = {2023-06-24T20:52:10},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:10},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-Fabozzi-2019,
  author           = {{Lopez de Prado}, Marcos and Fabozzi, Frank J.},
  date             = {2019-11-15},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {Crowdsourced Investment Research Through Tournaments},
  url              = {https://jfds.pm-research.com/content/early/2019/11/15/jfds.2019.1.016},
  urldate          = {2019-11-19},
  creationdate     = {2023-06-24T20:52:25},
  day              = {15},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:25},
  publisher        = {Institutional Investor Journals Umbrella},
  timestamp        = {2020-01-24 08:49},
}

@Article{dePrado-2020b,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Overfitting: causes and solutions},
  doi              = {10.2139/ssrn.3544431},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3544431},
  urldate          = {2020-03-06},
  abstract         = {When used incorrectly, the risk of machine learning (ML) overfitting is extremely high. However, ML counts with sophisticated methods to prevent: (a) train set overfitting, and (b) test set overfitting.Thus, the popular belief that ML overfits is false. A more accurate statement would be that: (1) in the wrong hands, ML overfits, and (2) in the right hands, ML is more robust to overfitting than classical methods.When it comes to modelling unstructured data, ML is the only choice. Classical statistics should be taught as a preparation for ML courses, with a focus on overfitting prevention.},
  creationdate     = {2023-06-24T20:52:25},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:25},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2020c,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Clustered feature importance (presentation slides)},
  doi              = {10.2139/ssrn.3517595},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3517595},
  urldate          = {2020-03-28},
  abstract         = {A substitution effect takes place when two or more explanatory variables share a substantial amount of information (predictive power).Under the presence of substitution effects, feature importance methods may not be able to determine robustly which variables are significant.This presentation discusses the Clustered Feature Importance (CFI) method, which is robust to linear as well as non-linear substitution effects.},
  creationdate     = {2023-06-24T20:52:25},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:25},
  timestamp        = {2020-07-23 13:38},
}

@Book{dePrado-2020f,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2020-04-30},
  title            = {Machine learning for asset managers},
  doi              = {10.1017/9781108883658},
  isbn             = {9781108792899},
  pagetotal        = {190},
  publisher        = {Cambridge University Press},
  url              = {https://www.cambridge.org/core/product/identifier/9781108883658/type/element},
  urldate          = {2020-04-09},
  abstract         = {Successful investment strategies are specific implementations of general theories. An investment strategy that lacks a theoretical justification is likely to be false. Hence, an asset manager should concentrate her efforts on developing a theory rather than on backtesting potential trading rules. The purpose of this Element is to introduce machine learning ({ML}) tools that can help asset managers discover economic and financial theories. {ML} is not a black box, and it does not necessarily overfit. {ML} tools complement rather than replace the classical statistical methods. Some of {ML}'s strengths include (1) a focus on out-of-sample predictability over variance adjudication; (2) the use of computational methods to avoid relying on (potentially unrealistic) assumptions; (3) the ability to  learn  complex specifications, including nonlinear, hierarchical, and noncontinuous interaction effects in a high-dimensional space; and (4) the ability to disentangle the variable search from the specification search, robust to multicollinearity and other substitution effects.},
  creationdate     = {2023-06-24T20:52:25},
  day              = {30},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:25},
  timestamp        = {2020-08-30 04:47},
}

@Article{dePrado-2019f,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2019},
  journaltitle     = {SSRN e-Print},
  title            = {Beyond econometrics: A roadmap towards financial machine learning},
  doi              = {10.2139/ssrn.3365282},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3365282},
  urldate          = {2020-06-03},
  abstract         = {One of the most exciting recent developments in financial research is the availability of new administrative, private sector and micro-level datasets that did not exist a few years ago. The unstructured nature of many of these observations, along with the complexity of the phenomena they measure, means that many of these datasets are beyond the grasp of econometric analysis. Machine learning ({ML}) techniques offer the numerical power and functional flexibility needed to identify complex patterns in a high-dimensional space. However, {ML} is often perceived as a black box, in contrast with the transparency of econometric approaches. This article demonstrates that each analytical step of the econometric process has a homologous step in {ML} analyses. By clearly stating this correspondence, our goal is to facilitate and reconcile the adoption of {ML} techniques among econometricians.},
  creationdate     = {2023-06-24T20:52:25},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:25},
  timestamp        = {2020-07-23 13:38},
}

@Article{dePrado-2020d,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Three Machine Learning Solutions to the Bias-Variance Dilemma},
  doi              = {10.2139/ssrn.3588594},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3588594},
  urldate          = {2020-05-31},
  abstract         = {Classical statistics (e.g., Econometrics) relies on assumptions that are often unrealistic in finance. Two critical assumptions are that the researcher has perfect knowledge about the model  specification, and that the researcher knows all the variables involved in a phenomenon (including all interaction effects). When those assumptions are incorrect, classical estimators are not guaranteed to be unbiased, or to be the most efficient among the unbiased, leading to poor performance.In this presentation we explore why machine learning algorithms are generally more appropriate for financial datasets, how they outperform classical estimators, and how they solve the bias-variance dilemma.},
  creationdate     = {2023-06-24T20:52:25},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:52:25},
  timestamp        = {2020-08-30 04:27},
}

@Article{dePrado-2020e,
  author           = {{Lopez de Prado}, Marcos},
  date             = {2020},
  journaltitle     = {Alternative Investment Analyst Review},
  title            = {Tactical Investment Algorithms},
  number           = {1},
  url              = {https://caia.org/aiar/4144#aiar-default-5},
  volume           = {9},
  abstract         = {Finance has two major limitations that prevent it from becoming a science, unlike physics, chemistry or biology. These two limitations, Popper's falsifiability criterion and complexity in the changing financial system, force researchers to rely on backtesting when creating investment algorithms. There are three types of backtests, which includes the walk-forward method, the resampling method, and the Monte Carlo (MC) method. In this paper, Lopez de Prado argues the MC method as the most useful of the three types of backtests. The MC method is further discussed with a practical example, a discussion of its advantages and criticisms, and finally a deeper dive into a key part of MC analysis referred to as the data-generating process (DGP).},
  creationdate     = {2023-06-24T20:52:25},
  modificationdate = {2023-06-24T20:52:25},
  timestamp        = {2020-06-24 22:00},
}

@InProceedings{Lu-et-al-2021b,
  author           = {Yahui Lu and Minghao Li and Xiaochu Tang and Hui Wang},
  booktitle        = {{IEEE} 24th International Conference on Computer Supported Cooperative Work in Design ({CSCWD})},
  date             = {2021-05},
  title            = {A Cluster Representative Selection Method for Stock Portfolio Based on Efficient Frontier},
  doi              = {10.1109/cscwd49262.2021.9437767},
  publisher        = {{IEEE}},
  abstract         = {Portfolio is a financial concept to combine several stocks to reduce the risks and improve the profits. To choose the basic members of portfolio, we can group similar stocks into one cluster and then choose representative stock from each cluster. In this paper, we focus on the method of choosing representative stocks in clusters. The ordinary representative of a cluster is often the center of that cluster. We propose a new cluster representative method MDR (maximum distance representatives). In our method MDR, we choose the stocks which has maximum distance with other representatives. MDR can construct a more diverse portfolio than center method. The effectiveness of cluster representative selection methods can be evaluated by an index IBEF based on the concept of efficient frontier. Our experiments show that MDR can effectively improve the efficient frontier, which means MDR can bring more profits than center representative method at the same risk level.},
  creationdate     = {2023-06-24T20:54:22},
  modificationdate = {2023-06-24T20:54:22},
}

@Article{Marti-et-al-2017a,
  author               = {Marti, Gautier and Andler, Sebastien and Nielsen, Frank and Donnat, Philippe},
  date                 = {2017},
  journaltitle         = {Proceedings of Machine Learning Research},
  title                = {Exploring and measuring non-linear correlations: Copulas, Lightspeed Transportation and Clustering},
  pages                = {59-69},
  volume               = {55},
  abstract             = {We propose a methodology to explore and measure the pairwise correlations that exist between variables in a dataset. The methodology leverages copulas for encoding dependence between two variables, state-of-the-art optimal transport for providing a relevant geometry to the copulas, and clustering for summarizing the main dependence patterns found between the variables. Some of the clusters centers can be used to parameterize a novel dependence coefficient which can target or forget specific dependence patterns. Finally, we illustrate and benchmark the methodology on several datasets. Code and numerical experiments are available online at \url{https://www.datagrapple.com/Tech} for reproducible research.},
  citeulike-article-id = {14291484},
  citeulike-linkout-0  = {http://arxiv.org/abs/1610.09659},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1610.09659},
  creationdate         = {2023-06-24T20:56:13},
  day                  = {30},
  modificationdate     = {2023-06-24T20:56:13},
  posted-at            = {2017-03-03 18:30:47},
  timestamp            = {2020-07-23 13:33},
}

@InCollection{Marti-et-al-2021a,
  author           = {Marti, Gautier and Nielsen, Frank and Bihkowski, Mikolaj and Donnat, Philippe},
  booktitle        = {Progress in Information Geometry},
  date             = {2021},
  title            = {A review of two decades of correlations, hierarchies, networks and clustering in financial markets},
  doi              = {10.1007/978-3-030-65459-7_10},
  pages            = {245-274},
  url              = {https://link.springer.com/chapter/10.1007/978-3-030-65459-7_10},
  abstract         = {We review the state of the art of clustering financial time series and the study of their correlations alongside other interaction networks. The aim of the review is to gather in one place the relevant material from different fields, e.g. machine learning, information geometry, econophysics, statistical physics, econometrics, behavioral finance. We hope it will help researchers to use more effectively this alternative modeling of the financial time series. Decision makers and quantitative researchers may also be able to leverage its insights. Finally, we also hope that this review will form the basis of an open toolbox to study correlations, hierarchies, networks and clustering in financial markets.},
  creationdate     = {2023-06-24T20:56:13},
  day              = {1},
  groups           = {Networks and investment management, Clustering and network analysis, Invest_Network},
  modificationdate = {2023-06-24T20:56:13},
  timestamp        = {2020-10-23 13:33},
}

@InCollection{Marti-et-al-2021b,
  author           = {Gautier Marti and Victor Goubet and Frank Nielsen},
  booktitle        = {International Conference on Geometric Science of Information},
  date             = {2021},
  title            = {{cCorrGAN}: Conditional Correlation {GAN} for Learning Empirical Conditional Distributions in the Elliptope},
  doi              = {10.1007/978-3-030-80209-7_66},
  pages            = {613--620},
  publisher        = {Springer International Publishing},
  abstract         = {We propose a methodology to approximate conditional distributions in the elliptope of correlation matrices based on conditional generative adversarial networks. We illustrate the methodology with an application from quantitative finance: Monte Carlo simulations of correlated returns to compare risk-based portfolio construction methods. Finally, we discuss about current limitations and advocate for further exploration of the elliptope geometry to improve results.},
  creationdate     = {2023-06-24T20:56:13},
  modificationdate = {2023-06-24T20:56:14},
}

@Article{Marti-et-al-2021,
  author           = {Gautier Marti and Victor Goubet and Frank Nielsen},
  date             = {2021-07-22},
  journaltitle     = {arXiv e-Print},
  title            = {cCorrGAN: Conditional Correlation GAN for Learning Empirical Conditional Distributions in the Elliptope},
  eprint           = {2107.10606},
  eprintclass      = {q-fin.ST},
  eprinttype       = {arXiv},
  abstract         = {We propose a methodology to approximate conditional distributions in the elliptope of correlation matrices based on conditional generative adversarial networks. We illustrate the methodology with an application from quantitative finance: Monte Carlo simulations of correlated returns to compare risk-based portfolio construction methods. Finally, we discuss about current limitations and advocate for further exploration of the elliptope geometry to improve results.},
  creationdate     = {2023-06-24T20:56:14},
  file             = {:http\://arxiv.org/pdf/2107.10606v1:PDF},
  keywords         = {q-fin.ST, cs.LG},
  modificationdate = {2023-06-24T20:56:14},
}

@Article{Marti-et-al-2020,
  author           = {Marti, Gautier and Nielsen, Frank and Bihkowski, Mikolaj and Donnat, Philippe},
  date             = {2020},
  journaltitle     = {arXiv e-Print},
  title            = {A review of two decades of correlations, hierarchies, networks and clustering in financial markets},
  eprinttype       = {arXiv},
  url              = {https:://arxiv.org/abs/1703.00485},
  urldate          = {2020-10-07},
  abstract         = {This document is a preliminary version of an in-depth review on the state of the art of clustering financial time series and the study of correlation networks. This preliminary document is intended for researchers in this field so that they can feedback to allow amendments, corrections and addition of new material unknown to the authors of this review. The aim of the document is to gather in one place the relevant material that can help the researcher in the field to have a bigger picture, the quantitative researcher to play with this alternative modeling of the financial time series, and the decision maker to leverage the insights obtained from these methods. We hope that this document will form a basis for implementation of an open toolbox of standard tools to study correlations, hierarchies, networks and clustering in financial markets. We also plan to maintain pointers to online material and an updated version of this work at www.datagrapple.com/Tech.},
  creationdate     = {2023-06-24T20:56:29},
  day              = {1},
  groups           = {Networks and investment management, Clustering and network analysis, Invest_Network},
  modificationdate = {2023-06-24T20:56:29},
  timestamp        = {2020-10-23 13:33},
}

@InCollection{Marti-et-al-2016b,
  author               = {Marti, Gautier and Nielsen, Frank and Donnat, Philippe},
  booktitle            = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  date                 = {2016-03},
  title                = {Optimal copula transport for clustering multivariate time series},
  doi                  = {10.1109/icassp.2016.7472103},
  isbn                 = {978-1-4799-9988-0},
  location             = {Shanghai},
  pages                = {2379--2383},
  publisher            = {IEEE},
  abstract             = {This paper presents a new methodology for clustering multivariate time series leveraging optimal transport between copulas. Copulas are used to encode both (i) intra-dependence of a multivariate time series, and (ii) inter-dependence between two time series. Then, optimal copula transport allows us to define two distances between multivariate time series: (i) one for measuring intra-dependence dissimilarity, (ii) another one for measuring inter-dependence dissimilarity based on a new multivariate dependence coefficient which is robust to noise, deterministic, and which can target specified dependencies.},
  citeulike-article-id = {14445613},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/icassp.2016.7472103},
  creationdate         = {2023-06-24T20:56:29},
  modificationdate     = {2023-06-24T20:56:29},
  posted-at            = {2017-10-05 09:12:20},
  timestamp            = {2019-09-14 22:09},
}

@InCollection{Marti-et-al-2017,
  author               = {Marti, Gautier and Nielsen, Frank and Donnat, Philippe and Andler, Sebastien},
  booktitle            = {Computational Information Geometry},
  date                 = {2017},
  title                = {On Clustering Financial Time Series: A Need for Distances Between Dependent Random Variables},
  doi                  = {10.1007/978-3-319-47058-0_8},
  editor               = {Nielsen, Frank and Critchley, Frank and Dodson, Christopher T. J.},
  pages                = {149--174},
  publisher            = {Springer International Publishing},
  series               = {Signals and Communication Technology},
  abstract             = {This artilce summarizes our work on the clustering of financial time series. It was written for a workshop on information geometry and its application for image and signal processing. This workshop brought several experts in pure and applied mathematics together with applied researchers from medical imaging, radar signal processing and finance. The authors belong to the latter group. This document was written as a long introduction to further development of geometric tools in financial applications such as risk or portfolio analysis. Indeed, risk and portfolio analysis essentially rely on covariance matrices. Besides that the Gaussian assumption is known to be inaccurate, covariance matrices are difficult to estimate from empirical data. To filter noise from the empirical estimate, Mantegna proposed using hierarchical clustering. In this work, we first show that this procedure is statistically consistent. Then, we propose to use clustering with a much broader application than the filtering of empirical covariance matrices from the estimated correlation coefficients. To be able to do that, we need to obtain distances between the financial time series that incorporate all the available information in these cross-dependent random processes.},
  citeulike-article-id = {14324929},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-47058-08},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-47058-08},
  creationdate         = {2023-06-24T20:56:29},
  groups               = {Clustering and network analysis},
  modificationdate     = {2023-06-24T20:56:29},
  posted-at            = {2017-03-30 22:23:54},
  timestamp            = {2020-05-10 01:38},
}

@InCollection{Marti-et-al-2015,
  author               = {Marti, Gautier and Very, Philippe and Donnat, Philippe and Nielsen, Frank},
  booktitle            = {IEEE 14th International Conference on Machine Learning and Applications (ICMLA)},
  date                 = {2015-12},
  title                = {A Proposal of a Methodological Framework with Experimental Guidelines to Investigate Clustering Stability on Financial Time Series},
  doi                  = {10.1109/icmla.2015.11},
  isbn                 = {978-1-5090-0287-0},
  location             = {Miami, FL, USA},
  pages                = {32--37},
  publisher            = {IEEE},
  abstract             = {We present in this paper an empirical framework motivated by the practitioner point of view on stability. The goal is to both assess clustering validity and yield market insights by providing through the data perturbations we propose a multi-view of the assets' clustering behaviour. The perturbation framework is illustrated on an extensive credit default swap time series database available online at www.datagrapple.com.},
  citeulike-article-id = {14357917},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/icmla.2015.11},
  creationdate         = {2023-06-24T20:56:29},
  groups               = {ML_Network_QWIM, ML_ClustTimeSrs},
  modificationdate     = {2023-06-24T20:56:29},
  posted-at            = {2017-05-16 13:54:49},
  timestamp            = {2019-10-11 14:22},
}

@Article{Massahi-et-al-2020,
  author           = {Mahdi Massahi and Masoud Mahootchi and Alireza Arshadi Khamseh},
  date             = {2020},
  journaltitle     = {Empirical Economics},
  title            = {Development of an efficient cluster-based portfolio optimization model under realistic market conditions},
  doi              = {10.1007/s00181-019-01802-5},
  abstract         = {Modern portfolio theory introduced by Markowitz in 1952 is the most popular portfolio optimization framework established based on the trade-off between risk and return as an operation research model. The main shortcoming of applying Markowitz portfolio optimization in practice is that the obtained optimal weights are really sensitive to the embedded uncertainty in return series of stocks. In this paper, it is demonstrated how using a new methodology of time series clustering as a remedy can lead to a more robust and accurate portfolio in terms of the gap between mean variance efficient frontier obtained from the optimization model and the one observed in reality. In this regard, two similarity measures, the autocorrelation coefficients and the weighted dynamic time warping, are used in an innovative way to construct the desired portfolio optimization model. Moreover, the effectiveness of proposed approach is investigated in two different market conditions: semi-realistic and full-realistic. In the first one, it is assumed that the forecasted and realized stocks mean returns are the same; however, these returns are not necessarily equal in the second market conditions. Finally, a database of stock prices from the literature is utilized to show the robustness and accuracy of the proposed approach in empirical results in comparison with applied similarity measures in previous researches.},
  creationdate     = {2023-06-24T20:57:08},
  journal          = {Empirical Economics},
  modificationdate = {2023-06-24T20:57:08},
  month            = {jan},
  publisher        = {Springer Science and Business Media {LLC}},
  timestamp        = {2020-09-07 22:48},
  year             = {2020},
}

@Article{Millington-Niranjan-2021a,
  author           = {Tristan Millington and Mahesan Niranjan},
  date             = {2021-07},
  journaltitle     = {Physica A: Statistical Mechanics and its Applications},
  title            = {Stability and similarity in financial networks -- How do they change in times of turbulence?},
  doi              = {10.1016/j.physa.2021.126016},
  pages            = {126016},
  volume           = {574},
  abstract         = {Diversified portfolios are a key component of modern portfolio theory, based on the idea of choosing uncorrelated or unrelated stocks to minimize risk. With this in mind, we use networks to study the correlations between stocks and how this varies over time, using daily returns from the S\&P500 (US), FTSE100 (UK) and DAX30 (Germany). We study both the full correlation networks and those filtered using the PMFG method. We conclude that stocks tend to become more similar in the full correlation networks during times of market disruption for the US and UK markets - implying that nodes that were once dissimilar (and therefore a good choice for a low risk portfolio) are no longer so, demonstrating the difficulties of choosing a diversified portfolio. Furthermore, these full networks are also more stable by certain measures during these periods of disruption, contrary to expectations. However, these apply less to the PMFGs and the German market.},
  creationdate     = {2023-06-24T20:59:05},
  modificationdate = {2023-06-24T20:59:05},
  publisher        = {Elsevier {BV}},
}

@Article{Millington-Niranjan-2021,
  author           = {Tristan Millington and Mahesan Niranjan},
  date             = {2021-03},
  journaltitle     = {Physica A: Statistical Mechanics and its Applications},
  title            = {Construction of minimum spanning trees from financial returns using rank correlation},
  doi              = {10.1016/j.physa.2020.125605},
  pages            = {125605},
  volume           = {566},
  abstract         = {The construction of minimum spanning trees (MSTs) from correlation matrices is an often used method to study relationships in the financial markets. However most of the work on this topic tends to use the Pearson correlation coefficient, which relies on the assumption of normality and can be brittle to the presence of outliers, neither of which is ideal for the study of financial returns. In this paper we study the inference of MSTs from daily US, UK and German financial returns using Pearson and two rank correlation methods, Spearman and Kendall's . MSTs constructed using these rank methods tend to be more stable and maintain more edges over the dataset than those constructed using Pearson correlation. The edge agreement between the Pearson and rank MSTs varies significantly depending on the state of the markets, but the rank MSTs generally show strong agreement at all times. Deviation from univariate normality can be related to changes in the correlation matrices but is more difficult to connect to changes in the MSTs. Irrelevant of coefficient, the trees tend to have similar topologies. Portfolios constructed from the MST correlation matrices have a smaller turnover than those from the full covariance matrix for the larger markets, but not for the smaller German market. Using a bootstrap method we find that the correlation matrices constructed using the rank correlations are more robust, but there is little difference between the robustness of the MSTs.},
  creationdate     = {2023-06-24T20:59:05},
  modificationdate = {2023-06-24T20:59:05},
  publisher        = {Elsevier {BV}},
}

@Article{Millington-Niranjan-2020,
  author           = {Millington, Tristan and Niranjan, Mahesan},
  date             = {2020-05-08},
  journaltitle     = {arXiv e-Print},
  title            = {Construction of Minimum Spanning Trees from Financial Returns using Rank Correlation},
  eprinttype       = {arxiv},
  url              = {https://arxiv.org/abs/2005.03963},
  abstract         = {The construction of minimum spanning trees ({MSTs}) from correlation matrices is an often used method to study relationships in the financial markets. However most of the work on this topic tends to use the Pearson correlation coefficient, which relies on the assumption of normality and can be brittle to the presence of outliers, neither of which is ideal for the study of financial returns. In this paper we study the inference of {MSTs} from daily {US} financial returns using Pearson and two rank correlation methods, Spearman and Kendall's tau. We find that the trees constructed using these rank methods tend to be more stable and maintain more edges over the dataset than those constructed using Pearson correlation, that there are significant differences in the agreement of the centrality of various sectors and that despite these, the trees tend to have similar topologies.},
  creationdate     = {2023-06-24T20:59:05},
  day              = {8},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T20:59:05},
  timestamp        = {2020-08-30 06:29},
}

@Article{Lakshtanov-Molyboga-2022,
  author           = {Evgeny Lakshtanov and Marat Molyboga},
  date             = {2022},
  journaltitle     = {SSRN e-Print},
  title            = {A Novel Approach to Denoising Correlation Matrices With Applications to Global Portfolio Management With a Large Number of Assets},
  doi              = {10.2139/ssrn.4258425},
  abstract         = {We introduce a new approach to denoising correlation matrices that imposes a block structure with a fixed block-dependent pair-wise correlation within each block and a constant correlation specified for each pair of blocks. We characterize the eigenvalue spectrum and modify the Marchenko-Pastur distribution of eigenvalues. We present approximate analytic solutions for the inverse problem of determining the block sizes and the correlation parameters under a broad set of assumptions. Our solution is based on a novel approach for improving correlation matrix estimation by a functional transformation of the original correlation matrix. Our correlation matrix denoising method has broad applications for global portfolio management with a large number of assets from many diverse asset classes.},
  creationdate     = {2023-06-24T20:59:53},
  modificationdate = {2023-06-24T20:59:53},
  publisher        = {Elsevier {BV}},
}

@Article{Molyboga-et-al-2020,
  author           = {Marat Molyboga and Junkai Qian and Chaohua He},
  date             = {2020},
  journaltitle     = {The Journal of Alternative Investments},
  title            = {Carry and Time-Series Momentum: A Match Made in Heaven},
  doi              = {10.3905/jai.2020.1.106},
  number           = {2},
  pages            = {84-93},
  url              = {https://jai.pm-research.com/content/early/2020/08/01/jai.2020.1.106},
  volume           = {23},
  abstract         = {This article introduces a novel approach to combining time-series momentum and carry trade by conditioning trading signals of time-series momentum on the sign of the basis, a key input for the carry trade. We find that this new technique applied to four major asset classes improved the Sharpe ratio of time-series momentum by approximately 0.17 net of fees. The improvement in performance is greater during recessions and, therefore, conditioning time-series momentum signals on the sign of the basis improves performance when it matters the most. Thus, the new approach has practical importance for investors and asset managers who attempt to improve their long-term performance without increasing downside risk during periods of market turbulence.},
  creationdate     = {2023-06-24T20:59:53},
  journal          = {The Journal of Alternative Investments},
  modificationdate = {2023-06-24T20:59:53},
  month            = {aug},
  publisher        = {Pageant Media {US}},
  timestamp        = {2020-08-05 09:17},
  year             = {2020},
}

@Article{Molyboga-2017,
  author               = {Molyboga, Marat},
  date                 = {2017},
  journaltitle         = {SSRN e-Print},
  title                = {Predicting Out-of-Sample Returns: Using Basis to Beat the Historical Average},
  url                  = {https://ssrn.com/abstract=3068321},
  abstract             = {This paper introduces an adaptive predictor that pools information across securities in four major asset classes (commodities, equities, fixed income and foreign exchange) while imposing restrictions on the sign and magnitude of coefficients in return forecasts. I demonstrate that the basis between spot and futures contracts predicts future returns across the asset classes. The predictor consistently beats the historical average, producing a median monthly out-of-sample R2, measured over the period between January 1986 and December 2016, of approximately 0.36 a value that is comparable to those of the best equity premium predictors considered in Campbell and Thompson (2008). A simple long-short strategy based on the new predictor delivers an out-of-sample alpha of 2.54.5\% per annum with respect to the asset pricing models considered and produces an out-of-sample Sharpe ratio of almost 0.5, which is particularly striking since the strategy is countercyclical. This performance is robust across sub-periods, market environments, portfolio construction methodologies and transaction costs. A cross-sectional structure analysis reveals that two observable common factors, constructed as equally-weighted indices of the bases of financial assets and commodities, are related to the short-term interest rate and the business cycle, respectively.},
  citeulike-article-id = {14487897},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3068321},
  creationdate         = {2023-06-24T20:59:53},
  groups               = {FrcstQWIM_ShortTerm, FrcstQWIM_MedLngTerm, FrcstQWIM_Bond, FrcstQWIM_Equity, FrcstQWIM_Test},
  modificationdate     = {2023-06-24T20:59:53},
  posted-at            = {2017-12-04 00:37:03},
  timestamp            = {2020-07-23 13:37},
}

@Article{Molyboga-et-al-2017,
  author               = {Molyboga, Marat and Baek, Seungho and Bilson, JohnF},
  date                 = {2017},
  journaltitle         = {Journal of Asset Management},
  title                = {Assessing hedge fund performance with institutional constraints: evidence from CTA funds},
  doi                  = {10.1057/s41260-017-0053-8},
  pages                = {1--19},
  abstract             = {Standard tests for persistence in hedge fund performance are not consistent with investment practices because they ignore performance reporting delay, overlook fund selection standards of institutional investors, and often use portfolios with too many funds. This paper introduces a set of tests based on a large-scale simulation framework and stochastic dominance methodology. These tests incorporate constraints that are standard practice in the institutional investment field. To illustrate this framework, we apply it to investigate momentum in the performance of hedge funds of the managed futures industry. We find persistence in performance of the top performing fund managers that is significant in statistical and economic terms. Our methodology extends the toolbox of performance persistence tests and results in findings that can be implemented by institutional investors.},
  citeulike-article-id = {14381619},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/s41260-017-0053-8},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1057/s41260-017-0053-8},
  creationdate         = {2023-06-24T20:59:53},
  groups               = {Manager selection, Hedge_Funds, Indic_FuturePerf},
  modificationdate     = {2023-06-24T20:59:53},
  posted-at            = {2017-06-23 15:39:02},
  publisher            = {Palgrave Macmillan UK},
  timestamp            = {2019-09-14 22:09},
}

@Article{Molyboga-LAhelec-2015,
  author               = {Molyboga, Marat and L'Ahelec, Christophe},
  date                 = {2015-07},
  journaltitle         = {SSRN e-Print},
  title                = {A Simulation-Based Methodology for Evaluating Hedge Fund Investments},
  url                  = {https://ssrn.com/abstract=2635537},
  abstract             = {This paper introduces a large scale simulation framework for evaluating hedge funds' investments subject to the realistic constraints of institutional investors. The method is customizable to the preferences and constraints of individual investors, including investment objectives, performance benchmarks, rebalancing period and the desired number of funds in a portfolio and can incorporate a large number of portfolio construction and fund selection approaches.

As a way to illustrate the methodology, we impose the framework on a subset of hedge funds in the managed futures space that contains 604 live and 1,323 defunct funds over the period 1993-2014. We then measure the out-of-sample performance of three hypothetical risk-parity portfolios and two hypothetical minimum risk portfolios and their marginal contributions to a typical 60-40 portfolio of stocks and bonds.

We find that an investment in managed futures improves an investor's performance regardless of portfolio construction methodology and that equal risk approaches are superior to minimum risk portfolios across all performance metrics considered in the study. Our paper is relevant for institutional investors in that it provides a robust and flexible framework for evaluating hedge fund investments given the specific preferences and constraints of individual investors.},
  citeulike-article-id = {13926606},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2635537},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2692096code923851.pdf?abstractid=2635537 and mirid=1},
  creationdate         = {2023-06-24T20:59:53},
  day                  = {26},
  groups               = {BenchmarkInvest, Hedge_Funds, Benchmark_AI},
  howpublished         = {Available at SSRN: ssrn.com/abstract=2635537},
  modificationdate     = {2023-06-24T20:59:53},
  owner                = {zkgst0c},
  posted-at            = {2016-02-06 04:24:14},
  timestamp            = {2020-07-23 13:37},
}

@Article{Molyboga-LAhelec-2016,
  author               = {Molyboga, Marat and L'Ahelec, Christophe},
  date                 = {2016-03},
  journaltitle         = {Journal of Asset Management},
  title                = {A simulation-based methodology for evaluating hedge fund investments},
  doi                  = {10.1057/jam.2016.3},
  issn                 = {1470-8272},
  abstract             = {This article introduces a large scale simulation framework for evaluating hedge funds' investments subject to the realistic constraints of institutional investors. The method is customizable to the preferences and constraints of individual investors, including investment objectives, performance benchmarks, rebalancing period and the desired number of funds in a portfolio and can incorporate a large number of portfolio construction and fund selection approaches. As a way to illustrate the methodology, we impose the framework on a subset of hedge funds in the managed futures space that contains 604 live and 1323 defunct funds over the period 1993 2014. We then measure the out-of-sample performance of three hypothetical risk-parity (RP) portfolios and two hypothetical minimum risk portfolios and their marginal contributions to a typical 60 40 portfolio of stocks and bonds. We find that an investment in managed futures improves an investor's performance regardless of portfolio construction methodology and that equal risk approaches are superior to minimum risk portfolios across all performance metrics considered in the study. Our article is relevant for institutional investors in that it provides a robust and flexible framework for evaluating hedge fund investments given the specific preferences and constraints of individual investors.},
  citeulike-article-id = {14030184},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2016.3},
  creationdate         = {2023-06-24T20:59:54},
  day                  = {24},
  groups               = {Hedge_Funds, Benchmark_AI},
  modificationdate     = {2023-06-24T20:59:54},
  owner                = {cristi},
  posted-at            = {2016-05-08 22:51:24},
  timestamp            = {2019-09-14 22:09},
}

@Article{Molyboga-LAhelec-2016a,
  author               = {Molyboga, Marat and L'Ahelec, Christophe},
  date                 = {2016-02},
  journaltitle         = {SSRN e-Print},
  title                = {Portfolio Management with Drawdown-Based Measures},
  url                  = {https://ssrn.com/abstract=2729522},
  abstract             = {This paper analyzes the portfolio management implications of using drawdown-based measures in allocation decisions. We introduce modified conditional expected drawdown (MCED), a new risk measure that is derived from portfolio drawdowns, or peak-to-trough losses, of demeaned constituents. We show that MCED exhibits the attractive properties of coherent risk measures that are present in conditional expected drawdown (CED) but lacking in the historical maximum drawdown (MDD) commonly used in the industry. This paper introduces a robust block bootstrap approach to calculating CED, MCED and marginal contributions from portfolio constituents. First, we show that MCED is less sensitive to sample error than CED and MDD. Second, we evaluate several drawdown-based minimum risk and equal-risk allocation approaches within the large scale simulation framework of Molyboga and L'Ahelec (2016) using a subset of hedge funds in the managed futures space that contains 613 live and 1,384 defunct funds over the 1993-2015 period. We find that the MCED-based equal-risk approach dominates the other drawdown-based techniques but does not consistently outperform the simple equal volatility-adjusted approach. This finding highlights the importance of carefully accounting for sample error, as reported in DeMiquel et al (2009), and cautions against over-relying on drawdown-based measures in portfolio management.},
  citeulike-article-id = {14039374},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2729522},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2733681code923851.pdf?abstractid=2729522 and mirid=1},
  creationdate         = {2023-06-24T20:59:54},
  day                  = {10},
  modificationdate     = {2023-06-24T20:59:54},
  owner                = {cristi},
  posted-at            = {2016-05-21 16:23:22},
  timestamp            = {2020-07-23 13:37},
}

@Article{Molyboga-LAhelec-2017,
  author               = {Molyboga, Marat and L'Ahelec, Christophe},
  date                 = {2017},
  journaltitle         = {The Journal of Alternative Investments},
  title                = {Portfolio Management with Drawdown-Based Measures},
  doi                  = {10.3905/jai.2017.19.3.075},
  number               = {3},
  pages                = {75--89},
  volume               = {19},
  abstract             = {This article analyzes the portfolio management implications of using drawdown-based measures in allocation decisions. The authors introduce modified conditional expected drawdown (MCED), a new risk measure that is derived from portfolio drawdowns, or peak-to-trough losses, of demeaned constituents. They show that MCED exhibits the attractive properties of coherent risk measures that are present in conditional expected drawdown (CED) but are lacking in the historical maximum drawdown (MDD) commonly used in the industry. This article introduces a robust block bootstrap approach to calculating CED, MCED, and marginal contributions from portfolio constituents. First, the authors show that MCED is less sensitive to sample error than CED and MDD. Second, they evaluate several drawdown-based minimum risk and equal-risk allocation approaches within the large-scale simulation framework of Molyboga and L'Ahelec via a subset of hedge funds in the managed futures space that contains 613 live and 1,384 defunct funds over the 1993-2015 period. The authors find that the MCED-based equal-risk approach dominates the other drawdown-based techniques but does not consistently outperform the simple equal volatility-adjusted approach. This finding highlights the importance of carefully accounting for sample error, as reported by DeMiquel et al., and cautions against overreliance on drawdown-based measures in portfolio management.},
  citeulike-article-id = {14270142},
  citeulike-linkout-0  = {http://www.iijournals.com/doi/abs/10.3905/jai.2017.19.3.075},
  creationdate         = {2023-06-24T20:59:54},
  modificationdate     = {2023-06-24T20:59:54},
  posted-at            = {2017-02-01 23:43:32},
  timestamp            = {2019-09-14 22:09},
}

@Article{Molyboga-2020,
  author           = {Marat Molyboga},
  date             = {2020},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {A Modified Hierarchical Risk Parity Framework for Portfolio Management},
  number           = {3},
  pages            = {128-139},
  url              = {https://jfds.pm-research.com/content/early/2020/07/03/jfds.2020.1.038},
  volume           = {2},
  abstract         = {This article introduces a modified hierarchical risk parity (MHRP) approach that extends the HRP approach by incorporating three intuitive elements commonly used by practitioners. The new approach (1) replaces the sample covariance matrix with an exponentially weighted covariance matrix with Ledoit-Wolf shrinkage; (2) improves diversification across portfolio constituents both within and across clusters by relying on an equal volatility, rather than an inverse variance, allocation approach; and (3) improves diversification across time by applying volatility targeting to portfolios. The author examines the impact of the enhancements on portfolios of commodity trading advisors within a large-scale Monte Carlo simulation framework that accounts for the realistic constraints of institutional investors. The author finds a striking improvement in the out-of-sample Sharpe ratio of 50\%, on average, along with a reduction in downside risk.},
  creationdate     = {2023-06-24T20:59:54},
  modificationdate = {2023-06-24T20:59:54},
  timestamp        = {2020-08-05 09:42},
}

@Article{Molyboga-et-al-2020a,
  author           = {Marat Molyboga and Larry Swedroe and Junkai Qian},
  date             = {2020-09},
  journaltitle     = {The Journal of Portfolio Management},
  title            = {Short-Term Trend: A Jewel Hidden in Daily Returns},
  doi              = {10.3905/jpm.2020.1.186},
  number           = {19},
  pages            = {154-167},
  volume           = {47},
  abstract         = {This article examines the performance of time-series momentum strategies using daily returns for 78 futures markets across four major asset classes between January 1985 and December 2017. The authors find that the 252-day, 63-day, and 21-day momentum strategies perform similarly to the previously documented 12-month, 3-month, and 1-month momentum strategies, respectively. The performance is stronger with volatility-based position sizing, robust to implementation considerations such as a one-day gap between signal generation and execution, and persistent across asset classes and subperiods. The authors introduce a shorter duration momentum strategy with a weekly rebalancing frequency, which cannot be replicated using monthly returns. The authors find that the short-term strategy is a strong diversifier to the longer-term strategies, but the benefit may be reduced, or even completely offset, if the quality of trade execution is poor. The authors also find that the positive contribution of short-term momentum is driven by its superior diversifying characteristics rather than by the rebalancing frequency effect.},
  creationdate     = {2023-06-24T20:59:54},
  modificationdate = {2023-06-24T20:59:54},
  publisher        = {Pageant Media {US}},
  timestamp        = {2020-09-28 23:07},
}

@Article{Molyboga-2020a,
  author           = {Marat Molyboga},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Back to Basis: A Universal Return Predictor Across Asset Classes},
  doi              = {10.2139/ssrn.3690628},
  abstract         = {This paper shows analytically that the basis between spot and futures contracts contains information about future returns of securities across the asset classes of commodities, equity indices, fixed income and foreign exchange. The bases in commodities are positively correlated with a leading indicator of the business cycle whereas the bases in the financial assets are negatively related to the short-term rate. The return predictability of the basis can be captured with a simple multi-asset long-short strategy which produces an out-of-sample Sharpe ratio of 0.5 and an alpha of 2.5\%-4.5\% per annum with respect to commonly used asset pricing models. Specifically, the analysis includes five Fama-French Factors, a bond index and futures risk premia of multi-asset momentum, value, time-series momentum, and four asset-specific carry factors. The strategy performance is counter-cyclical and robust to transaction costs.},
  creationdate     = {2023-06-24T20:59:54},
  modificationdate = {2023-06-24T20:59:54},
  publisher        = {Elsevier {BV}},
  timestamp        = {2020-10-31 16:22},
}

@Article{McGee-Olmo-2022,
  author           = {Richard J. McGee and Jose Olmo},
  date             = {2022-07},
  journaltitle     = {Quantitative Finance},
  title            = {Optimal characteristic portfolios},
  doi              = {10.1080/14697688.2022.2094282},
  number           = {10},
  pages            = {1853-1870},
  volume           = {22},
  abstract         = {Characteristic-sorted portfolios are the workhorses of modern empirical finance, deployed widely to evaluate anomalies and construct asset pricing models. We propose a new method for their estimation that is simple to compute, makes no ex-ante assumption on the nature of the relationship between the characteristic and returns, and does not require ad hoc selections of percentile breakpoints or portfolio weighting schemes. Characteristic portfolio weights are implied directly from data, through maximizing a Mean-Variance objective function with mean and variance estimated non-parametrically from the cross-section of assets. To illustrate the method, we evaluate the size, value and momentum anomalies and find overwhelming empirical evidence of the outperformance of our methodology compared to standard methods for constructing characteristic-sorted portfolios.},
  creationdate     = {2023-06-24T21:00:26},
  modificationdate = {2023-06-24T21:00:26},
  publisher        = {Informa {UK} Limited},
}

@Article{Olmo-2021,
  author           = {Jose Olmo},
  date             = {2021},
  journaltitle     = {Quantitative Finance},
  title            = {Optimal portfolio allocation and asset centrality revisited},
  doi              = {10.1080/14697688.2021.1937298},
  number           = {9},
  pages            = {1475-1490},
  volume           = {21},
  abstract         = {This paper revisits the relationship between eigenvector asset centrality and optimal asset allocation in a minimum variance portfolio. We show that the standard definition of eigenvector centrality is misleading when the adjacency matrix in a network can take negative values. This is, for example, the case when the network topology is induced by the correlation matrix between assets in a portfolio. To correct for this, we introduce the concept of positive and negative eigenvector centrality. Our results show that the loss function associated to the minimum variance portfolio is positively/negatively related to the positive and negative eigenvector centrality under short-selling constraints but cannot be generalized beyond that. Furthermore, in contrast to what is claimed in the related literature, this relationship does not imply any monotonic relationship between the centrality of an asset and its optimal portfolio allocation. These theoretical insights are illustrated empirically in a portfolio allocation exercise with assets from U.S. and U.K. financial markets.},
  creationdate     = {2023-06-24T21:00:26},
  modificationdate = {2023-06-24T21:00:26},
  publisher        = {Informa {UK} Limited},
}

@Article{Laborda-Olmo-2021,
  author           = {Ricardo Laborda and Jose Olmo},
  date             = {2021},
  journaltitle     = {Journal of Financial Econometrics},
  title            = {Hedging Demand in Long-Term Asset Allocation with an Application to Carry Trade Strategies},
  doi              = {10.1093/jjfinec/nbaa034},
  abstract         = {We derive a closed-form expression for the mean and marginal hedging demand on risky assets in long-term asset allocation problems for individuals with constant relative risk aversion preferences. Our parametric portfolio policy rule accommodates an arbitrarily large number of state variables for predicting the state of nature and number of assets in the portfolio. The closed-form expression for the hedging demand is exact under polynomial specifications of the portfolio policy rule and a suitable approximation for unknown smooth parametric portfolio policy rules using Taylor expansions. The hedging demand on risky assets depends positively on the predictability of the risky asset and the persistence of the predictors, and negatively on the degree of investor's relative risk aversion. We illustrate these insights empirically for a basket of currencies by showing the outperformance of rebalancing carry trade strategies over different investment horizons against a short-term (myopic) portfolio.},
  creationdate     = {2023-06-24T21:00:26},
  modificationdate = {2023-06-24T21:00:26},
  publisher        = {Oxford University Press ({OUP})},
  timestamp        = {2021-01-17 18:45},
}

@Article{McGee-Olmo-2020,
  author           = {Richard McGee and Jose Olmo},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Optimal Characteristic Portfolios},
  doi              = {10.2139/ssrn.3638177},
  abstract         = {Characteristic-sorted portfolios are the workhorses of modern empirical finance, deployed widely to evaluate anomalies and construct asset pricing models. We propose a new method for their estimation that is simple to compute; makes no ex-ante assumption on the nature of the relationship between the characteristic and returns; and does not require ad hoc selections of percentile breakpoints or portfolio weighting schemes. Characteristic portfolio weights are implied directly from data, through maximizing the expected value of a mean-variance utility function estimated non-parametrically over the cross section of the assets. To illustrate the method we use it to evaluate the size, value and momentum anomalies.},
  creationdate     = {2023-06-24T21:00:26},
  modificationdate = {2023-06-24T21:00:26},
  timestamp        = {2020-08-11 00:21},
}

@Article{Laborda-Olmo-2017,
  author               = {Laborda, Ricardo and Olmo, Jose},
  date                 = {2017-10},
  journaltitle         = {International Journal of Forecasting},
  title                = {Optimal asset allocation for strategic investors},
  doi                  = {10.1016/j.ijforecast.2017.05.003},
  issn                 = {0169-2070},
  number               = {4},
  pages                = {970--987},
  volume               = {33},
  abstract             = {This paper studies optimal asset allocation for investors over multiple investment horizons. Rather than first model the various features of the conditional return distribution and subsequently characterize the portfolio choice, we focus directly on the dependence of the portfolio weights on the predictor variables through a linear parametric portfolio policy rule. This characterization allows us to apply GMM estimation and testing methods to sample analogues of the multiperiod Euler equations that characterize our optimal portfolio choice. Our model accommodates an arbitrarily large number of assets in the portfolio and state variables in the information set. The empirical results for a portfolio of stocks, bonds and cash provide ample support to the linear specification of the portfolio weights and reveal significant differences between myopic (one-period) and strategic (long-term) optimal portfolio allocations.},
  citeulike-article-id = {14412288},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2017.05.003},
  creationdate         = {2023-06-24T21:00:26},
  groups               = {SAA, FcstQWIM_Bond},
  modificationdate     = {2023-06-24T21:00:26},
  posted-at            = {2017-08-10 23:29:15},
  timestamp            = {2019-04-02 15:53},
}

@Article{Olmo-SansoNavarro-2012,
  author               = {Olmo, Jose and Sanso-Navarro, Marcos},
  date                 = {2012-08},
  journaltitle         = {Journal of Banking and Finance},
  title                = {Forecasting the performance of hedge fund styles},
  doi                  = {10.1016/j.jbankfin.2012.04.016},
  issn                 = {0378-4266},
  number               = {8},
  pages                = {2351--2365},
  volume               = {36},
  abstract             = {This article predicts the relative performance of hedge fund investment styles using time-varying conditional stochastic dominance tests. These tests allow for the construction of dynamic trading strategies based on nonparametric density forecasts of hedge fund returns. During the recent financial turmoil, our tests predict a superior performance for the Global Macro investment style compared with the other strategies of 'Directional Traders'. The Dedicated Short Bias investment style is stochastically dominated by the other directional styles. These results are confirmed by simple nonparametric tests constructed from realized excess returns. Further, by utilizing a cross-validation method for optimal bandwidth parameter selection, we discover the factors that have predictive power regarding the density of hedge fund returns. We observe that different factors have forecasting power for different regions of the returns distribution and, more importantly, that the Fung and Hsieh factors have power not only for describing the risk premium but also, if appropriately exploited, for density forecasting. This article predicts the relative performance of directional hedge fund styles. We apply time-varying conditional stochastic dominance tests. Our tests predict a superior performance of the Global Macro style. We also find out which factors have predictive power for the density of returns.},
  citeulike-article-id = {10784474},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jbankfin.2012.04.016},
  creationdate         = {2023-06-24T21:00:26},
  groups               = {Hedge_Funds, FrcstQWIM_Other},
  modificationdate     = {2023-06-24T21:00:26},
  owner                = {cristi},
  posted-at            = {2016-07-01 11:50:38},
  timestamp            = {2019-09-14 22:10},
}

@Article{Papenbrock-et-al-2021a,
  author           = {Jochen Papenbrock and Peter Schwendner and Philipp Sandner},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Can Adaptive Seriational Risk Parity Tame Crypto Portfolios?},
  doi              = {10.2139/ssrn.3877143},
  abstract         = {As cryptocoins are not tied to fundamental values or to investor protection regulation, their price dynamics is unhinged in both directions.
In institutional asset management of conventional asset classes, target volatility concepts and dynamic allocation heuristics are popular to improve the robustness of portfolio.
Can similar techniques also be used to construct delevered and diversified portfolios of crypto assets? A robust candidate approach for allocation is Hierarchical Risk Parity (HRP), as it incorporates a filtered correlation structure and is less sensitive to noise than quadratic optimization, as shown in several studies. Recent publications have extended the concept of HRP in several directions. We compare some of these extensions to determine which variant is most useful for constructing crypto baskets. We find that a particular type of adaptive HRP strategy outperforms other extensions on a risk-adjusted basis, leading us to a deeper investigation of the changing nature of correlation structures between cryptos - both quantitatively and visually. We find that structural breaks in crypto correlations are prevalent and that the best-fitting hierarchical cluster representations change over time, which is only captured by distance matrix-based adaptive HRP approaches.},
  creationdate     = {2023-06-24T21:01:53},
  modificationdate = {2023-06-24T21:01:53},
  publisher        = {Elsevier {BV}},
}

@Article{Papenbrock-et-al-2021,
  author           = {Jochen Papenbrock and Peter Schwendner and Markus Jaeger and Stephan Krugel},
  date             = {2021-03},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {Matrix Evolutions: Synthetic Correlations and Explainable Machine Learning for Constructing Robust Investment Portfolios},
  doi              = {10.3905/jfds.2021.1.056},
  number           = {2},
  pages            = {51--69},
  volume           = {3},
  abstract         = {In this article, the authors present a novel and highly flexible concept to simulate correlation matrixes of financial markets. It produces realistic outcomes regarding stylized facts of empirical correlation matrixes and requires no asset return input data. The matrix generation is based on a multiobjective evolutionary algorithm, so the authors call the approach matrix evolutions. It is suitable for parallel implementation and can be accelerated by graphics processing units and quantum-inspired algorithms. The approach is useful for backtesting, pricing, and hedging correlation-dependent investment strategies and financial products. Its potential is demonstrated in a machine learning case study for robust portfolio construction in a multi-asset universe: An explainable machine learning program links the synthetic matrixes to the portfolio volatility spread of hierarchical risk parity versus equal risk contribution.},
  creationdate     = {2023-06-24T21:01:54},
  modificationdate = {2023-06-24T21:01:54},
  publisher        = {Pageant Media {US}},
}

@Article{Papenbrock-Schwendner-2015,
  author               = {Papenbrock, Jochen and Schwendner, Peter},
  date                 = {2015},
  journaltitle         = {Financial Markets and Portfolio Management},
  title                = {Handling risk-on/risk-off dynamics with correlation regimes and correlation networks},
  doi                  = {10.1007/s11408-015-0248-2},
  number               = {2},
  pages                = {125--147},
  volume               = {29},
  abstract             = {In this paper, we present a framework for detecting distinct correlation regimes and analyzing the emerging state dependences for a multi-asset futures portfolio from 1998 to 2013. These correlation regimes have been significantly different since the financial crisis of 2008 than they were previously; cluster tracking shows that asset classes are now less separated.

We identify distinct  risk-on and risk-off assets with the help of correlation networks. In addition to visualizing, we quantify these observations using suitable metrics for the clusters and correlation networks. The framework will be useful for financial risk management, portfolio construction, and asset allocation.},
  citeulike-article-id = {13934872},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11408-015-0248-2},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s11408-015-0248-2},
  creationdate         = {2023-06-24T21:01:54},
  groups               = {Networks and investment management, Regime_Invest, Regime_Identif},
  modificationdate     = {2023-06-24T21:01:54},
  owner                = {cristi},
  posted-at            = {2016-02-18 04:27:47},
  publisher            = {Springer US},
}

@Article{Papenbrock-et-al-2020,
  author               = {Papenbrock, Jochen and Schwendner, Peter and Markus Jaeger and Stephan Krugel},
  date                 = {2020},
  journaltitle         = {SSRN e-Print},
  title                = {Matrix Evolutions: Synthetic Correlations and Explainable Machine Learning for Constructing Robust Investment Portfolios},
  url                  = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3663220},
  abstract             = {In this paper we present a novel and highly flexible method to simulate correlation matrices of financial markets. It produces realistic outcomes regarding stylized facts of empirical correlation matrices and requires no asset return input data. The matrix generation is based on a multi-objective evolutionary algorithm so we call the approach "Matrix Evolutions". It is suitable for parallel implementation and can be accelerated by graphics processing units (GPUs) and quantum-inspired algorithms. The approach can be used for pricing, hedging and trading correlation-based financial products. We demonstrate the potential of Matrix Evolutions in a machine learning case study for robust portfolio construction in a multi-asset universe. In this study we organize an explainable machine learning program to establish a link from the simulated matrices to relative investment performance. The training data consists of the synthetic matrices produced by Matrix Evolutions and an automatic labeling by Monte-Carlo simulation of the relative investment performance of the following two approaches for portfolio construction: the novel Hierarchical Risk Parity approach by Lopez de Prado (2016b) which is based on representation learning and the traditional equal risk contribution approach.},
  citeulike-article-id = {13926386},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2702275},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2702275code2340836.pdf?abstractid=2702275 and mirid=1},
  creationdate         = {2023-06-24T21:01:54},
  day                  = {11},
  howpublished         = {Available at SSRN: ssrn.com/abstract=2702275},
  modificationdate     = {2023-06-24T21:01:54},
  owner                = {zkgst0c},
  posted-at            = {2016-02-05 21:30:45},
}

@Article{Zareei-2021,
  author           = {Abalfazl Zareei},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Optimal versus Naive Diversification: False Discoveries, Transaction Costs and Machine Learning},
  doi              = {10.2139/ssrn.3346139},
  abstract         = {This paper shows that sophisticated diversification strategies never underperform the 1/N rule when adjusting for multiple testing; however, their edge is severely undermined by transaction costs. As a way forward, this paper provides a machine learning approach for ex-ante strategy selection. By linking the characteristics of investment scenarios to the out-of-sample performance of strategies, the algorithm never underperforms the 1/N rule, even in the presence of relatively high transaction costs.},
  creationdate     = {2023-06-24T21:02:40},
  modificationdate = {2023-06-24T21:02:40},
  publisher        = {Elsevier {BV}},
}

@Article{Zareei-2015,
  author               = {Zareei, Abalfazl},
  date                 = {2015-06},
  journaltitle         = {SSRN e-Print},
  title                = {Network Centrality, Failure Prediction and Systemic Risk},
  url                  = {https://ssrn.com/abstract=2613282},
  abstract             = {A financial market can be expressed in a network structure where the stocks resides as nodes and the links account for returns correlation. Centrality measure in the financial network structure captures firms' embeddedness and connectivity in the capital market structure. This paper investigates firms' centrality in the financial network as an explanatory variable in corporate failure prediction and also as a systemic risk measure. First, when analyzing the CDS spreads, I find peripheral firms in the network to have higher average CDS spreads and higher propensity to CDS jump events. Second, centrality is found to increase the explanatory power of default prediction models and moreover, it is negatively related to failure and bankruptcy probability. This implies that peripheral firms in the network are more likely to fail. Finally, examining the out-of-sample performance of centrality as a systemic risk measure, I find that centrality distinguish correctly the firms that suffered a higher loss during the 2007/2008 crisis period.},
  citeulike-article-id = {14148615},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2613282},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2638911code2238434.pdf?abstractid=2613282 and mirid=1},
  creationdate         = {2023-06-24T21:02:41},
  day                  = {2},
  groups               = {Networks and investment management},
  modificationdate     = {2023-06-24T21:02:41},
  owner                = {cristi},
  posted-at            = {2016-09-28 20:16:59},
}

@Article{Zareei-2015a,
  author               = {Zareei, Abalfazl},
  date                 = {2015-12},
  journaltitle         = {The Journal of Network Theory in Finance},
  title                = {Network centrality, failure prediction and systemic risk},
  doi                  = {10.21314/jntf.2015.013},
  issn                 = {2055-7795},
  number               = {4},
  pages                = {73--97},
  volume               = {1},
  abstract             = {A financial market can be expressed as a network structure in which the stocks reside as nodes and the links account for returns correlation. The centrality measure in the financial network structure captures firms' embeddedness and connectivity in the capital market structure. This paper investigates firms' centrality in the financial network as an explanatory variable in corporate-failure prediction as well as a measure of firms' systemic importance. First, when analyzing credit default swap (CDS) spreads, we find that peripheral firms in the network have higher average CDS spreads and a higher propensity for CDS jump events. Second, centrality is found to increase the explanatory power of default prediction models, and, moreover, it is negatively related to failure and bankruptcy probability. This implies that peripheral firms in the network are more likely to fail. Finally, by examining the out-of-sample performance of centrality as a measure of systemic importance, we find that centrality correctly distinguishes the firms that suffered higher losses during the 2007-8 crisis period.},
  citeulike-article-id = {14148619},
  citeulike-linkout-0  = {http://dx.doi.org/10.21314/jntf.2015.013},
  creationdate         = {2023-06-24T21:02:41},
  groups               = {Networks and investment management},
  modificationdate     = {2023-06-24T21:02:41},
  owner                = {cristi},
  posted-at            = {2016-09-28 20:22:18},
}

@PhdThesis{Zareei-2016,
  author               = {Zareei, Abalfazl},
  date                 = {2016},
  institution          = {Universidad Carlos III de Madrid},
  title                = {New insights in portfolio selection modeling},
  abstract             = {Recent advancements in the field of network theory commence a new line of developments in portfolio selection techniques that stands on the ground of perceiving financial market as a network with assets as nodes and links accounting for various types of relationships among financial assets. In the first chapter, we model the shock propagation mechanism among assets via network theory and provide an approach to construct well-diversified portfolios that are resilient to shock propagation and contagion issues in the volatile and crisis periods. The second chapter analyzes the influence of the hedging network among assets on the portfolio diversification attributes. Building on the hedging network perspective of the market, we propose a network centrality measure to find stocks that are most suitable to form a well-diversified portfolio. The results clearly shows that our diversification strategy performs better than the conventional diversification strategies both in-sample and out-of-sample. In the third chapter, we analyze the market network constructed by adopting assets as nodes and their returns' correlation as links. We theoretically show that there is a negative relationship between the centrality of assets in such a network and the weights assigned to them in the Markowitz prescription. Based on our theoretical findings, we propose a portfolio selection strategy that out-performs well-known benchmarks while presenting positive and significant Carhart alphas.},
  citeulike-article-id = {14148622},
  creationdate         = {2023-06-24T21:02:41},
  groups               = {BenchmarkInvest},
  modificationdate     = {2023-06-24T21:02:41},
  owner                = {cristi},
  posted-at            = {2016-09-28 20:26:31},
}

@Article{Peralta-Zareei-2014,
  author               = {Peralta, Gustavo and Zareei, Abalfazl},
  date                 = {2014-04},
  journaltitle         = {SSRN e-Print},
  title                = {A Network Approach to Portfolio Selection},
  url                  = {https://ssrn.com/abstract=2430309},
  abstract             = {A financial market can be conceived as a network with stocks as nodes and links accounting for returns' correlation. We theoretically prove the negative relation between the centrality of each individual stock in the market network and the weight it receives in an optimal wealth allocation. Therefore, optimal portfolios assign wealth toward the periphery of the market network in order to reduce the influence of central assets. Next, we empirically investigate the major determinants of stock's centrality and also its relative stability. We show that both, financial and market variables are fundamental drivers of stocks' centrality. Our results also make evident the increased stability of those stocks that are centrally placed in the network. Finally, we explore by means of in-sample and out-of-sample analysis, how network-based investment strategies could be applied to enhance portfolio's performance. Our out-of-sample experiments show positive and significant Jensen's alphas that are robust to diverse specifications. The major contribution of the paper regards to the employment of the stock market network as a useful device to simplify the portfolio selection process via targeting a group of stocks belonging to certain region of the structure.},
  citeulike-article-id = {13997408},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2430309},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2574234code2225550.pdf?abstractid=2430309 and mirid=1},
  creationdate         = {2023-06-24T21:02:41},
  day                  = {29},
  groups               = {Networks and investment management},
  modificationdate     = {2023-06-24T21:02:41},
  owner                = {cristi},
  posted-at            = {2016-04-05 04:51:43},
}

@Article{Peralta-Zareei-2016,
  author               = {Peralta, Gustavo and Zareei, Abalfazl},
  date                 = {2016-09},
  journaltitle         = {Journal of Empirical Finance},
  title                = {A network approach to portfolio selection},
  doi                  = {10.1016/j.jempfin.2016.06.003},
  issn                 = {0927-5398},
  pages                = {157--180},
  volume               = {38},
  abstract             = {Low-central stocks receive higher weights in optimal allocation. Financial and market variables are major drivers of stocks' centrality. We construct a network-based investment strategy that performs well out-of-sample. Our network-based strategy results in positive and significant Carhart's alphas. In this study, a financial market is conceived as a network where the securities are nodes and the links account for returns' correlations. We theoretically prove the negative relationship between the centrality of assets in this financial market network and their optimal weights under the Markowitz framework. Therefore, optimal portfolios overweight low-central securities to avoid the large variances that result when highly influential stocks are included in the investor's opportunity set. Next, we empirically investigate the major financial and market determinants of stock's centralities. The evidence indicates that highly central nodes tend to coincide with older, larger-cap, cheaper and financially riskier securities. Finally, we explore by means of in-sample and out-of-sample analysis the extent to which the structure of the stock market network can be employed to improve the portfolio selection process. We propose a network-based investment strategy that outperforms well-known benchmarks while presenting positive and significant Carhart alphas. The major contribution of the paper is to employ the financial market network as a useful device to improve the portfolio selection process by targeting a group of assets according to their centrality.},
  citeulike-article-id = {14148613},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jempfin.2016.06.003},
  creationdate         = {2023-06-24T21:02:41},
  groups               = {Networks and investment management, Invest_Network},
  modificationdate     = {2023-06-24T21:02:41},
  owner                = {cristi},
  posted-at            = {2016-09-28 20:16:03},
}

@Article{Zareei-2019,
  author           = {Zareei, Abalfazl},
  date             = {2019-12},
  journaltitle     = {Journal of Banking \& Finance},
  title            = {Network origins of portfolio risk},
  doi              = {10.1016/j.jbankfin.2019.105663},
  issn             = {0378-4266},
  pages            = {105663},
  url              = {https://linkinghub.elsevier.com/retrieve/pii/S0378426619302389},
  urldate          = {2019-12-03},
  volume           = {109},
  abstract         = {This paper shows that shocks, in the presence of asymmetric propagation structures, diminish investors diversification benefits. First, we construct an interdependency network with assets as nodes, and links corresponding to cross-dependency in returns. Second, we show that higher heterogeneity in the structure of the network increases portfolio risk. In particular, diversification among assets with star-like network structures, where a central asset cross-affects other assets in the portfolio, results in the lowest level of diversification benefits. Finally, we empirically demonstrate that two distinct datasets of U.S. industries and international stock markets greatly resemble star-like network structures.},
  creationdate     = {2023-06-24T21:02:41},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T21:02:41},
}

@Article{Raffinot-2016b,
  author               = {Raffinot, Thomas},
  date                 = {2016},
  journaltitle         = {SSRN e-Print},
  title                = {Investing Through Economic Cycles with Ensemble Machine Learning Algorithms},
  url                  = {https://ssrn.com/abstract=2785583},
  abstract             = {Ensemble machine learning algorithms, referred to as random forest (Breiman (2001)) and as boosting (Schapire (1990)), are applied to quickly and accurately detect economic turning points in the United States and in the euro area. The two key features of those algorithms are their abilities to entertain a large number of predictors and to perform estimation and variable selection simultaneously. The real-time ability to nowcast economic turning points is gauged. To assess the value of the models, profit maximization measures are employed in addition to more standard criteria. When comparing predictive accuracy and profit measures, the model confidence set procedure proposed by Hansen et al. (2011) is applied to avoid data snooping. The investment strategies based on the models achieve impressive risk-adjusted returns: macroeconomists can get rich nowcasting economic turning points.},
  citeulike-article-id = {14146764},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2785583},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2785583code2270025.pdf?abstractid=2785583 and mirid=1},
  creationdate         = {2023-06-24T21:04:19},
  day                  = {28},
  groups               = {Machine learning and investment strategies, ML_InvestSelect},
  modificationdate     = {2023-06-24T21:04:19},
  owner                = {cristi},
  posted-at            = {2016-09-26 22:33:00},
}

@Article{Raffinot-2016a,
  author               = {Raffinot, Thomas},
  date                 = {2016},
  journaltitle         = {SSRN e-Print},
  title                = {Can Macroeconomists Get Rich Nowcasting Economic Turning Points with a Simple Machine-Learning Algorithm?},
  url                  = {https://ssrn.com/abstract=2545256},
  abstract             = {To nowcast economic turning points, probabilistic indicators are created from a simple and transparent machine-learning algorithm known as Learning Vector Quantization (LVQ). The real-time ability of the indicators to quickly and accurately detect economic turning points in the United States and in the euro area is gauged. To assess the value of the indicators, profit maximization measures based on trading strategies are employed in addition to more standard criteria. When comparing predictive accuracy and profit measures, the model confidence set procedure proposed by Hansen et al. (2011) is applied to avoid data snooping. A substantial improvement in profit measures over the benchmark is found: macroeconomists can get rich nowcasting economic turning points.},
  citeulike-article-id = {14146765},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2545256},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2817407code2270025.pdf?abstractid=2545256 and mirid=1},
  creationdate         = {2023-06-24T21:04:19},
  day                  = {7},
  modificationdate     = {2023-06-24T21:04:19},
  owner                = {cristi},
  posted-at            = {2016-09-26 22:36:04},
}

@Article{Raffinot-2015,
  author               = {Raffinot, Thomas},
  date                 = {2014-10},
  journaltitle         = {SSRN e-Print},
  title                = {A Simple and Efficient Macroeconomic Framework for Asset Allocation},
  url                  = {https://ssrn.com/abstract=2504378},
  abstract             = {The recent financial and economic crisis highlighted the need for a better understanding of the complex relationship between financial markets and the macroeconomy. Many so-called diversified portfolios performed very badly and faced widespread criticism afterwards. To alleviate the difficulties encountered in the context of the Great Recession, this paper attempts to provide a simple and efficient macroeconomic framework for asset allocation.

To this aim, this paper points out the importance of the growth cycle introduced by Mintz (1974) for euro and dollar-based investors. Moreover, Anas and Ferrara (2004) refine the description of different economic phases by jointly considering the classical business cycle and the growth cycle. This article emphasizes how this economic cyclical framework can improve strategic asset allocation choices and that dynamic macro-based regime-switching asset allocations achieve superior risk adjusted returns than static alternative in the euro area and in the United States.},
  citeulike-article-id = {13978621},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2504378},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2601269code2270025.pdf?abstractid=2504378 and mirid=1},
  creationdate         = {2023-06-24T21:04:19},
  day                  = {2},
  modificationdate     = {2023-06-24T21:04:19},
  owner                = {cristi},
  posted-at            = {2016-03-13 00:22:49},
}

@Article{Raffinot-2017,
  author               = {Raffinot, Thomas},
  date                 = {2017},
  journaltitle         = {SSRN e-Print},
  title                = {Time-Varying Risk Premiums and Economic Cycles},
  doi                  = {10.2139/ssrn.2949914},
  issn                 = {1556-5068},
  abstract             = {Asset returns are not correlated with the business cycle but are primarily caused by the economic cycles. To validate this claim, economic cycles are first rigorously defined, namely the classical business cycle and the growth cycle, better known as the output gap. The description of different economic phases is refined by jointly considering both economic cycles. It improves the classical analysis of economic cycles by considering sometimes two distinct phases and sometimes four distinct phases. The theoretical influence of economic cycles on time-varying risk premiums is then explained based on two key economic concepts: nominal GDP and adaptive expectations. Simple dynamic investment strategies confirm the importance of economical cycles, especially the growth cycle, for euro and dollar-based investors. At last, this economic cyclical framework can improve strategic asset allocation choices.},
  citeulike-article-id = {14337053},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.2949914},
  creationdate         = {2023-06-24T21:04:22},
  groups               = {RiskRet_BusCycle},
  modificationdate     = {2023-06-24T21:04:22},
  posted-at            = {2017-04-13 20:17:53},
}

@Article{Raffinot-2018a,
  author           = {Raffinot, Thomas},
  date             = {2018-08-23},
  journaltitle     = {SSRN e-Print},
  title            = {The Hierarchical Equal Risk Contribution Portfolio},
  url              = {https://ssrn.com/abstract=3237540},
  abstract         = {Building upon the fundamental notion of hierarchy, the "Hierarchical Risk Parity" (HRP) and the "Hierarchical Clustering based Asset Allocation" (HCAA), the Hierarchical Equal Risk Contribution Portfolio (HERC) aims at diversifying capital allocation and risk allocation. HERC merges and enhances the machine learning approach of HCAA and the Top-Down recursive bisection of HRP. In more detail, the modified Top-Down recursive division is based on the shape of dendrogram, follows an Equal Risk Contribution allocation and is extended to downside risk measures such as conditional value at risk (CVaR) and Conditional Drawdown at Risk (CDaR). The out-of-sample performances of hierarchical clustering based portfolios are evaluated across two empirical datasets, which differ in terms of number of assets and composition of the universe (multi-assets and individual stocks). Empirical results highlight that HERC Portfolios based on downside risk measures achieve statistically better risk-adjusted performances, especially those based on the CDaR.},
  creationdate     = {2023-06-24T21:04:24},
  day              = {23},
  f1000-projects   = {QuantInvest},
  groups           = {Invest_Risk, ML_Network_QWIM, ML_InvestSelect},
  modificationdate = {2023-06-24T21:04:24},
}

@Article{Raffinot-Benoit-2018,
  author               = {Raffinot, Thomas and Sylvain Benoit},
  date                 = {2018},
  journaltitle         = {SSRN e-Print},
  title                = {Investing Through Economic Cycles with Ensemble Machine Learning Algorithms},
  url                  = {https://ssrn.com/abstract=2785583},
  abstract             = {Ensemble machine learning algorithms, referred to as random forest (Breiman (2001)) and as boosting (Schapire (1990)), are applied to quickly and accurately detect economic turning points in the United States and in the euro area. The two key features of those algorithms are their abilities to entertain a large number of predictors and to perform estimation and variable selection simultaneously. The real-time ability to nowcast economic turning points is gauged. To assess the value of the models, profit maximization measures are employed in addition to more standard criteria. When comparing predictive accuracy and profit measures, the model confidence set procedure proposed by Hansen et al. (2011) is applied to avoid data snooping. The investment strategies based on the models achieve impressive risk-adjusted returns: macroeconomists can get rich nowcasting economic turning points.},
  citeulike-article-id = {14146764},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2785583},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2785583code2270025.pdf?abstractid=2785583 and mirid=1},
  creationdate         = {2023-06-24T21:04:28},
  day                  = {28},
  groups               = {Machine learning and investment strategies, ML_InvestSelect},
  modificationdate     = {2023-06-24T21:04:28},
  owner                = {cristi},
  posted-at            = {2016-09-26 22:33:00},
}

@Article{Roncalli-2023,
  author           = {Thierry Roncalli},
  date             = {2023},
  journaltitle     = {SSRN e-Print},
  title            = {Course 2022-2023 in Sustainable Finance},
  doi              = {10.2139/ssrn.4339823},
  abstract         = {These lectures notes have been written for the course in Sustainable Finance given at the University of Paris-Saclay. The slides cover the following topics: (1) Introduction, (2) ESG Investing, (3) Impact of ESG Investing on Asset Prices and Portfolio Returns, (4) Equity Portfolio Optimization with ESG Scores, (5) Sustainable Financial Products, Impact Investing \& Engagement, (6) Global Warming \& Climate Change, (7) Economic Modeling of Climate Change, (8) Climate Risk Measures, (9) Transition Risk Modeling, (10) Portfolio Construction with Climate Risk, (11) Equity and Bond Portfolio Optimization with Green Preferences, (12) Physical Risk Modeling, and (13) Climate Stress Testing and Risk Management.},
  creationdate     = {2023-06-24T21:05:19},
  modificationdate = {2023-06-24T21:05:19},
  publisher        = {Elsevier {BV}},
}

@Article{Roncalli-2022a,
  author           = {Thierry Roncalli},
  date             = {2022},
  journaltitle     = {SSRN e-Print},
  title            = {Handbook of Sustainable Finance},
  doi              = {10.2139/ssrn.4277875},
  abstract         = {This handbook in Sustainable Finance corresponds to the lecture notes of the course given at University Paris-Saclay, ENSAE and Sorbonne University. It covers the following chapters: 1. Introduction, 2. ESG Scoring, 3. Financial Performance of ESG Investing, 4. Sustainable Financial Products, 5. Impact Investing, 6. Voting Policy \& Engagement, 7. Extra-financial Accounting, 8. Economic Modeling of Climate Change, 9. Climate Risk Measures, 10. Transition Risk Modeling, 11. Portfolio Optimization, 12. Physical Risk Modeling, 13. Climate Stress Testing, 14. Conclusion, 15. Appendix A Technical Appendix, 16. Appendix B Solutions to the Tutorial Exercises},
  creationdate     = {2023-06-24T21:05:19},
  modificationdate = {2023-06-24T21:05:19},
  publisher        = {Elsevier {BV}},
}

@Article{Roncalli-2021c,
  author           = {Thierry Roncalli},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Green and Sustainable Finance, ESG Investing and Climate Risk},
  url              = {https://ssrn.com/abstract=3769378},
  abstract         = {These lectures notes cover the following topics:
\begin{itemize}
\item ESG Investing (definition, ESG scoring and rating systems, ESG performance in stock and bond markets);
\item Climate Risk (definition, modeling, regulation, portfolio management);
\item  Sustainable financing products (SRI strategies, green and social bonds, sustainability-linked securities);
\item Impact Investing (definition, SDG, voting policy and engagement, reporting)
5\item Tutorial Exercises (probability distribution of an ESG score, optimal design of ESG rating systems, enhanced ESG score and tracking error control).
\end{itemize}

These lectures notes have been written for the advanced course in Asset Management given at the University of Paris-Sacla},
  creationdate     = {2023-06-24T21:05:19},
  modificationdate = {2023-06-24T21:05:19},
  publisher        = {Elsevier {BV}},
}

@Article{Roncalli-2021,
  author           = {Thierry Roncalli},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Advanced Course in Asset Management},
  doi              = {10.2139/ssrn.3773484},
  abstract         = {These presentation slides have been written for the Advanced Course in Asset Management (theory and applications) given at the University of Paris-Saclay. They contain 15 tutorial exercises and 5 main lectures: 
\begin{enumerate}
\item Portfolio Optimization
\item Risk Budgeting
\item Smart Beta, Factor Investing and Alternative Risk Premia \item Green and Sustainable Finance, ESG Investing and Climate Risk 
\item Machine Learning in Asset Management
\end{enumerate}

 The Table of contents is the following:

\leavevmode\newline  Part 1. Portfolio Optimization
1. Theory of portfolio optimization
1.a. The Markowitz framework
1.b. Capital asset pricing model (CAPM)
1.c. Portfolio optimization in the presence of a benchmark
1.d. Black-Litterman model
2. Practice of portfolio optimization
2.a. Covariance matrix
2.b. Expected returns
2.c. Regularization of optimized portfolios
2.d. Adding constraints
3. Tutorial exercises
3.a. Variations on the efficient frontier
3.b. Beta coefficient
3.c. Black-Litterman model

\leavevmode\newline 

Part 2. Risk Budgeting
1. The ERC portfolio
1.a. Definition
1.b. Special cases
1.c. Properties
1.d. Numerical solution
2. Extensions to risk budgeting portfolios
2.a. Definition of RB portfolios
2.b. Properties of RB portfolios
2.c. Diversification measures
2.d. Using risk factors instead of assets
3. Risk budgeting, risk premia and the risk parity strategy
3.a. Diversified funds
3.b. Risk premium
3.c. Risk parity strategies
3.d. Performance budgeting portfolios
4. Tutorial exercises
4.a. Variation on the ERC portfolio
4.b. Weight concentration of a portfolio
4.c. The optimization problem of the ERC portfolio
4.d. Risk parity funds

\leavevmode\newline 

Part 3. Smart Beta, Factor Investing and Alternative Risk Premia
1. Risk-based indexation
1.a. Capitalization-weighted indexation
1.b. Risk-based portfolios
1.c. Comparison of the four risk-based portfolios
1.d. The case of bonds
2. Factor investing
2.a. Factor investing in equities
2.b. How many risk factors?
2.c. Construction of risk factors
2.d. Risk factors in other asset classes
3. Alternative risk premia
3.a. Definition
3.b. Carry, value, momentum and liquidity
3.c. Portfolio allocation with ARP
4. Tutorial exercises
4.a. Equally-weighted portfolio
4.b. Most diversified portfolio
4.c. Computation of risk-based portfolios
4.d. Building a carry trade exposure

\leavevmode\newline 

Part 4. Green and Sustainable Finance, ESG Investing and Climate Risk
1. ESG investing
1.a. Introduction to sustainable finance
1.b. ESG scoring
1.c. Performance in the stock market
1.d. Performance in the corporate bond market
2. Climate risk
2.a. Introduction to climate risk
2.b. Climate risk modeling
2.c. Regulation of climate risk
2.d. Portfolio management with climate risk
3. Sustainable financing products
3.a. SRI Investment funds
3.b. Green bonds
3.c. Social bonds
3.d. Other sustainability-linked strategies
4. Impact investing
4.a. Definition
4.b. Sustainable development goals (SDG)
4.c. Voting policy, shareholder activism and engagement
4.d. The challenge of reporting
5. Tutorial exercises
5.a. Probability distribution of an ESG score
5.b. Enhanced ESG score and tracking error control

\leavevmode\newline 

Part 5. Machine Learning in Asset Management
1. Portfolio optimization
1.a. Standard optimization algorithms
1.b. Machine learning optimization algorithms
1.c. Application to portfolio allocation
2. Pattern learning and self-automated strategies
3. Market generators
4. Tutorial exercises
4.a. Portfolio optimization with CCD and ADMM algorithms
4.b. Regularized portfolio optimization},
  creationdate     = {2023-06-24T21:05:19},
  modificationdate = {2023-06-24T21:05:19},
  publisher        = {Elsevier {BV}},
}

@Article{Roncalli-2022,
  author           = {Thierry Roncalli},
  date             = {2022},
  journaltitle     = {SSRN e-Print},
  title            = {A Course in Sustainable Finance},
  doi              = {10.2139/ssrn.4023934},
  abstract         = {These lectures notes have been written for the course in Sustainable Finance given at the University of Paris-Saclay. The 725 slides cover the following topics: (1) the Market of ESG Investing, Ecosystem of Responsible Investing (2) ESG Scoring \& Ratings, Performance of ESG Investing, ESG Financing, ESG Risk Premium (3) Sustainable Financing Products, Green \& Social Bonds, Impact Investing, Voting Policy \& Engagement (4) Global Warming, Economic Modeling, Climate Risk Measures, Transition Risk, Physical Risk (5) Climate Investing, Portfolio Decarbonization, Net Zero Carbon Metrics, Portfolio Alignment and (6) Carbon Budget, Trend Modeling, Scoring System, Portfolio Optimization with Climate Constraints.},
  creationdate     = {2023-06-24T21:05:19},
  modificationdate = {2023-06-24T21:05:19},
  publisher        = {Elsevier {BV}},
}

@Article{Roncalli-et-al-2021a,
  author           = {Thierry Roncalli and Amina Cherief and Fatma Karray-Meziou and Margaux Regnault},
  date             = {2021-05-18},
  journaltitle     = {arXiv e-Print},
  title            = {Liquidity Stress Testing in Asset Management -- Part 2. Modeling the Asset Liquidity Risk},
  eprint           = {2105.08377},
  eprintclass      = {q-fin.RM},
  eprinttype       = {arXiv},
  abstract         = {This article is part of a comprehensive research project on liquidity risk in asset management, which can be divided into three dimensions. The first dimension covers liability liquidity risk (or funding liquidity) modeling, the second dimension focuses on asset liquidity risk (or market liquidity) modeling, and the third dimension considers the asset-liability management of the liquidity gap risk (or asset-liability matching). The purpose of this research is to propose a methodological and practical framework in order to perform liquidity stress testing programs, which comply with regulatory guidelines (ESMA, 2019, 2020) and are useful for fund managers. The review of the academic literature and professional research studies shows that there is a lack of standardized and analytical models. The aim of this research project is then to fill the gap with the goal of developing mathematical and statistical approaches, and providing appropriate answers. In this second article focused on asset liquidity risk modeling, we propose a market impact model to estimate transaction costs. After presenting a toy model that helps to understand the main concepts of asset liquidity, we consider a two-regime model, which is based on the power-law property of price impact. Then, we define several asset liquidity measures such as liquidity cost, liquidation ratio and shortfall or time to liquidation in order to assess the different dimensions of asset liquidity. Finally, we apply this asset liquidity framework to stocks and bonds and discuss the issues of calibrating the transaction cost model.},
  creationdate     = {2023-06-24T21:05:20},
  file             = {:http\://arxiv.org/pdf/2105.08377v1:PDF},
  keywords         = {q-fin.RM, q-fin.PM, q-fin.TR, stat.AP, 91B05, 91G70, 62P05, G.3},
  modificationdate = {2023-06-24T21:05:20},
}

@Article{Roncalli-et-al-2020a,
  author           = {Theo Roncalli and Theo {Le Guenedal} and Frederic Lepetit and Thierry Roncalli and Takaya Sekine},
  date             = {2020-08-30},
  journaltitle     = {arXiv e-Print},
  title            = {Measuring and Managing Carbon Risk in Investment Portfolios},
  eprint           = {2008.13198},
  eprintclass      = {q-fin.PM},
  eprinttype       = {arXiv},
  abstract         = {This article studies the impact of carbon risk on stock pricing. To address this, we consider the seminal approach of G\"orgen \textsl{et al.} (2019), who proposed estimating the carbon financial risk of equities by their carbon beta. To achieve this, the primary task is to develop a brown-minus-green (or BMG) risk factor, similar to Fama and French (1992). Secondly, we must estimate the carbon beta using a multi-factor model. While G\"orgen \textsl{et al.} (2019) considered that the carbon beta is constant, we propose a time-varying estimation model to assess the dynamics of the carbon risk. Moreover, we test several specifications of the BMG factor to understand which climate change-related dimensions are priced in by the stock market. In the second part of the article, we focus on the carbon risk management of investment portfolios. First, we analyze how carbon risk impacts the construction of a minimum variance portfolio. As the goal of this portfolio is to reduce unrewarded financial risks of an investment, incorporating the carbon risk into this approach fulfils this objective. Second, we propose a new framework for building enhanced index portfolios with a lower exposure to carbon risk than capitalization-weighted stock indices. Finally, we explore how carbon sensitivities can improve the robustness of factor investing portfolios.},
  creationdate     = {2023-06-24T21:05:20},
  file             = {:http\://arxiv.org/pdf/2008.13198v1:PDF},
  keywords         = {q-fin.PM, q-fin.PR, q-fin.RM, 90C30, G.1.6},
  modificationdate = {2023-06-24T21:05:20},
}

@Article{Roncalli-2021b,
  author           = {Thierry Roncalli},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Liquidity Stress Testing in Asset Management (Comprehensive Report)},
  doi              = {10.2139/ssrn.3981920},
  abstract         = {This report is made up of four research papers, which have been written to perform liquidity stress testing programs, which comply with ESMA regulatory guidelines: (1) Roncalli, T., Karray-Meziou, F., Pan, F., and Regnault, M. (2021), Liquidity Stress Testing in Asset Management 

\begin{enumerate}
\item Part 1. Modeling the Liability Liquidity Risk ; (2) Roncalli, T., Cherief, A., Karray-Meziou, F., and Regnault, M. (2021), Liquidity Stress Testing in Asset Management 
\item Part 2. Modeling the Asset Liquidity Risk ; (3) Roncalli, T. (2021), Liquidity Stress Testing in Asset Management
\item Part 3. Managing the Asset-Liability Liquidity Risk ; (4) Roncalli, T., and Cherief, A. (2021), Liquidity Stress Testing in Asset Management 
\item Part 4. A Step-by-step Practical Guide.
\end{enumerate}},
  creationdate     = {2023-06-24T21:05:20},
  modificationdate = {2023-06-24T21:05:20},
  publisher        = {Elsevier {BV}},
}

@Article{Roncalli-Cherief-2021,
  author           = {Thierry Roncalli and Amina Cherief},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Liquidity Stress Testing in Asset Management - Part 4. A Step-by-step Practical Guide},
  doi              = {10.2139/ssrn.3981916},
  abstract         = {This article is part of a comprehensive research project on liquidity risk in asset management, which can be divided into three dimensions. The first dimension covers liability liquidity risk (or funding liquidity) modeling, the second dimension focuses on asset liquidity risk (or market liquidity) modeling, and the third dimension considers the asset-liability management of the liquidity gap risk (or asset-liability matching). The purpose of this research is to propose a methodological and practical framework in order to perform liquidity stress testing programs, which comply with regulatory guidelines (ESMA, 2019, 2020) and are useful for fund managers. The review of the academic literature and professional research studies shows that there is a lack of standardized and analytical models. The aim of this research project is then to fill the gap with the goal of developing mathematical and statistical approaches, and providing appropriate answers. The three dimensions have been developed in the published working papers: (1) modeling the liability liquidity risk, (2) modeling the asset liquidity risk and (3) managing the asset-liability liquidity risk. This fourth working paper provides three examples and the comprehensive details to compute the redemption coverage ratio, implement reverse stress testing and estimate the liquidation cost of the redemption portfolio. The portfolios have been chosen in order to cover the main asset classes: large-cap stocks, small-cap stocks, sovereign bonds and corporate bonds. Since we provide the data in the appendix, these basic examples are easily reproducible and may help quantitative analysts to understand the different steps to implement liquidity stress testing in asset management.},
  creationdate     = {2023-06-24T21:05:20},
  modificationdate = {2023-06-24T21:05:20},
  publisher        = {Elsevier {BV}},
}

@Article{Roncalli-et-al-2020,
  author           = {Th{\'{e}}o Roncalli and Theo {Le Guenedal} and Frederic Lepetit and Thierry Roncalli and Takaya Sekine},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Measuring and Managing Carbon Risk in Investment Portfolios},
  doi              = {10.2139/ssrn.3681266},
  abstract         = {This article studies the impact of carbon risk on stock pricing. To address this, we consider the seminal approach of Gorgen et al. (2019), who proposed estimating the carbon financial risk of equities by their carbon beta. To achieve this, the primary task is to develop a brown-minus-green (or BMG) risk factor, similar to Fama and French (1992). Secondly, we must estimate the carbon beta using a multi-factor model. While Gorgen et al. (2019) considered that the carbon beta is constant, we propose a time-varying estimation model to assess the dynamics of the carbon risk. Moreover, we test several specifications of the BMG factor to understand which climate change-related dimensions are priced in by the stock market. In the second part of the article, we focus on the carbon risk management of investment portfolios. First, we analyze how carbon risk impacts the construction of a minimum variance portfolio. As the goal of this portfolio is to reduce unrewarded financial risks of an investment, incorporating the carbon risk into this approach fulfills this objective. Second, we propose a new framework for building enhanced index portfolios with a lower exposure to carbon risk than capitalization-weighted stock indices. Finally, we explore how carbon sensitivities can improve the robustness of factor investing portfolios.},
  creationdate     = {2023-06-24T21:05:20},
  modificationdate = {2023-06-24T21:05:20},
  publisher        = {Elsevier {BV}},
}

@Article{Roncalli-2021a,
  author           = {Thierry Roncalli},
  date             = {2021-10-04},
  journaltitle     = {arXiv e-Print},
  title            = {Liquidity Stress Testing in Asset Management -- Part 3. Managing the Asset-Liability Liquidity Risk},
  eprint           = {2110.01302},
  eprintclass      = {q-fin.RM},
  eprinttype       = {arXiv},
  abstract         = {This article is part of a comprehensive research project on liquidity risk in asset management, which can be divided into three dimensions. The first dimension covers the modeling of the liability liquidity risk (or funding liquidity), the second dimension is dedicated to the modeling of the asset liquidity risk (or market liquidity), whereas the third dimension considers the management of the asset-liability liquidity risk (or asset-liability matching). The purpose of this research is to propose a methodological and practical framework in order to perform liquidity stress testing programs, which comply with regulatory guidelines (ESMA, 2019, 2020) and are useful for fund managers. In this third and last research paper focused on managing the asset-liability liquidity risk, we explore the ALM tools that can be put in place to control the liquidity gap. These ALM tools can be split into three categories: measurement tools, management tools and monitoring tools. In terms of measurement tools, we focus on the computation of the redemption coverage ratio (RCR), which is the central instrument of liquidity stress testing programs. We also study the redemption liquidation policy and the different implementation methodologies, and we show how reverse stress testing can be developed. In terms of liquidity management tools, we study the calibration of liquidity buffers, the pros and cons of special arrangements (redemption suspensions, gates, side pockets and in-kind redemptions) and the effectiveness of swing pricing. In terms of liquidity monitoring tools, we compare the macro- and micro-approaches of liquidity monitoring in order to identify the transmission channels of liquidity risk.},
  creationdate     = {2023-06-24T21:05:20},
  file             = {:http\://arxiv.org/pdf/2110.01302v1:PDF},
  keywords         = {q-fin.RM, q-fin.CP, q-fin.PM, q-fin.TR, 91B05, 91G70, 62P05, G.3},
  modificationdate = {2023-06-24T21:05:20},
}

@Article{Roncalli-et-al-2021,
  author           = {Thierry Roncalli and Fatma Karray-Meziou and Francois Pan and Margaux Regnault},
  date             = {2021-01-06},
  journaltitle     = {arXiv e-Print},
  title            = {Liquidity Stress Testing in Asset Management -- Part 1. Modeling the Liability Liquidity Risk},
  doi              = {10.13140/RG.2.2.14893.72165},
  eprinttype       = {arXiv},
  url              = {https://arxiv.org/abs/2101.02110},
  abstract         = {This article is part of a comprehensive research project on liquidity risk in asset management, which can be divided into three dimensions. The first dimension covers liability liquidity risk (or funding liquidity) modeling, the second dimension focuses on asset liquidity risk (or market liquidity) modeling, and the third dimension considers asset-liability liquidity risk management (or asset-liability matching). The purpose of this research is to propose a methodological and practical framework in order to perform liquidity stress testing programs, which comply with regulatory guidelines (ESMA, 2019) and are useful for fund managers. The review of the academic literature and professional research studies shows that there is a lack of standardized and analytical models. The aim of this research project is then to fill the gap with the goal to develop mathematical and statistical approaches, and provide appropriate answers. In this first part that focuses on liability liquidity risk modeling, we propose several statistical models for estimating redemption shocks. The historical approach must be complemented by an analytical approach based on zero-inflated models if we want to understand the true parameters that influence the redemption shocks. Moreover, we must also distinguish aggregate population models and individual-based models if we want to develop behavioral approaches. Once these different statistical models are calibrated, the second big issue is the risk measure to assess normal and stressed redemption shocks. Finally, the last issue is to develop a factor model that can translate stress scenarios on market risk factors into stress scenarios on fund liabilities.},
  creationdate     = {2023-06-24T21:05:20},
  file             = {:http://arxiv.org/pdf/2101.02110v1},
  keywords         = {q-fin.RM, q-fin.CP, stat.AP, 91B05, 91G70, 62P05, G.3},
  modificationdate = {2023-06-24T21:05:20},
}

@InCollection{Perrin-Roncalli-2020,
  author           = {Sarah Perrin and Thierry Roncalli},
  booktitle        = {Machine Learning for Asset Management: New Developments and Financial Applications},
  date             = {2020},
  title            = {Machine Learning Optimization Algorithms {\&} Portfolio Allocation},
  doi              = {10.1002/9781119751182.ch8},
  editor           = {Emmanuel Jurczenko},
  pages            = {261--328},
  publisher        = {Wiley},
  abstract         = {This chapter shows how portfolio allocation can benefit from the development of large-scale portfolio optimization algorithms such as the coordinate descent, the alternating direction method of multipliers, the proximal gradient method, and Dykstra's algorithm. With these optimization algorithms, it considers more complex portfolio optimization programs with non-quadratic objective function, regularization with penalty functions and nonlinear constraints. The chapter discusses three main models of smart beta portfolios: the equal risk contribution portfolio, the risk budgeting portfolio, and the most diversified portfolio. A robo-advisor has two main objectives. The first objective is to know the investor better than a traditional asset manager. Because of this better knowledge, the robo-advisor may propose a more appropriate asset allocation. The second objective is to perform the task in a systematic way and to build an automated rebalancing process.},
  creationdate     = {2023-06-24T21:05:20},
  modificationdate = {2023-06-24T21:05:20},
  month            = {jun},
  year             = {2020},
}

@Conference{Roncalli-2012,
  author           = {T. Roncalli},
  booktitle        = {6th R/Rmetrics Meielisalp Workshop and Summer School on Computational Finance and Financial Engineering},
  date             = {2012},
  title            = {From Portfolio Optimization to Risk Parity},
  creationdate     = {2023-06-24T21:05:25},
  howpublished     = {Available at http://www.thierry-roncalli.com/download/rmetrics-risk-parity.pdf},
  modificationdate = {2023-06-24T21:05:25},
  owner            = {zkgst0c},
}

@Article{Roncalli-2010,
  author               = {Roncalli, Thierry},
  date                 = {2013-09},
  journaltitle         = {SSRN e-Print},
  title                = {Understanding the Impact of Weights Constraints in Portfolio Theory},
  url                  = {https://ssrn.com/abstract=2321309},
  abstract             = {Risk parity is an allocation method used to build diversified portfolios that does not rely on any assumptions of expected returns, thus placing risk management at the heart of the strategy. This explains why risk parity became a popular investment model after the global financial crisis in 2008. However, risk parity has also been criticized because it focuses on managing risk concentration rather than portfolio performance, and is therefore seen as being closer to passive management than active management.

In this article, we show how to introduce assumptions of expected returns into risk parity portfolios. To do this, we consider a generalized risk measure that takes into account both the portfolio return and volatility. However, the trade-off between performance and volatility contributions creates some difficulty, while the risk budgeting problem must be clearly defined.

After deriving the theoretical properties of such risk budgeting portfolios, we apply this new model to asset allocation. First, we consider long-term investment policy and the determination of strategic asset allocation. We then consider dynamic allocation and show how to build risk parity funds that depend on expected returns.},
  citeulike-article-id = {12743634},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2321309},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2325906code903940.pdf?abstractid=2321309 and mirid=1},
  creationdate         = {2023-06-24T21:05:25},
  day                  = {6},
  howpublished         = {Available at SSRN: ssrn.com/abstract=1761625},
  modificationdate     = {2023-06-24T21:05:25},
  owner                = {cristi},
  posted-at            = {2016-01-19 04:42:24},
}

@Book{Roncalli-2013,
  author           = {T. Roncalli},
  date             = {2013},
  title            = {Introduction to Risk Parity and Budgeting},
  pagetotal        = {440},
  publisher        = {Chapman and Hall/CRC},
  url              = {http://www.thierry-roncalli.com/RiskParityBook.html},
  abstract         = {Although portfolio management didn't change much during the 40 years after the seminal works of Markowitz and Sharpe, the development of risk budgeting techniques marked an important milestone in the deepening of the relationship between risk and asset management. Risk parity then became a popular financial model of investment after the global financial crisis in 2008. Today, pension funds and institutional investors are using this approach in the development of smart indexing and the redefinition of long-term investment policies.

Written by a well-known expert of asset management and risk parity, Introduction to Risk Parity and Budgeting provides an up-to-date treatment of this alternative method to Markowitz optimization. It builds financial exposure to equities and commodities, considers credit risk in the management of bond portfolios, and designs long-term investment policy.

The first part of the book gives a theoretical account of portfolio optimization and risk parity. The author discusses modern portfolio theory and offers a comprehensive guide to risk budgeting. Each chapter in the second part presents an application of risk parity to a specific asset class. The text covers risk-based equity indexation (also called smart beta) and shows how to use risk budgeting techniques to manage bond portfolios. It also explores alternative investments, such as commodities and hedge funds, and applies risk parity techniques to multi-asset classes.

The book's first appendix provides technical materials on optimization problems, copula functions, and dynamic asset allocation. The second appendix contains 30 tutorial exercises. Solutions to the exercises, slides for instructors, and Gauss computer programs to reproduce the book's examples, tables, and figures are available on the author's website.},
  creationdate     = {2023-06-24T21:05:25},
  groups           = {Risk_Budgeting, Invest_Risk},
  modificationdate = {2023-06-24T21:05:25},
  owner            = {zkgst0c},
}

@InCollection{Roncalli-2013a,
  author           = {T. Roncalli},
  booktitle        = {Frontiers in Finance, Petits Dejeuners de la Finance},
  date             = {2013},
  title            = {Risk Parity: A (New) Tool for Asset Management},
  url              = {http://www.thierry-roncalli.com/download/Risk-Parity-PDJF-2013.pdf},
  creationdate     = {2023-06-24T21:05:25},
  groups           = {Risk_Budgeting, Invest_Risk},
  howpublished     = {Available at http://www.thierry-roncalli.com/download/Risk-Parity-PDJF-2013.pdf},
  modificationdate = {2023-06-24T21:05:25},
  owner            = {zkgst0c},
}

@Article{Roncalli-2013b,
  author               = {Roncalli, Thierry},
  date                 = {2013-09},
  journaltitle         = {SSRN e-Print},
  title                = {Introducing Expected Returns into Risk Parity Portfolios: A New Framework for Tactical and Strategic Asset Allocation},
  url                  = {https://ssrn.com/abstract=2321309},
  abstract             = {Risk parity is an allocation method used to build diversified portfolios that does not rely on any assumptions of expected returns, thus placing risk management at the heart of the strategy. This explains why risk parity became a popular investment model after the global financial crisis in 2008. However, risk parity has also been criticized because it focuses on managing risk concentration rather than portfolio performance, and is therefore seen as being closer to passive management than active management.

In this article, we show how to introduce assumptions of expected returns into risk parity portfolios. To do this, we consider a generalized risk measure that takes into account both the portfolio return and volatility. However, the trade-off between performance and volatility contributions creates some difficulty, while the risk budgeting problem must be clearly defined.

After deriving the theoretical properties of such risk budgeting portfolios, we apply this new model to asset allocation. First, we consider long-term investment policy and the determination of strategic asset allocation. We then consider dynamic allocation and show how to build risk parity funds that depend on expected returns.},
  citeulike-article-id = {12743634},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2321309},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2325906code903940.pdf?abstractid=2321309 and mirid=1},
  creationdate         = {2023-06-24T21:05:25},
  day                  = {6},
  groups               = {Risk_Budgeting},
  howpublished         = {Available at SSRN: ssrn.com/abstract=2321309},
  modificationdate     = {2023-06-24T21:05:25},
  owner                = {cristi},
  posted-at            = {2016-01-19 04:42:24},
}

@Article{Roncalli-2015,
  author               = {Roncalli, Thierry},
  date                 = {2013-09},
  journaltitle         = {SSRN e-Print},
  title                = {Factor Investing and Equity Portfolio Construction},
  url                  = {https://ssrn.com/abstract=2321309},
  citeulike-article-id = {12743634},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2321309},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2325906code903940.pdf?abstractid=2321309 and mirid=1},
  creationdate         = {2023-06-24T21:05:25},
  day                  = {6},
  groups               = {Invest_Factor},
  howpublished         = {Available at http://www.thierry-roncalli.com/download/Factor-Investing-Slides.pdf},
  modificationdate     = {2023-06-24T21:05:25},
  owner                = {cristi},
  posted-at            = {2016-01-19 04:42:24},
}

@TechReport{Roncalli-2014,
  author           = {Thierry Roncalli},
  date             = {2014},
  institution      = {Lyxor Asset Management},
  title            = {Risk factor investing explained},
  url              = {https://www.next-finance.net/Risk-factor-investing-explained},
  creationdate     = {2023-06-24T21:05:25},
  groups           = {Invest_Factor},
  modificationdate = {2023-06-24T21:05:25},
  owner            = {zkgst0c},
}

@Article{Roncalli-2015a,
  author               = {Roncalli, Thierry},
  date                 = {2015},
  journaltitle         = {Bankers, Markets and Investors},
  title                = {Introducing Expected Returns into Risk Parity Portfolios: A New Framework for Asset Allocation},
  number               = {138},
  pages                = {18--28},
  abstract             = {Risk parity is an allocation method used to build diversified portfolios that does not rely on any assumptions of expected returns, thus placing risk management at the heart of the strategy. This explains why risk parity became a popular investment model after the global financial crisis in 2008. However, risk parity has also been criticized because it focuses on managing risk concentration rather than portfolio performance, and is therefore seen as being closer to passive management than active management. In this article, we show how to introduce assumptions of expected returns into risk parity portfolios. To do this, we consider a generalized risk measure that takes into account both the portfolio return and volatility. However, the trade-off between performance and volatility contributions creates some difficulty, while the risk budgeting problem must be clearly defined. After deriving the theoretical properties of such risk budgeting portfolios, we apply this new model to asset allocation. First, we compare risk budgeting portfolios and optimized portfolios and illustrate that the new approach defines a defensive model of active management. Then, we consider long-term investment policy and the determination of strategic asset allocation.},
  citeulike-article-id = {14184919},
  creationdate         = {2023-06-24T21:05:25},
  groups               = {Risk_Budgeting},
  keywords             = {active, allocation, asset, budgeting, erc, expected, management, parity, portfolio, returns, risk, shortfall, strategic, tactical, value-at-risk, *file-import-16-11-15},
  modificationdate     = {2023-06-24T21:05:25},
  owner                = {cristi},
  posted-at            = {2016-11-15 22:30:12},
}

@Article{Roncalli-2017,
  author               = {Roncalli, Thierry},
  date                 = {2017},
  journaltitle         = {SSRN e-Print},
  title                = {Alternative Risk Premia: What Do We Know?},
  url                  = {https://ssrn.com/abstract=2868425},
  abstract             = {The concept of alternative risk premia is an extension of the factor investing approach. Factor investing consists in building long-only equity portfolios, which are directly exposed to common risk factors like size, value or momentum. Alternative risk premia designate non-traditional risk premia other than a long exposure to equities and bonds. They may involve equities, rates, credit, currencies or commodities and correspond to long/short portfolios. However, contrary to traditional risk premia, it is more difficult to define alternative risk premia and which risk premia really matter. In fact, the term "alternative risk premia encompasses"two different types of systematic risk factor: skewness risk premia and market anomalies. For example, the most frequent alternative risk premia are carry and momentum, which are respectively a skewness risk premium and a market anomaly. Because the returns of alternative risk premia exhibit heterogeneous patterns in terms of statistical properties, option profile and drawdown, asset allocation is more complex than with traditional risk premia. In this context, risk diversification cannot be reduced to volatility diversification and skewness risk becomes a key component of portfolio optimization. Understanding these different concepts and how they interconnect is essential for improving multi-asset allocation.},
  citeulike-article-id = {14346957},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2868425},
  creationdate         = {2023-06-24T21:05:25},
  groups               = {Factor based investing},
  modificationdate     = {2023-06-24T21:05:25},
  posted-at            = {2017-04-29 20:54:02},
}

@InCollection{Roncalli-2017a,
  author               = {Roncalli, Thierry},
  booktitle            = {Factor Investing},
  date                 = {2017},
  title                = {Alternative Risk Premia: What Do We Know?},
  doi                  = {10.1016/b978-1-78548-201-4.50010-6},
  isbn                 = {9781785482014},
  pages                = {227--264},
  publisher            = {Elsevier},
  abstract             = {The concept of alternative risk premia (ARP) is an extension of the factor investing approach. Factor investing consists of building long-only equity portfolios which are directly exposed to common risk factors such as size, value or momentum. ARP designate non-traditional risk premia other than a long exposure to equities and bonds. They may involve equities, rates, credit, currencies or commodities and correspond to long-short portfolios. However, contrary to traditional risk premia, it is more difficult to define ARP in terms of which risk premia really matter. In fact, the term "alternative risk premia" encompasses two different types of systematic risk factor: skewness risk premia and market anomalies. For example, the most frequent ARP are carry and momentum, which are, respectively, a skewness risk premium and a market anomaly. Because the returns of ARP exhibit heterogeneous patterns in terms of statistical properties, option profiles and drawdown, asset allocation is more complex than with traditional risk premia. In this context, risk diversification cannot be reduced to volatility diversification and skewness risk becomes a key component of portfolio optimization. Understanding these different concepts and how they interconnect is essential for improving multi-asset allocation.},
  citeulike-article-id = {14499086},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/b978-1-78548-201-4.50010-6},
  creationdate         = {2023-06-24T21:05:25},
  groups               = {Invest_Risk, RiskPremia_FixedIncome, RiskPremia_Other, RiskPremia_Alt, MultiFactor_Invest},
  modificationdate     = {2023-06-24T21:05:25},
  posted-at            = {2017-12-08 00:41:21},
}

@Article{Roncalli-2018,
  author               = {Roncalli, Thierry},
  date                 = {2018},
  journaltitle         = {SSRN e-Print},
  title                = {Keep up the Momentum},
  url                  = {https://ssrn.com/abstract=3083921},
  abstract             = {The momentum risk premium is one of the most important alternative risk premia alongside the carry risk premium. However, it appears that it is not always well understood. For example, is it an alpha or a beta exposure? Is it a skewness risk premium or a market anomaly? Does it pursue a performance objective or a hedging objective? What are the differences between time-series and cross-section momentum? What are the main drivers of momentum returns? What does it mean when we say that it is a convex and not a concave strategy? Why is the momentum risk premium a diversifying engine, and not an absolute return strategy?

The goal of this paper is to provide specific and relevant answers to all these questions. The answers can already be found in the technical paper "Understanding the Momentum Risk Premium" published recently by Jusselin et al. (2017). However, the underlying mathematics can be daunting to readers. Therefore, this discussion paper presents the key messages and the associated financial insights behind these results.

Among the main findings, one result is of the most importance. To trend is to diversify in bad times. In good times, trend-following strategies offer no significant diversification power. Indeed, they are beta strategies. This is not a problem, since investors do not need to be diversified at all times. In particular, they do not need diversification in good times, because they do not want that the positive returns generated by some assets to be cancelled out by negative returns on other assets. This is why diversification may destroy portfolio performance in good times. Investors only need diversification in bad economic times and stressed markets.

This diversification asymmetry is essential when investing in beta strategies like alternative risk premia. On the contrary, this diversification asymmetry is irrelevant when investing in absolute return strategies. However, we know that generating performance with alpha strategies is much more difficult than generating performance with beta strategies. Therefore, beta is beautiful, but convex beta is precious and scarce. Among risk premia, momentum is one of the few strategies to offer this diversification asymmetry. This is why investing in momentum is a decision of portfolio construction, and not a search for alpha.},
  citeulike-article-id = {14514580},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=3083921},
  creationdate         = {2023-06-24T21:05:26},
  modificationdate     = {2023-06-24T21:05:26},
  posted-at            = {2018-01-10 17:33:08},
}

@Article{Roncalli-Weisang-2012,
  author               = {Roncalli, T. and Weisang, G.},
  date                 = {2012-07},
  journaltitle         = {Quantitative Finance},
  title                = {Beyond Risk Parity: Using Non-Gaussian Risk Measures and Risk Factors},
  citeulike-article-id = {13933252},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2015.1046907},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2015.1046907},
  creationdate         = {2023-06-24T21:05:26},
  day                  = {3},
  groups               = {Risk_Budgeting},
  howpublished         = {Available at http://www.thierry-roncalli.com/download/Beyond-Risk-Parity.pdf},
  modificationdate     = {2023-06-24T21:05:26},
  owner                = {cristi},
  posted-at            = {2016-02-15 16:56:16},
  publisher            = {Routledge},
}

@Article{Roncalli-Weisang-2016,
  author               = {Roncalli, T. and Weisang, G.},
  date                 = {2016-07},
  journaltitle         = {Quantitative Finance},
  title                = {Risk parity portfolios with risk factors},
  doi                  = {10.1080/14697688.2015.1046907},
  number               = {3},
  pages                = {377-388},
  volume               = {16},
  abstract             = {Portfolio construction and risk budgeting are the focus of many studies by academics and practitioners. In particular, diversification has spawned much interest and has been defined very differently.

In this paper, we analyse a method to achieve portfolio diversification based on the decomposition of the portfolio's risk into risk factor contributions.

First, we expose the relationship between risk factor and asset contributions.

Secondly, we formulate the diversification problem in terms of risk factors as an optimization program.

Finally, we illustrate our methodology with a real example of building a strategic asset allocation based on economic factors for a pension fund facing liability constraints.},
  citeulike-article-id = {13933252},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2015.1046907},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2015.1046907},
  creationdate         = {2023-06-24T21:05:26},
  day                  = {3},
  groups               = {Risk_Budgeting, PortfOptim_Factor, Invest_Risk},
  journal              = {Quantitative Finance},
  modificationdate     = {2023-06-24T21:05:26},
  owner                = {cristi},
  posted-at            = {2016-02-15 16:56:16},
  publisher            = {Routledge},
}

@Article{Richard-Roncalli-2019,
  author           = {Richard, Jean-Charles and Roncalli, Thierry},
  date             = {2019-02-15},
  journaltitle     = {arXiv e-Print},
  title            = {Constrained Risk Budgeting Portfolios: Theory, Algorithms, Applications and Puzzles},
  url              = {https://arxiv.org/abs/1902.05710},
  urldate          = {2019-03-07},
  abstract         = {This article develops the theory of risk budgeting portfolios, when we would like to impose weight constraints. It appears that the mathematical problem is more complex than the traditional risk budgeting problem. The formulation of the optimization program is particularly critical in order to determine the right risk budgeting portfolio. We also show that numerical solutions can be found using methods that are used in large-scale machine learning problems. Indeed, we develop an algorithm that mixes the method of cyclical coordinate descent (CCD), alternating direction method of multipliers (ADMM), proximal operators and Dykstra's algorithm. This theoretical body is then applied to some investment problems. In particular, we show how to dynamically control the turnover of a risk parity portfolio and how to build smart beta portfolios based on the ERC approach by improving the liquidity of the portfolio or reducing the small cap bias. Finally, we highlight the importance of the homogeneity property of risk measures and discuss the related scaling puzzle.},
  creationdate     = {2023-06-24T21:05:26},
  day              = {15},
  f1000-projects   = {QuantInvest},
  groups           = {Risk_Budgeting, Invest_Risk},
  modificationdate = {2023-06-24T21:05:26},
}

@Article{Perrin-Roncalli-2019,
  author           = {Perrin, Sarah and Roncalli, Thierry},
  date             = {2019},
  journaltitle     = {SSRN e-Print},
  title            = {Machine learning optimization algorithms \& portfolio allocation},
  doi              = {10.2139/ssrn.3425827},
  issn             = {1556-5068},
  url              = {https://ssrn.com/abstract=3425827},
  urldate          = {2019-08-10},
  abstract         = {Portfolio optimization emerged with the seminal paper of Markowitz (1952). The original mean-variance framework is appealing because it is very efficient from a computational point of view. However, it also has one well-established failing since it can lead to portfolios that are not optimal from a financial point of view (Michaud, 1989). Nevertheless, very few models have succeeded in providing a real alternative solution to the Markowitz model. The main reason lies in the fact that most academic portfolio optimization models are intractable in real life although they present solid theoretical properties. By intractable we mean that they can be implemented for an investment universe with a small number of assets using a lot of computational resources and skills, but they are unable to manage a universe with dozens or hundreds of assets. However, the emergence and the rapid development of robo-advisors means that we need to rethink portfolio optimization and go beyond the traditional mean-variance optimization approach.Another industry and branch of science has faced similar issues concerning large-scale optimization problems. Machine learning and applied statistics have long been associated with linear and logistic regression models. Again, the reason was the inability of optimization algorithms to solve high-dimensional industrial problems. Nevertheless, the end of the 1990s marked an important turning point with the development and the rediscovery of several methods that have since produced impressive results. The goal of this paper is to show how portfolio allocation can benefit from the development of these large-scale optimization algorithms. Not all of these algorithms are useful in our case, but four of them are essential when solving complex portfolio optimization problems. These four algorithms are the coordinate descent, the alternating direction method of multipliers, the proximal gradient method and the Dykstra's algorithm. This paper reviews them and shows how they can be implemented in portfolio allocation.},
  creationdate     = {2023-06-24T21:05:26},
  f1000-projects   = {QuantInvest},
  groups           = {ML_NumOptimiz},
  modificationdate = {2023-06-24T21:05:26},
}

@Conference{Roncalli-2019,
  author           = {Thierry Roncalli},
  booktitle        = {SwissQuant Conference},
  date             = {2019},
  title            = {How Machine Learning Can Improve Portfolio Allocation of Robo-Advisors},
  url              = {http://www.thierry-roncalli.com/download/SwissQuant-Conference-Robo-Roncalli-2019.pdf},
  abstract         = {solving portfolio optimization with machine learning algorithms},
  creationdate     = {2023-06-24T21:05:26},
  modificationdate = {2023-06-24T21:05:26},
}

@TechReport{Roncalli-2019a,
  author           = {Thierry Roncalli},
  date             = {2019},
  institution      = {Amundi},
  title            = {Portfolio Allocation: From Quadratic Programming to Machine Learning Optimization Algorithms},
  url              = {http://www.thierry-roncalli.com/download/qp-ml-portfolio-optimization.pdf},
  abstract         = {In this presentation, we show how the development of the portfolio optimization has been related to the development of quadratic programming (QP) algorithms. We make a parallel between portfolio allocation problems and some statistical modeling problems (least squares, lasso and svm). Since the nineties, the emergence of machine learning has changed the landscape of optimization. New algorithms have emerged, for example CCD or ADMM. After a review of these techniques, we show how they can be used to solve new problems in asset allocation. We first consider mean-variance optimized portfolios and illustrate how they can be regularized when they are used in robo-advisors. Then, we apply CCD and ADMM algorithms to risk budgeting portfolios.},
  booktitle        = {SwissQuant Conference},
  creationdate     = {2023-06-24T21:05:26},
  modificationdate = {2023-06-24T21:05:26},
}

@Article{Sakurai-et-al-2021a,
  author           = {Yutaka Sakurai and Yusuke Yuki and Ryota Katsuki and Takashi Yazane and Fumio Ishizaki},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Quantum-Inspired Weighting Approach to Correlation Diversified Passive Portfolio Strategy},
  doi              = {10.2139/ssrn.3487195},
  abstract         = {In this paper, we develop a passive strategy improving index investing, which we call Correlation Diversified Portfolio strategy. The proposed method adjusts weight vector of original index based on the permutation of assets composing the original index. We seek the permutation of assets such that assets with strong correlation to many other assets should be placed in the central part of permutation. By reducing the weights of assets placed in the central part of permutation, we can construct portfolios which are more diversified and have better risk-return characteristics than original index. To examine the usefulness and computational feasibility of the proposed method, we apply it to three major indices of US and Japan, and we provide numerical experiments. The numerical experiments show that portfolios constructed by the proposed method can achieve higher return with lower volatility than the original indices, while their behaviors are still similar to those of the original indices.},
  creationdate     = {2023-06-24T21:06:18},
  modificationdate = {2023-06-24T21:06:18},
  publisher        = {Elsevier {BV}},
}

@Article{Sakurai-et-al-2021,
  author           = {Yutaka Sakurai and Yusuke Yuki and Ryota Katsuki and Takashi Yazane and Fumio Ishizaki},
  date             = {2021},
  journaltitle     = {The Journal of Investment Strategies},
  title            = {Correlation diversified passive portfolio strategy based on permutation of assets},
  doi              = {10.21314/jois.2021.010},
  abstract         = {In this paper we develop a passive strategy to improve index investing, which we call the correlation diversified portfolio strategy. The proposed method adjusts the weight vector of the original index based on the permutation of the assets belonging to the original index. We seek the permutation of these assets such that those assets with a strong correlation to many other assets are placed in the center of the permutation. By reducing the weights of such central assets, we can construct portfolios that are more diversified and have better risk-return characteristics than the original index. We solve this asset-permutation problem by adopting a quantum-inspired approach. Concretely, we convert this permutation problem into a quadratic unconstrained binary optimization problem and use simulated annealing on a personal computer or annealing machine to find a near-optimal solution in a reasonable time. To examine the usefulness and computational feasibility of the proposed method, we apply it to three major indexes of the United States and Japan, and we provide numerical experiments that show portfolios constructed by the proposed method can achieve a higher return with lower volatility compared with the original indexes, while their behaviors are still similar to those of the original indexes.},
  creationdate     = {2023-06-24T21:06:18},
  modificationdate = {2023-06-24T21:06:18},
  publisher        = {Infopro Digital Services Limited},
}

@Article{Sass-Thos-2022,
  author           = {Jorn Sass and Anna-Katharina Thos},
  date             = {2022},
  journaltitle     = {Econometrics and Statistics},
  title            = {Risk reduction and portfolio optimization using clustering methods},
  doi              = {10.1016/j.ecosta.2021.11.010},
  abstract         = {Diversification is one of the main pillars of investment strategies. The prominent equal weight or one-over-N portfolio, which puts equal weight on each asset, is apart from its simplicity a strategy which is hard to outperform in realistic settings. But depending on the number of considered assets it can lead to very large portfolios. An approach to reduce the number of chosen assets based on clustering is proposed and its advantages and disadvantages are investigated. Using clustering techniques the possible assets are separated into non-overlapping clusters and the assets within a cluster are ordered by their Sharpe ratio. Then the best asset of each portfolio is chosen to be a member of the new portfolio with equal weights, the cluster portfolio. It is shown that this portfolio inherits the advantages of the equal weight portfolio and that it can even outperform it empirically. To this end different performance measures are used to compare the portfolios on simulated and real data. To explain the observations on real data, explanatory results are derived in an extreme model setting and analyzed in several simulation studies.},
  creationdate     = {2023-06-24T21:06:55},
  modificationdate = {2023-06-24T21:06:55},
  publisher        = {Elsevier {BV}},
}

@Article{Scherer-Lehner-2023,
  author           = {Bernd Scherer and Sebastian Lehner},
  date             = {2023},
  journaltitle     = {Journal of Asset Management},
  title            = {Trust me, I am a Robo-advisor},
  doi              = {10.1057/s41260-022-00284-y},
  abstract         = {A two-step iterative estimation of a risk model, alternating between a cross-sectional and time-series regression, aims to achieve an in-sample consistent representation of risk factors, such that the security exposure matrix input of the cross-sectional step is equal to the output exposure matrix estimated in the subsequent time-series step. The sequence of estimated exposure matrices is proven to converge to a fixed point. The condition for a fixed point is identified and proven necessary and sufficient. The presented mathematical proof of viability of the two-step iterative estimation is complementary to earlier research in this area.},
  creationdate     = {2023-06-24T21:07:28},
  modificationdate = {2023-06-24T21:07:28},
  publisher        = {Springer Science and Business Media {LLC}},
  timestamp        = {2023-01-04},
}

@Article{Scherer-2022,
  author           = {Bernd Scherer},
  date             = {2022-05},
  journaltitle     = {The Journal of Portfolio Management},
  title            = {Portfolio Factory},
  doi              = {10.3905/jpm.2022.1.364},
  number           = {7},
  pages            = {7--13},
  volume           = {48},
  abstract         = {The author formalizes the concept of a portfolio factory as the automated implementation of an active model portfolio on a wide range of client portfolios that differ across benchmarks, regulatory constraints, or client-specific constraints. Within this context, the article shows that portfolio factory as an active portfolio optimization problem is equivalent to tracking error minimization between an active model portfolio and active client portfolio. This approach allows well-established portfolio optimization frameworks and creates greater acceptance among investment professionals because portfolio factories create optimal tracking portfolios.},
  creationdate     = {2023-06-24T21:07:28},
  modificationdate = {2023-06-24T21:07:28},
  publisher        = {Pageant Media {US}},
}

@Article{Scherer-2021,
  author           = {Bernd Scherer},
  date             = {2021-08},
  journaltitle     = {Journal of Asset Management},
  title            = {Adding alternative assets: return enhancement, diversification or hedging?},
  doi              = {10.1057/s41260-021-00238-w},
  pages            = {437-442},
  volume           = {22},
  abstract         = {Adding assets (so-called extensions) to an already existing portfolio is a reoccurring question in times of rapidly expanding investment opportunity sets. Examples for this "how much" question are the incorporation of liquid alternative assets in the form of hedge funds or alternative risk premia in a global balanced portfolio, the addition of global equities to a domestic equity portfolios or simply the optimal allocation of corporate credit within a government debt portfolio. While this is hardly a new question and a variety of tools have already been established, we suggest a new framework to decompose the demand for risky assets in economically meaningful components. This allows us to identify whether a particular allocation is driven by demand created from noisy return estimates or by more predictable hedging and diversification demand.},
  creationdate     = {2023-06-24T21:07:28},
  modificationdate = {2023-06-24T21:07:28},
  publisher        = {Springer Science and Business Media {LLC}},
}

@Article{Scherer-Lehner-2021a,
  author           = {Bernd Scherer and Sebastian Lehner},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {What Drives Robo-Advice?},
  doi              = {10.2139/ssrn.3807921},
  abstract         = {The promise of robo-advisory firms is to provide low cost access to diversified portfolios built in accordance with the academic literature on normative portfolio choice. We investigate the latter claim. How much normative advice does robo-advice contain? For this purpose we web-scrap portfolio recommendations for 151200 investor types (input combinations from an online questionnaire) for one of the largest US robo-advisors. Our results show that the type of investment goal and the length of time horizon are dominating inputs with significant influence on recommended equity allocations. Normative advice in the form of Merton type hedging demands plays no role at all.},
  creationdate     = {2023-06-24T21:07:28},
  modificationdate = {2023-06-24T21:07:28},
  publisher        = {Elsevier {BV}},
}

@Article{Scherer-Lehner-2021,
  author           = {Bernd Scherer and Sebastian Lehner},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Investor Experience and Portfolio Choice},
  doi              = {10.2139/ssrn.3828373},
  abstract         = {MiFID II forces banks and wealth managers to ask clients for their investment knowledge and experience. The implied regulatory view is that less experience should result in less risk taking. While this is neither shared in theoretical nor in empirical finance, it becomes a source of legal risk for asset managers and banks. How do banks react? So far this question was impossible to answer. The relevant data have not been available as they are not shared by banks. We circumvene this problem by using publicly available portfolio recommendations from robo-advisory firms. These firms fall under the same regulations as banks and wealth managers with respect to MiFID II investor profiling and are often owned by traditional banks. It is therefore reasonable to assume that their treatment of investor experience is similar to traditional banks' approaches.},
  creationdate     = {2023-06-24T21:07:28},
  modificationdate = {2023-06-24T21:07:28},
  publisher        = {Elsevier {BV}},
}

@Article{Scherer-2002,
  author               = {Scherer, Bernd},
  date                 = {2002-11},
  journaltitle         = {Financial Analysts Journal},
  title                = {Portfolio Resampling: Review and Critique},
  doi                  = {10.2469/faj.v58.n6.2489},
  issn                 = {0015-198X},
  number               = {6},
  pages                = {98--109},
  volume               = {58},
  abstract             = {A well-understood fact of asset allocation is that the traditional portfolio optimization algorithm is too powerful for the quality of the inputs. Recently, a new concept called resampled efficiency has been introduced into the asset management world to deal with estimation error.

The objective of this article is to describe this new technology, put it into the context of established procedures, and point to some peculiarities of the approach. Even though portfolio resampling is a thoughtful heuristic, some features make it difficult to interpret by the inexperienced.},
  citeulike-article-id = {13978590},
  citeulike-linkout-0  = {http://dx.doi.org/10.2469/faj.v58.n6.2489},
  creationdate         = {2023-06-24T21:07:28},
  modificationdate     = {2023-06-24T21:07:28},
  owner                = {cristi},
  posted-at            = {2016-03-12 22:20:25},
}

@Article{Scherer-2004,
  author               = {Scherer, Bernd},
  date                 = {2004},
  journaltitle         = {Financial Markets and Portfolio Management},
  title                = {Resampled efficiency and portfolio choice},
  doi                  = {10.1007/s11408-004-0403-7},
  number               = {4},
  pages                = {382--398},
  volume               = {18},
  abstract             = {Uninformative priors that only leverage the covariance matrix and leave expected returns unchanged. The efficient set remains unchanged.

Statistical priors that shrink return estimates towards the grand asset mean. The resulting portfolios converge towards the minimum variance portfolio if the available data history is short.

Informative priors that shrink return estimates towards equilibrium returns. Resulting allocations converge to the global market portfolio.},
  citeulike-article-id = {13978592},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11408-004-0403-7},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s11408-004-0403-7},
  creationdate         = {2023-06-24T21:07:28},
  modificationdate     = {2023-06-24T21:07:28},
  owner                = {cristi},
  posted-at            = {2016-03-12 22:26:26},
  publisher            = {Kluwer Academic Publishers},
}

@Article{Scherer-2007,
  author               = {Scherer, Bernd},
  date                 = {2007-03},
  journaltitle         = {Journal of Asset Management},
  title                = {Can robust portfolio optimisation help to build better portfolios?},
  doi                  = {10.1057/palgrave.jam.2250049},
  issn                 = {1470-8272},
  number               = {6},
  pages                = {374--387},
  volume               = {7},
  abstract             = {Estimation error has always been acknowledged as a substantial problem in portfolio construction. Various approaches exist that range from Bayesian methods with a very strong rooting in decision theory to practitioner-based heuristics with no rooting in decision theory at all as portfolio resampling.

Robust optimisation is the latest attempt to address estimation error directly in the portfolio construction process. It will be shown that robust optimisation is equivalent to Bayesian shrinkage estimators and offer no marginal value relative to the former.

The implied shrinkage that comes with robust optimisation is difficult to control. Consistent with the ad hoc treatment of uncertainty aversion in robust optimisation, it can be seen that out of sample performance largely depends on the appropriate choice of uncertainty aversion, with no guideline on how to calibrate this parameter or how to make it consistent with the more well-known risk aversion.},
  citeulike-article-id = {1183348},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/palgrave.jam.2250049},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/pal/jam/2007/00000007/00000006/art00002},
  citeulike-linkout-2  = {http://link.springer.com/article/10.1057/palgrave.jam.2250049},
  creationdate         = {2023-06-24T21:07:28},
  groups               = {PortfOptim_Robust},
  modificationdate     = {2023-06-24T21:07:28},
  owner                = {cristi},
  posted-at            = {2016-03-12 21:19:26},
  publisher            = {Palgrave Macmillan UK},
}

@Article{Scherer-2009,
  author               = {Scherer, Bernd},
  date                 = {2009},
  journaltitle         = {Financial Markets and Portfolio Management},
  title                = {A note on portfolio choice for sovereign wealth funds},
  doi                  = {10.1007/s11408-009-0105-2},
  abstract             = {The current vast account surpluses of commodity-rich nations, combined with record account deficits in developed markets (the United States, Britain) have created a new type of investor. Sovereign wealth funds (SWF) are instrumental in deciding how these surpluses will be invested. We need to better understand the investment problem for an SWF in order to project future investment flows. Extending Gintschel and Scherer (J. Asset Manag. 9(3):215-238, 2008), we apply the portfolio choice problem for a sovereign wealth fund in a Campbell and Viceira (Strategic Asset Allocation, 2002) strategic asset allocation framework. Changing the analysis from a one to a multi-period framework allows us to establish a three-fund separation. We split the optimal portfolio for an SWF into speculative demand as well as hedge demand against oil price shocks and shocks to the short-term risk-free rate. In addition, all terms now depend on the investor's time horizon. We show that oil-rich countries should hold bonds and that the optimal investment policy for an SWF as a long-term investor is determined by long-run covariance matrices that differ from the correlation inputs that one-period (myopic) investors use.},
  citeulike-article-id = {5046731},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11408-009-0105-2},
  citeulike-linkout-1  = {http://www.springerlink.com/content/w624331422060533},
  creationdate         = {2023-06-24T21:07:28},
  modificationdate     = {2023-06-24T21:07:28},
  owner                = {zkgst0c},
  posted-at            = {2016-08-31 15:49:47},
}

@Article{Scherer-2010a,
  author               = {Scherer, Bernd},
  date                 = {2010-06},
  journaltitle         = {Financial Markets and Portfolio Management},
  title                = {A note on asset management and market risk},
  doi                  = {10.1007/s11408-010-0137-7},
  issn                 = {1555-4961},
  abstract             = {The textbook view on risk in asset management companies is summarized by Hull (Risk Management and Financial Institutions): For an asset manager the greatest risk is operational risk. Using evidence from various panel regression models, we show that asset management revenues carry substantial market risks, a finding that challenges not only academic risk management literature on the predominance of operative risks, but also the current industry practice of not hedging market risks that are systematically built into the revenue-generation process. For asset management companies to return to an annuity model, these risks need to be managed more actively. Shareholders do not want to be exposed to market beta by investing in asset management companies; they want to participate in these companies' alpha generation and take advantage of their fund-gathering expertise as financial intermediaries.},
  citeulike-article-id = {7378920},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11408-010-0137-7},
  citeulike-linkout-1  = {http://www.springerlink.com/content/e83w8006w5524mw4},
  creationdate         = {2023-06-24T21:07:28},
  day                  = {22},
  groups               = {Annuities},
  modificationdate     = {2023-06-24T21:07:28},
  owner                = {zkgst0c},
  posted-at            = {2016-08-31 16:03:57},
}

@InCollection{Scherer-2011,
  author           = {B. Scherer},
  booktitle        = {LQG Conference, Oxford},
  date             = {2011},
  title            = {The 10 Biggest Myths in Asset Management},
  creationdate     = {2023-06-24T21:07:28},
  howpublished     = {Available at http://www.ftc.at/images/File/20110906_scherer-10myths.pdf},
  modificationdate = {2023-06-24T21:07:28},
  owner            = {zkgst0c},
}

@Article{Scherer-2012,
  author               = {Scherer, Bernd},
  date                 = {2012-10},
  journaltitle         = {Quantitative Finance},
  title                = {Market risks in asset management companies},
  doi                  = {10.1080/14697688.2011.650185},
  number               = {10},
  pages                = {1547--1556},
  volume               = {12},
  abstract             = {This paper shows that revenues from a sample of publicly traded US asset management companies carry substantial market risks. Not only does this challenge the academic risk management literature about the predominance of operative risks in asset management, it is also at odds with current practice in asset management firms.

Asset managers do not hedge market risks even though these risks are systematically built into the revenue generation process. This is surprising as shareholders would not optimally choose asset management companies as their source of market beta. They rather prefer to participate in alpha generation and fund gathering expertise of investment managers as financial intermediaries.

At the very minimum, asset managers need to monitor their fees at risk to understand what impact product design, benchmark choice and fee contract design have on revenue volatility. This calls for a much wider interpretation of the risk management function that too narrowly focuses on client risks.},
  citeulike-article-id = {13933207},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2011.650185},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2011.650185},
  creationdate         = {2023-06-24T21:07:28},
  day                  = {1},
  modificationdate     = {2023-06-24T21:07:28},
  owner                = {cristi},
  posted-at            = {2016-02-15 16:20:25},
  publisher            = {Routledge},
}

@Article{Scherer-2012a,
  author               = {Scherer, Bernd},
  date                 = {2012-03},
  journaltitle         = {Journal of Asset Management},
  title                = {Risk parity in US futures markets},
  doi                  = {10.1057/jam.2012.4},
  issn                 = {1470-8272},
  number               = {3},
  pages                = {155--161},
  volume               = {13},
  abstract             = {Risk parity allocates identical percentage contribution to risk to each individual asset. In the absence of established theoretical foundations, investors and product suppliers attribute the strong historical performance of risk parity portfolios to better diversification. This is an ill-founded belief.

For US futures data I show that risk parity is not about diversification, but about higher return expectations for leveraged low-risk bonds. Although this is consistent with leverage aversion, it is incompatible with consumption-based asset pricing. In contrast to past work, I use futures data instead of diversified equity and bond indices. This allows concerns raised earlier about the availability of historic implementation costs or the historic price of leverage to be sidestepped.},
  citeulike-article-id = {13968876},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2012.4},
  creationdate         = {2023-06-24T21:07:28},
  day                  = {22},
  groups               = {Risk_Budgeting},
  modificationdate     = {2023-06-24T21:07:28},
  owner                = {cristi},
  posted-at            = {2016-03-06 05:41:57},
}

@InCollection{Scherer-2015,
  author               = {Scherer, Bernd},
  booktitle            = {Risk-Based and Factor Investing},
  date                 = {2015},
  title                = {Target Volatility},
  doi                  = {10.1016/b978-1-78548-008-9.50007-1},
  isbn                 = {9781785480089},
  pages                = {173--193},
  publisher            = {Elsevier},
  abstract             = {It is a common practice among investment practitioners to scale (leverage) their portfolio positions in risky assets (or trading strategies) according to forecasted volatility. There is a cross-sectional and a time series aspect of this procedure. In the cross-sectional dimension, we scale all assets to a common target volatility. After scaling, all assets will display similar realized volatility, i.e. all assets will be equally important for overall portfolio risk and performance.

The objective here is better asset diversification. If all assets display equal Sharpe ratio and equal correlation, the resulting portfolio is also mean variance efficient. In the time series dimension, we want to achieve better time diversification, i.e. we want to make every period equally important for overall portfolio risk and performance. Both aspects of diversification can be found to different degrees in risk parity (predominantly cross sectional volatility scaling) as well as target volatility (predominantly time series volatility scaling) products.

This contribution will focus on the theoretical and empirical foundations of target volatility. If target volatility offers better time diversification (even without larger expected returns), target volatility products will outperform buy and hold products by offering lower risk per unit of return. This will then lead to higher Sharpe ratios and mean variance utility.},
  citeulike-article-id = {13978530},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/b978-1-78548-008-9.50007-1},
  creationdate         = {2023-06-24T21:07:29},
  modificationdate     = {2023-06-24T21:07:29},
  owner                = {cristi},
  posted-at            = {2016-03-12 19:11:57},
}

@Book{Scherer-2015a,
  author               = {Scherer, Bernd},
  date                 = {2015-03},
  title                = {Portfolio Construction and Risk Budgeting (Fifth Edition)},
  isbn                 = {1904339697},
  pagetotal            = {350},
  publisher            = {Risk Books},
  url                  = {https://riskbooks.com/portfolio-construction-and-risk-budgeting-5th-edition},
  abstract             = {Completely updated and extended to cover the rapid expansion of the literature since the financial crises, this new edition of Portfolio Construction and Risk Budgeting provides the reader with a clear overview of the subject. The author presents quantitative methods and comprehensive and up-to-date coverage of alternative portfolio construction techniques, ranging from traditional methods based on mean- variance and lower-partial moments approaches, through Bayesian techniques, to more recent developments such as portfolio re-sampling and stochastic programming solutions using scenario optimisation.},
  citeulike-article-id = {14177384},
  citeulike-linkout-0  = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20 and amp;path=ASIN/1904339697},
  citeulike-linkout-1  = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21 and amp;path=ASIN/1904339697},
  citeulike-linkout-2  = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21 and amp;path=ASIN/1904339697},
  citeulike-linkout-3  = {http://www.amazon.jp/exec/obidos/ASIN/1904339697},
  citeulike-linkout-4  = {http://www.amazon.co.uk/exec/obidos/ASIN/1904339697/citeulike00-21},
  citeulike-linkout-5  = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20 and path=ASIN/1904339697},
  citeulike-linkout-6  = {http://www.worldcat.org/isbn/1904339697},
  citeulike-linkout-7  = {http://books.google.com/books?vid=ISBN1904339697},
  citeulike-linkout-8  = {http://www.amazon.com/gp/search?keywords=1904339697 and index=books and linkCode=qs},
  citeulike-linkout-9  = {http://www.librarything.com/isbn/1904339697},
  creationdate         = {2023-06-24T21:07:29},
  day                  = {08},
  groups               = {Risk_Budgeting, Invest_Risk},
  howpublished         = {Hardcover},
  modificationdate     = {2023-06-24T21:07:29},
  owner                = {cristi},
  posted-at            = {2016-11-04 19:56:56},
}

@Article{Scherer-2006,
  author               = {Scherer, Bernd},
  date                 = {2006},
  journaltitle         = {Journal of Asset Management},
  title                = {A note on the out-of-sample performance of resampled efficiency},
  doi                  = {10.1057/palgrave.jam.2240211},
  number               = {3-4},
  pages                = {170--178},
  volume               = {7},
  abstract             = {The concept of resampled efficiency (RE) is debated both in academia as well as among practitioners. For supporters of RE the litmus test seems to be out-of-sample performance. While Markowitz and Usmen have shown that RE outperforms a Bayesian alternative, the present study is able to reverse their results. The key is to understand that Bayesian methods are literally impossible to test out-of-sample. For every distribution, a prior will be found that will outperform resampling (and vice versa). Equally, for every prior, a distribution will be found where resampling outperforms. The fact that one method outperforms another for a given set of data means little. In the absence of theory, investors do not know when one method will outperform the other, as they do not know the true distribution},
  citeulike-article-id = {14314155},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/palgrave.jam.2240211},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1057/palgrave.jam.2240211},
  creationdate         = {2023-06-24T21:07:29},
  modificationdate     = {2023-06-24T21:07:29},
  posted-at            = {2017-03-19 18:34:15},
  publisher            = {Palgrave Macmillan UK},
}

@Article{Scherer-2004a,
  author               = {Scherer, Bernd},
  date                 = {2004},
  journaltitle         = {Journal of Asset Management},
  title                = {An alternative route to performance hypothesis testing},
  doi                  = {10.1057/palgrave.jam.2240123},
  number               = {1},
  pages                = {5--12},
  volume               = {5},
  abstract             = {A wide variety of risk-return ratios are routinely reported in sales pitches as well as academic publications. Little attempt has been made, however, to look at the small sample distributions of these estimators in order to derive confidence bands. The reason for this has been the extreme difficulty of working out the required statistics for most risk-return ratios. Rather than following classical statistics, this paper relies on a general and robust method which not only provides confidence intervals for arbitrary risk-return ratios, sample sizes and distribution, but is also fairly easy to implement.},
  citeulike-article-id = {14313235},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/palgrave.jam.2240123},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1057/palgrave.jam.2240123},
  creationdate         = {2023-06-24T21:07:29},
  modificationdate     = {2023-06-24T21:07:29},
  posted-at            = {2017-03-17 22:14:14},
  publisher            = {Palgrave Macmillan UK},
}

@Article{Scherer-2017,
  author               = {Scherer, Bernd},
  date                 = {2017},
  journaltitle         = {Financial Markets and Portfolio Management},
  title                = {Algorithmic portfolio choice: lessons from panel survey data},
  doi                  = {10.1007/s11408-016-0282-8},
  number               = {1},
  pages                = {49--67},
  volume               = {31},
  abstract             = {Automated asset management offerings algorithmically assign risky portfolios to individual investors based on investor characteristics such as age, net income, or self-assessment of risk aversion. Using new German household panel data, we investigate the key household characteristics that drive private asset allocation decisions. This information allows us to assess which set of variables should be included in algorithmic portfolio advice. Using heavily cross-validated classification trees, we find that a combination of household balance sheet variables-describing the ability to take risks (e.g., net wealth)-and household personal characteristics-describing the willingness to take risks (e.g., risk aversion)-best explain the cross-sectional variation in household portfolio choice. Our empirical evidence is in line with models of portfolio choice under decreasing relative risk aversion and fixed investment costs. The results suggest the utility of a more holistic modeling of household characteristics. Including background risks in the form of household leverage not only makes investment sense, but is also the new regulatory reality under MIFID II rules. Robo-advisors are strongly advised to act accordingly.},
  citeulike-article-id = {14334795},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11408-016-0282-8},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s11408-016-0282-8},
  creationdate         = {2023-06-24T21:07:29},
  modificationdate     = {2023-06-24T21:07:29},
  posted-at            = {2017-04-09 22:46:50},
  publisher            = {Springer US},
}

@Article{Scherer-2001,
  author               = {Scherer, B.},
  date                 = {2001},
  journaltitle         = {Journal of Asset Management},
  title                = {A note on tracking error funding assumptions},
  doi                  = {10.1057/palgrave.jam.2240048},
  number               = {3},
  pages                = {235--240},
  volume               = {2},
  abstract             = {This paper tries to show that changing the funding assumptions in calculating marginal contributions to tracking error can significantly enhance the interpretation and acceptance of portfolio risk decomposition and implied alpha calculation. While standard risk software assumes that active positions are funded from cash, this is not intuitive for the portfolio managers involved. A long position in US 10 years could also be funded from 5-year bonds in the US (intracountry spread trade), from a combination of cash and 5-year bonds (barbell trade), from a short position in Canada (intercountry spread trade) or from a short position in 10-year corporate bonds (credit spread trade). Thinking of risk positions as long/short trades allows portfolio managers to monitor risk quickly and decompose it into meaningful trade categories.},
  citeulike-article-id = {14337868},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/palgrave.jam.2240048},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1057/palgrave.jam.2240048},
  creationdate         = {2023-06-24T21:07:29},
  modificationdate     = {2023-06-24T21:07:29},
  posted-at            = {2017-04-15 17:19:14},
  publisher            = {Palgrave Macmillan UK},
}

@Book{Scherer-Martin-2005,
  author           = {Scherer, B. and Martin, R. D.},
  date             = {2005},
  title            = {Modern Portfolio Optimization},
  pagetotal        = {391},
  publisher        = {Springer},
  url              = {https://www.springer.com/gp/book/9780387210162},
  abstract         = {In recent years portfolio optimization and construction methodologies have become an increasingly critical ingredient of asset and fund management, while at the same time portfolio risk assessment has become an essential ingredient in risk management, and this trend will only accelerate in the coming years. Unfortunately there is a large gap between the limited treatment of portfolio construction methods that are presented in most university courses with relatively little hands-on experience and limited computing tools, and the rich and varied aspects of portfolio construction that are used in practice in the finance industry. Current practice demands the use of modern methods of portfolio construction that go well beyond the classical Markowitz mean-variance optimality theory and require the use of powerful scalable numerical optimization methods. This book fills the gap between current university instruction and current industry practice by providing a comprehensive computationally-oriented treatment of modern portfolio optimization and construction methods},
  creationdate     = {2023-06-24T21:07:29},
  modificationdate = {2023-06-24T21:07:29},
  owner            = {zkgst0c},
}

@Book{Scherer-Winston-2014,
  author           = {Bernd Scherer and Kenneth Winston},
  date             = {2014},
  title            = {The Oxford Handbook of Quantitative Asset Management},
  pagetotal        = {530},
  publisher        = {Oxford University Press},
  url              = {https://global.oup.com/academic/product/the-oxford-handbook-of-quantitative-asset-management-9780199553433?cc=fr&lang=en&},
  abstract         = {Quantitative portfolio management has become a highly specialized discipline. Computing power and software improvements have advanced the field to a level that would not have been thinkable when Harry Markowitz began the modern era of quantitative portfolio management in 1952. In addition to raw computing power, major advances in financial economics and econometrics have shaped academia and the financial industry over the last sixty years. While the idea of a general theory of finance is still only a distant hope, asset managers now have tools in the financial engineering kit that address specific problems in their industry.

The Oxford Handbook of Quantitative Asset Management consists of seven sections that explore major themes in current theoretical and practical use. These themes span all aspects of a modern quantitative investment organization. Contributions from academics and practitioners working in leading investment management organizations bring together the key theoretical and practical aspects of the field to provide a comprehensive overview of the major developments in the area},
  creationdate     = {2023-06-24T21:07:29},
  modificationdate = {2023-06-24T21:07:29},
  owner            = {zkgst0c},
}

@Article{Scherer-Xu-2007,
  author               = {Scherer, Bernd and Xu, Xiaodong},
  date                 = {2007-01},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {The Impact of Constraints on Value-Added},
  doi                  = {10.3905/jpm.2007.690605},
  issn                 = {0095-4918},
  number               = {4},
  pages                = {45--54},
  volume               = {33},
  abstract             = {While cynics view investment constraints as a lawyers' way of managing risk or simply as a manifestation of the risk management of the 1950s when no computers and risk models were available, other observers regard constraints as a worst case safeguard against a breakdown in risk management practices and a practical reality of peer risk in institutional investing. A closer look at the costs of constraints suggests a new methodology to measure the impact of individual constraints on an investor's value added. This approach differs from current methods that either focus on a headline measure of information ratio shrinkage (also called transfer coefficient) or express the impact of constraints as distortions in implied alphas},
  citeulike-article-id = {14313455},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2007.690605},
  creationdate         = {2023-06-24T21:07:29},
  modificationdate     = {2023-06-24T21:07:29},
  posted-at            = {2017-03-18 15:56:36},
}

@Article{Scherer-2013,
  author           = {Scherer, B.},
  date             = {2013-02-28},
  journaltitle     = {The Journal of Derivatives},
  title            = {Synchronize Your Data or Get Out of Step with Your Risks},
  doi              = {10.3905/jod.2013.20.3.075},
  issn             = {1074-1240},
  number           = {3},
  pages            = {75--84},
  urldate          = {2019-06-27},
  volume           = {20},
  abstract         = {The rapid increase in economic globalization and international diversification of investment portfolios impose new problems in performance assessment and risk management. Research showing relatively low correlation among equities traded in different countries suggests substantial risk reduction is possible from international diversification. In this article, Scherer points to the statistical difficulty of measuring correlation properly for securities traded in national markets that are open and closed at different times in the day. If the trading day starts in Japan, by the time the U.S. stock market opens, the European markets have been trading for hours and the Japanese stock market is already closed. A major event with worldwide implications that occurs during New York trading hours will not show up in the Nikkei until the next day, while it will be reflected in returns on the German DAX either the same day or the next, depending on the time. Procedures that synchronize returns to correct correlation estimates for this effect show that true correlations are much higher than uncorrected estimates computed from returns. Without such corrections, risk measures like Value-at-Risk will be significantly underestimated.},
  creationdate     = {2023-06-24T21:07:29},
  day              = {28},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T21:07:29},
}

@Article{Scherer-Ebertz-1998,
  author           = {Scherer, Bernhard and Ebertz, Thomas},
  date             = {1998-04-30},
  journaltitle     = {The Journal of Wealth Management},
  title            = {A simple model for lifetime asset allocation},
  doi              = {10.3905/jwm.1998.409796},
  issn             = {1534-7524},
  number           = {2},
  pages            = {27--30},
  urldate          = {2019-07-13},
  volume           = {1},
  creationdate     = {2023-06-24T21:07:29},
  day              = {30},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T21:07:29},
}

@Article{Scherer-2012b,
  author               = {Scherer, Bernd},
  date                 = {2012-03},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Asset Allocation with Shadow Assets},
  doi                  = {10.3905/jwm.2012.15.3.030},
  number               = {3},
  pages                = {30-35},
  url                  = {https://jwm.pm-research.com/content/15/3/30},
  volume               = {15},
  abstract             = {The wealth of most investors contains both financial assets as well as nonfinancial assets. The author defines shadow assets as (mostly) nonfinancial and nontradable assets that are exogenous to the investor's asset allocation decision, such as human capital, nonfinancial sovereign assets (e.g., underground oil reserves), the present value of future alumni contributions for university endowments, or the nonlisted family business for the client of a family office.
Ignoring shadow assets is unfortunate, as it is the existence and nature of shadow assets that distinguishes private investors, university endowments, sovereign wealth funds, and family offices and hence leads to different demands for risky securities. Shadow assets influence the outcome of asset allocation decisions via their covariance with financial assets and their effect on total wealth. Adding shadow assets has two effects on investors: It makes investors more aggressive and can create demand for hedges. Investment advice in the client's best interests needs to incorporate shadow assets as well as shadow liabilities, as optimal allocations differ considerably when shadow components are properly accounted for},
  citeulike-article-id = {13968876},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2012.4},
  creationdate         = {2023-06-24T21:07:29},
  day                  = {22},
  groups               = {Risk_Budgeting},
  modificationdate     = {2023-06-24T21:07:29},
  owner                = {cristi},
  posted-at            = {2016-03-06 05:41:57},
}

@Article{Scherer-Apel-2020,
  author           = {Scherer, Bernd and Apel, Matthias},
  date             = {2020-02-19},
  journaltitle     = {The Journal of Alternative Investments},
  title            = {Business cycle-related timing of alternative risk premia strategies},
  doi              = {10.3905/jai.2020.1.091},
  issn             = {1520-3255},
  number           = {4},
  pages            = {8-24},
  urldate          = {2020-03-07},
  volume           = {22},
  abstract         = {Time variation in risk premia is not a violation of market efficiency but rather a reflection of time-varying economic rewards. By analyzing macroeconomic sensitivities (proxying for good and bad times), the authors show that time-varying returns of certain alternative risk premia strategies are significantly related to economic conditions. On the basis of identified return patterns, the authors construct a risk premia timing strategy that adds statistically significant marginal performance with low turnover. They confront data mining concerns by successfully cross validating their model across various investment universes.},
  creationdate     = {2023-06-24T21:07:29},
  day              = {19},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T21:07:29},
}

@Article{Scherer-2020,
  author           = {Scherer, Bernd},
  date             = {2020-05},
  journaltitle     = {Journal of Asset Management},
  title            = {Alternative risk premia: contagion and portfolio choice},
  doi              = {10.1057/s41260-020-00158-1},
  issn             = {1470-8272},
  number           = {3},
  pages            = {178-191},
  urldate          = {2020-06-13},
  volume           = {21},
  abstract         = {Portfolio managers regard contagion as the death of diversification. The simultaneous jump to worst decile returns for most investments in a portfolio is hard to offset by diversification alone. Our results find substantial contagion across {ARP} strategies, which is difficult to predict. We derive the optimal asset allocation for an {ARP} portfolio under contagion risk and show that the investor  best defence is to take less portfolio leverage. In addition, he should shy away from assets that perform poorly in contagion states.},
  creationdate     = {2023-06-24T21:07:29},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T21:07:29},
}

@Article{Schwendner-et-al-2021,
  author           = {Peter Schwendner and Jochen Papenbrock and Markus Jaeger and Stephan Krugel},
  date             = {2021},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {Adaptive Seriational Risk Parity and Other Extensions for Heuristic Portfolio Construction Using Machine Learning and Graph Theory},
  doi              = {10.3905/jfds.2021.1.078},
  number           = {4},
  pages            = {65-83},
  volume           = {3},
  abstract         = {In this article, the authors present a conceptual framework named adaptive seriational risk parity (ASRP) to extend hierarchical risk parity (HRP) as an asset allocation heuristic. The first step of HRP (quasi-diagonalization), determining the hierarchy of assets, is required for the actual allocation done in the second step (recursive bisectioning). In the original HRP scheme, this hierarchy is found using single-linkage hierarchical clustering of the correlation matrix, which is a static tree-based method. The authors compare the performance of the standard HRP with other static and adaptive tree-based methods, as well as seriation-based methods that do not rely on trees. Seriation is a broader concept allowing reordering of the rows or columns of a matrix to best express similarities between the elements. Each discussed variation leads to a different time series reflecting portfolio performance using a 20-year backtest of a multi-asset futures universe. Unsupervised learningbased on these time-series creates a taxonomy that groups the strategies in high correspondence to the construction hierarchy of the various types of ASRP. Performance analysis of the variations shows that most of the static tree-based alternatives to HRP outperform the single-linkage clustering used in HRP on a risk-adjusted basis. Adaptive tree methods show mixed results, and most generic seriation-based approaches underperform.},
  creationdate     = {2023-06-24T21:08:24},
  modificationdate = {2023-06-24T21:08:24},
}

@Article{Serur-Avellaneda-2021,
  author           = {Juan Andr{\'{e}}s Serur and Marco Avellaneda},
  date             = {2021},
  journaltitle     = {SSRN e-Print},
  title            = {Hierarchical {PCA} and Modeling Asset Correlations},
  doi              = {10.2139/ssrn.3903460},
  abstract         = {Modeling cross-sectional correlations between thousands of stocks, acrosscountries and industries, can be challenging. In this paper, we demonstratethe advantages of using Hierarchical Principal Component Analysis (HPCA)over the classic PCA. We also introduce a statistical clustering algorithmto identify homogeneous clusters of stocks or "synthetic sectors". We apply these methods to study cross-sectional correlations in the US, Europe, China,and Emerging Markets.},
  creationdate     = {2023-06-24T21:09:01},
  modificationdate = {2023-06-24T21:09:01},
  publisher        = {Elsevier {BV}},
}

@InProceedings{Shirota-Murakami-2021,
  author           = {Yukari Shirota and Akane Murakami},
  booktitle        = {{IEEE} International Conference on Service Operations and Logistics, and Informatics ({SOLI})},
  date             = {2021-12},
  title            = {Long-term Time Series Data Clustering of Stock Prices for Portfolio Selection},
  doi              = {10.1109/soli54607.2021.9672407},
  publisher        = {{IEEE}},
  abstract         = {In this paper, clustering for stock data is conducted with two clustering methods, k-Shape and k-means with DTW distance measure and the results are compared. The data is the top 129 global electronics manufactures' stock prices from 2018 to 2020 which included the worst Christmas in 2018 and the beginning of COVID-19 outbreak. The involved countries are US, China, Taiwan, Korea, Japan and some others. The clustering results by k-Shape indicate distinctively different effects on those countries' stock markets due to the COVID-19 turmoil. The patterns of the clusters can be visualized to identify the differences among the clusters. We found that each of eight clusters comprises of the same country companies. From that, we could guess that investors or their algorithms tend to invest in companies according to its country rather than the individual company's performance.},
  creationdate     = {2023-06-24T21:10:17},
  modificationdate = {2023-06-24T21:10:17},
}

@Article{Snow-2020e,
  author           = {Derek Snow},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Predicting Earnings Surprises},
  doi              = {10.2139/ssrn.3420722},
  abstract         = {Nonlinear classification models can predict future earnings surprises with a high accuracy by using pricing and earnings input data. Surprises of 15\% or more can be predicted with 71\% accuracy. These predictions can be used to form profitable trading strategies. Additional variables have been created using signal-processing and handcrafted feature-engineering methods. Some of these variables have in the past been known to be related to analyst bias. The machine learning model in effect corrects for analyst mistakes and biases by incorporating these variables into a nonlinear prediction model to predict future earnings surprises.},
  creationdate     = {2023-06-24T21:10:55},
  modificationdate = {2023-06-24T21:10:55},
  publisher        = {Elsevier {BV}},
}

@Article{Snow-2020d,
  author           = {Derek Snow},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {{DataGene}: A Framework for Dataset Similarity},
  doi              = {10.2139/ssrn.3619626},
  abstract         = {DataGene is developed to identify data set similarity between real and synthetic datasets as well as train, test, and validation datasets. For many modelling and software development tasks there is a need for datasets to have share similar characteristics. This has traditionally been achieved with visualizations, DataGene seeks to replace these visual methods with a range of novel quantitative methods. Please see the GitHub repository to inspect and install the Python code.},
  creationdate     = {2023-06-24T21:10:55},
  modificationdate = {2023-06-24T21:10:55},
  publisher        = {Elsevier {BV}},
}

@Article{Snow-2019,
  author           = {Snow, Derek},
  date             = {2019},
  journaltitle     = {SSRN e-Print},
  title            = {Machine learning in asset management},
  doi              = {10.2139/ssrn.3420952},
  issn             = {1556-5068},
  url              = {https://ssrn.com/abstract=3420952},
  urldate          = {2019-08-10},
  abstract         = {This paper investigates various machine learning trading and portfolio optimisation models and techniques. The notebooks to this paper are Python based. By last count there are about 15 distinct trading varieties and around 100 trading strategies. Code and data are made available where appropriate. The hope is that this paper will organically grow with future developments in machine learning and data processing techniques. Changes can be tracked on the GitHub repository.},
  creationdate     = {2023-06-24T21:10:56},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T21:10:56},
}

@Article{Snow-2019a,
  author           = {Snow, Derek},
  date             = {2019},
  journaltitle     = {SSRN e-Print},
  title            = {Financial Event Prediction using Machine Learning},
  doi              = {10.2139/ssrn.3481555},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3481555},
  urldate          = {2020-01-01},
  abstract         = {Financial machine learning (FinML) has in recent years developed into a subdiscipline in its own right with enthusiastic experimenters at its helm. Since investigating the subject five years ago, there has been an almost exponential progress in academic interest. When I first started thinking about machine learning in finance, formal research was sparse, to say the least. As a result, I have had the luxury to pick from a broad range of topics and decided to investigate financial event prediction using machine learning, giving rise to the eponymous title.FinML research can loosely be divided into four streams. The first concerns asset price prediction where researchers attempt to predict the future value of securities using a machine learning methodology. The second stream involves predicting hard or soft financial events like earnings surprises, corporate defaults, and mergers and acquisitions. The third stream entails the prediction and or estimation of values not directly related to the price of a security, such as future revenue, firm valuation, credit ratings, etc. The fourth and last stream comprises the use of machine learning techniques to solve traditional optimisation problems in finance like optimal execution and the optimal construction of portfolios. This thesis, in particular, focuses on the use of machine learning in financial event prediction. In the past, finance academics had to be content with mostly linear models that could only ingest a small number of variables of a particular type. Now we can use non-linear models with a larger number of variables and more versatile data types. In this thesis, I show how machine learning can lead to significant improvements in financial event prediction, more specifically, in earnings surprise, bankruptcy and facility closure predictions, all of which have significant financial implications for the businesses and stakeholders alike.},
  creationdate     = {2023-06-24T21:10:56},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T21:10:56},
}

@Article{Snow-2020,
  author           = {Snow, Derek},
  date             = {2020-01-31},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {Machine Learning in Asset Management Part 1: Portfolio Construction Trading Strategies},
  issue            = {1},
  number           = {1},
  pages            = {10-23},
  url              = {https://jfds.pm-research.com/content/2/1/10},
  urldate          = {2020-03-07},
  volume           = {2},
  abstract         = {This is the first in a series of articles dealing with machine learning in asset management. Asset management can be broken into the following tasks: (1) portfolio construction, (2) risk management, (3) capital management, (4) infrastructure and deployment, and (5) sales and marketing. This article focuses on portfolio construction using machine learning. Historically, algorithmic trading could be more narrowly defined as the automation of sell-side trade execution, but since the introduction of more advanced algorithms, the definition has grown to include idea generation, alpha factor design, asset allocation, position sizing, and the testing of strategies. Machine learning, from the vantage of a decision-making tool, can help in all these areas.},
  creationdate     = {2023-06-24T21:10:56},
  day              = {31},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T21:10:56},
  publisher        = {Institutional Investor Journals Umbrella},
}

@Article{Snow-2020b,
  author           = {Snow, Derek},
  date             = {2020-04-22},
  journaltitle     = {SSRN e-Print},
  title            = {DeltaPy: A Framework for Tabular Data Augmentation in Python},
  url              = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3582219},
  urldate          = {2020-05-03},
  abstract         = {A range of data abstractions have come to the fore since the re-emergence of machine learning. This includes procedures like feature engineering, extraction, transformation, and selection, as well as data preprocessing, generation, synthesisation, and augmentation. This report attempts to unify some of this terminology with the development of a bare-bones Python package, DeltaPy.},
  creationdate     = {2023-06-24T21:10:56},
  day              = {22},
  f1000-projects   = {QuantInvest},
  groups           = {ML_DataAugment},
  modificationdate = {2023-06-24T21:10:56},
}

@Article{Snow-2020a,
  author           = {Snow, Derek},
  date             = {2020-06-02},
  journaltitle     = {SSRN e-Print},
  title            = {{MTSS}-{GAN}: Multivariate Time Series Simulation Generative Adversarial Networks},
  url              = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3616557},
  urldate          = {2020-06-19},
  abstract         = {{MT\SS}-{GAN} is a new generative adversarial network ({GAN}) developed to simulate diverse multivariate time series ({MTS}) data with finance applications in mind. The purpose of this synthesiser is two-fold, we both want to generate data that accurately represents the original data, while also having the flexibility to generate data with novel and unique relationships that could help with model testing and robustness checks. The method is inspired by stacked {GANs} originally designed for image generation. Stacked {GANs} have produced some of the best quality images, for that reason {MT\SS}-{GAN} is expected to be a leading contender in multivariate time series generation.},
  creationdate     = {2023-06-24T21:10:56},
  day              = {2},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T21:10:56},
}

@Article{Snow-2020c,
  author           = {Snow, Derek},
  date             = {2020-01-31},
  journaltitle     = {The Journal of Financial Data Science},
  title            = {Machine Learning in Asset Management - Part 2: Portfolio Construction - Weight Optimization},
  issue            = {2},
  pages            = {17-24},
  url              = {https://jfds.pm-research.com/content/2/2/17},
  urldate          = {2020-03-07},
  volume           = {2},
  abstract         = {This is the second in a series of articles dealing with machine learning in asset management. This article focuses on portfolio weighting using machine learning. Following from the previous article (Snow 2020), which looked at trading strategies, this article identifies different weight optimization methods for supervised, unsupervised, and reinforcement learning frameworks. In total, seven submethods are summarized, with the code made available for further exploration.},
  creationdate     = {2023-06-24T21:10:56},
  day              = {31},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T21:10:56},
  publisher        = {Institutional Investor Journals Umbrella},
}

@Article{Swedroe-2021j,
  author           = {Larry Swedroe},
  date             = {2021},
  journaltitle     = {Advisor Perspectives},
  title            = {The Troubling Evidence Against Private Equity and Venture Capital},
  url              = {https://www.advisorperspectives.com/articles/2021/06/10/the-troubling-evidence-against-private-equity-and-venture-capital},
  abstract         = {Investors who believe in active management rely on the past performance of the managers they select. Unfortunately, when it comes to stocks, bonds and hedge funds, there's no evidence of persistence of outperformance beyond the randomly expected. However, there has been some evidence of persistence of outperformance in the investment/asset class of private equity/venture capital (VC).

But new research challenges those findings and makes a compelling case that advisors and their clients should proceed with caution in those assets classes, investing only when they are confident they have identified a compelling strategic advantage.},
  creationdate     = {2023-06-24T21:11:37},
  modificationdate = {2023-06-24T21:11:37},
}

@Article{Swedroe-2020g,
  author           = {Larry Swedroe},
  date             = {2020},
  journaltitle     = {Advisor Perspectives},
  title            = {Actively Managed Funds Fail When Needed the Most},
  url              = {https://www.advisorperspectives.com/articles/2020/04/06/actively-managed-funds-fail-when-needed-the-most},
  abstract         = {Advisors had little use for actively managed funds over the recent bull market; index funds did exceptionally well. But just when those actively managed funds were most needed - over the recent market downturn - they failed to protect investors.},
  creationdate     = {2023-06-24T21:11:37},
  modificationdate = {2023-06-24T21:11:37},
}

@Article{Swedroe-2020f,
  author           = {Larry Swedroe},
  date             = {2020},
  journaltitle     = {Advisor Perspectives},
  title            = {Fact and Fiction about Low-Risk Investing},
  url              = {https://www.advisorperspectives.com/articles/2020/06/09/fact-and-fiction-about-low-risk-investing},
  abstract         = {One of the most popular and successful strategies over the last decade has been low-risk (i.e., low-beta or low-volatility) investing. New research shows that those strategies have persisted even after the publication of the research documenting their existence, and that they can be pursued in low-cost, low-turnover portfolios.},
  creationdate     = {2023-06-24T21:11:37},
  modificationdate = {2023-06-24T21:11:37},
}

@Article{Swedroe-2020e,
  author           = {Larry Swedroe},
  date             = {2020},
  journaltitle     = {Advisor Perspectives},
  title            = {The Forgotten History of Value Investing},
  url              = {https://www.advisorperspectives.com/articles/2020/06/01/the-forgotten-history-of-value-investing},
  abstract         = {Given the dramatic underperformance of value stocks since 2017, it's understandable that many are abandoning the strategy, believing that the premium has vanished. But, studious observers of market history know that value faced similar death sentences previously, only to undergo a rapid reincarnation and deliver spectacular returns.},
  creationdate     = {2023-06-24T21:11:37},
  modificationdate = {2023-06-24T21:11:37},
}

@Article{Swedroe-2020d,
  author           = {Larry Swedroe},
  date             = {2020},
  journaltitle     = {Advisor Perspectives},
  title            = {A Lost Decade for the Fama-French Factors},
  url              = {https://www.advisorperspectives.com/articles/2020/05/13/a-lost-decade-for-the-fama-french-factors},
  abstract         = {The poor performance of the Fama-French factors over the last decade has led many to question the existence of the premiums. But new research shows that those 10 years were not unique, and that factor-based investing have prevailed following periods of underperformance.},
  creationdate     = {2023-06-24T21:11:37},
  modificationdate = {2023-06-24T21:11:37},
}

@Article{Swedroe-2020c,
  author           = {Larry Swedroe},
  date             = {2020},
  journaltitle     = {Advisor Perspectives},
  title            = {Actively Managed Funds Underperform on a Risk-Adjusted Basis},
  url              = {https://www.advisorperspectives.com/articles/2020/07/01/actively-managed-funds-underperform-on-a-risk-adjusted-basis},
  abstract         = {Large-scale studies have shown that actively managed funds underperform their passive benchmarks on an absolute basis. New research shows that this is also true on a risk-adjusted basis - and this is true across asset classes and sub-classes.},
  creationdate     = {2023-06-24T21:11:38},
  modificationdate = {2023-06-24T21:11:38},
}

@Article{Swedroe-2020b,
  author           = {Larry Swedroe},
  date             = {2020},
  journaltitle     = {Advisor Perspectives},
  title            = {Factor-Based Investing Beats Active Management for Bonds},
  url              = {https://www.advisorperspectives.com/articles/2020/08/17/factor-based-investing-beats-active-management-for-bonds},
  abstract         = {Factor-driven investing, while highly popular among equity investors, has not been as widely adopted in the bond market. But research shows that a factor-based approach to bond investing is superior to attempting to identify top-performing active bond managers.},
  creationdate     = {2023-06-24T21:11:38},
  modificationdate = {2023-06-24T21:11:38},
}

@Article{Swedroe-2020a,
  author           = {Larry Swedroe},
  date             = {2020},
  journaltitle     = {Advisor Perspectives},
  title            = {The Importance of Diversification in Achieving Long-Term Goals},
  url              = {https://www.advisorperspectives.com/articles/2020/10/26/the-importance-of-diversification-in-achieving-long-term-goals},
  abstract         = {My 2007 book, Wise Investing Made Simple: Larry Swedroe's Tales to Enrich Your Future, contained 27 tales to educate investors about important investment concepts and strategies. This article is in the spirit of those tales. The examples are hypothetical.},
  creationdate     = {2023-06-24T21:11:38},
  modificationdate = {2023-06-24T21:11:38},
}

@Article{Swedroe-2020,
  author           = {Larry Swedroe},
  date             = {2020},
  journaltitle     = {Advisor Perspectives},
  title            = {Luck, Skill and Their Role in Passive Fund Returns},
  url              = {https://www.advisorperspectives.com/articles/2020/12/07/luck-skill-and-their-role-in-passive-fund-returns},
  abstract         = {Investors may choose a passive fund because they don't believe they can distinguish between luck and skill among active fund managers. But new research shows that the issue of luck and skill plays an important role in passive fund returns too.},
  creationdate     = {2023-06-24T21:11:38},
  modificationdate = {2023-06-24T21:11:38},
}

@Article{Swedroe-2021i,
  author           = {Larry Swedroe},
  date             = {2021},
  journaltitle     = {Advisor Perspectives},
  title            = {Bond Ratings Tell Only Part of the Story},
  url              = {https://www.advisorperspectives.com/articles/2021/01/06/bond-ratings-tell-only-part-of-the-story},
  abstract         = {Bonds with the same S\&P or Moody's credit rating can vary greatly in terms of their risk and subsequent return. New research shows that fixed income investors must also consider their credit spreads.},
  creationdate     = {2023-06-24T21:11:38},
  modificationdate = {2023-06-24T21:11:38},
}

@Article{Swedroe-2021h,
  author           = {Larry Swedroe},
  date             = {2021},
  journaltitle     = {Advisor Perspectives},
  title            = {The Battle of Factor Models},
  url              = {https://www.advisorperspectives.com/articles/2021/01/25/the-battle-of-factor-models},
  abstract         = {Since the development of the first asset pricing model, the capital asset pricing model (CAPM), academic research has attempted to find models that increase the explanatory power of the cross-section of stock returns.},
  creationdate     = {2023-06-24T21:11:38},
  modificationdate = {2023-06-24T21:11:38},
}

@Article{Swedroe-2021g,
  author           = {Larry Swedroe},
  date             = {2021},
  journaltitle     = {Advisor Perspectives},
  title            = {An Out-of-Sample Test for Factor-Based Strategies},
  url              = {https://www.advisorperspectives.com/articles/2021/02/15/an-out-of-sample-test-for-factor-based-strategies},
  abstract         = {Factor-based models are often criticized for data mining. One way to address that charge is with "out-of-sample" testing over longer time frames. But that takes time. New research provides an alternative out-of-sample test - using emerging-market bonds.},
  creationdate     = {2023-06-24T21:11:38},
  modificationdate = {2023-06-24T21:11:38},
}

@Article{Swedroe-2021f,
  author           = {Larry Swedroe},
  date             = {2021},
  journaltitle     = {Advisor Perspectives},
  title            = {Is There Illiquidity in Equity Returns?},
  url              = {https://www.advisorperspectives.com/articles/2021/02/22/is-there-illiquidity-in-equity-returns},
  abstract         = {Liquidity is valuable to investors. Therefore, they should demand higher expected return (a risk premium) for less liquid stocks. But new research shows they have not earned that extra return in public equity markets.},
  creationdate     = {2023-06-24T21:11:38},
  modificationdate = {2023-06-24T21:11:38},
}

@Article{Swedroe-2021e,
  author           = {Larry Swedroe},
  date             = {2021},
  journaltitle     = {Advisor Perspectives},
  title            = {Revisionist History in ESG Ratings},
  url              = {https://www.advisorperspectives.com/articles/2021/05/16/revisionist-history-in-esg-ratings},
  abstract         = {Investors following an environmental, social and governance (ESG) mandate can achieve their goals only if they can accurately and consistently identify stocks that meet their criteria. But new research shows that those criteria have been subject to arbitrary revisions and that there are wide discrepancies among the vendors providing the data.},
  creationdate     = {2023-06-24T21:11:39},
  modificationdate = {2023-06-24T21:11:39},
}

@Article{Swedroe-2021d,
  author           = {Larry Swedroe},
  date             = {2021},
  journaltitle     = {Advisor Perspectives},
  title            = {Socially Responsible Funds Do Not Deliver Excess Returns},
  url              = {https://www.advisorperspectives.com/articles/2021/05/31/socially-responsible-funds-do-not-deliver-excess-returns},
  abstract         = {Funds with a socially responsible or environmental, social and governance (ESG) mandate may allow clients to feel good about their investments. But new research shows that they should not expect excess returns.},
  creationdate     = {2023-06-24T21:11:39},
  modificationdate = {2023-06-24T21:11:39},
}

@Article{Swedroe-2021b,
  author           = {Larry Swedroe},
  date             = {2021},
  journaltitle     = {Advisor Perspectives},
  title            = {The Role of Financial Risk Tolerance in Investment Policy},
  url              = {https://www.advisorperspectives.com/articles/2021/06/14/the-role-of-financial-risk-tolerance-in-investment-policy},
  abstract         = {Evaluating an investor's ability, willingness and need to take risk, and then designing his or her portfolio accordingly, are the most crucial functions of investment/financial planning and the "fintech" software that supports the profession. Thus, the ability to estimate an investor's financial risk tolerance (FRT) is key to the success of a financial plan. Financial advisors know that psychological factors, such as financial anxiety, financial satisfaction (the knowledge that one has "enough"), obsession with money, personality type, self-esteem and sensation-seeking behavior, are all important contributors in determining an investor's FRT - the willingness to accept financial risk.},
  creationdate     = {2023-06-24T21:11:43},
  modificationdate = {2023-06-24T21:11:43},
}

@Article{Swedroe-2021a,
  author           = {Larry Swedroe},
  date             = {2021},
  journaltitle     = {Advisor Perspectives},
  title            = {Do Wide Divergences in ESG Ratings Doom Investors?},
  url              = {https://www.advisorperspectives.com/articles/2021/06/20/do-wide-divergences-in-esg-ratings-doom-investors},
  abstract         = {Six competing vendors rate how companies perform along environmental, social and governance (ESG) standards. But because those ratings differ widely across vendors, investors cannot reliably construct portfolios that meet their personal criteria.},
  creationdate     = {2023-06-24T21:11:43},
  modificationdate = {2023-06-24T21:11:43},
}

@Article{Swedroe-2021,
  author           = {Larry Swedroe},
  date             = {2021},
  journaltitle     = {Advisor Perspectives},
  title            = {ESG Investing Means Lower Bond Yields},
  url              = {https://www.advisorperspectives.com/articles/2021/04/19/esg-investing-means-lower-bond-yields},
  abstract         = {There has been an explosion in academic research on the impact of implementing environmental, social and governance (ESG) on the risk and returns of equity portfolios. Research on fixed income, which has received less attention, shows that positive ESG scores correlate with lower yield spreads, decreasing future returns for bond investors.},
  creationdate     = {2023-06-24T21:11:43},
  modificationdate = {2023-06-24T21:11:43},
}

@Article{Swedroe-Grogan-2009,
  author               = {Swedroe, Larry and Grogan, Kevin},
  date                 = {2009-11},
  journaltitle         = {The Journal of Investing},
  title                = {The Maturity of Fixed-Income Assets and Portfolio Risk},
  doi                  = {10.3905/joi.2009.18.4.107},
  issn                 = {1068-0896},
  number               = {4},
  pages                = {107--110},
  volume               = {18},
  abstract             = {This article focuses on the decision of the appropriate maturity of nominal return Treasury bonds. The question it seeks to answer is: Are investors compensated for taking on additional risk by extending the maturity of fixed-income assets? As we will demonstrate, the answer (at least in terms of investments in Treasury bonds) is that it depends on the investor's overall asset allocation and how far out the maturities are extended.},
  citeulike-article-id = {14322249},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/joi.2009.18.4.107},
  creationdate         = {2023-06-24T21:11:43},
  modificationdate     = {2023-06-24T21:11:43},
  posted-at            = {2017-03-29 06:31:45},
}

@Article{Tang-et-al-2021,
  author           = {Wenpin Tang and Xiao Xu and Xun Yu Zhou},
  date             = {2021-03-26},
  journaltitle     = {arXiv e-Print},
  title            = {Asset Selection via Correlation Blockmodel Clustering},
  eprinttype       = {arXiv},
  url              = {https://arxiv.org/abs/2103.14506},
  abstract         = {We aim to cluster financial assets in order to identify a small set of stocks to approximate the level of diversification of the whole universe of stocks. We develop a data-driven approach to clustering based on a correlation blockmodel in which assets in the same cluster have the same correlations with all other assets. We devise an algorithm to detect the clusters, with a theoretical analysis and a practical guidance. Finally, we conduct an empirical analysis to attest the performance of the algorithm.},
  creationdate     = {2023-06-24T21:12:24},
  file             = {:http\://arxiv.org/pdf/2103.14506v1:PDF},
  keywords         = {q-fin.PM, q-fin.CP, q-fin.ST},
  modificationdate = {2023-06-24T21:12:24},
}

@Article{Thiagarajan-et-al-2022a,
  author           = {Ramu Thiagarajan and Richard F. Lacaille and Simona Mocuta and Hanbin Im},
  date             = {2022-09},
  journaltitle     = {The Journal of Impact and {ESG} Investing},
  title            = {Macro Implications of Climate Risks},
  doi              = {10.3905/jesg.2022.1.060},
  number           = {2},
  pages            = {7-19},
  volume           = {3},
  abstract         = {Climate risks and rational policies to help manage the transition to net zero are among the most important topics today for central banks, sovereign governments, regulatory bodies, and the macroeconomy at large. There has been an active debate across the globe about the need, scope, scale, and depth of policies to facilitate a smooth journey to net zero by 2050. In this article, the authors examine the different channels by which climate risks can impact the economy and discuss the role of central banks in fighting climate change. Climate risks induce supply and demand shocks, which can have a material impact on inflation and growth. The authors concur with the concern of the European Central Bank that the transition could be inflationary in the near term. Although important, they believe that the role of central banks is likely to be somewhat limited in the battle against climate change; the pathways to effect change through central bank policy are indirect and dependent on transmission through other channels that weaken efficacy. Nevertheless, the authors conclude that incorporating appropriate climate risk tests into the regulatory framework fall within the central banks' mandate of ensuring financial stability.},
  creationdate     = {2023-06-24T21:13:23},
  modificationdate = {2023-06-24T21:13:23},
  publisher        = {Pageant Media {US}},
}

@Article{Thiagarajan-et-al-2022,
  author           = {Ramu Thiagarajan and Hanbin Im and Jiho Han and Aaron Hurd and Gaurav Mallik},
  date             = {2022-03},
  journaltitle     = {The Journal of Investing},
  title            = {Financial Globalization and Its Implications for Asset Allocation in a Dollar World},
  doi              = {10.3905/joi.2022.1.225},
  number           = {5},
  pages            = {7-27},
  volume           = {31},
  abstract         = {Unprecedented monetary policy measures carried out by the US Federal Reserve in response to the COVID-19 pandemic have raised the question of whether US dollar hegemony will continue going forward. In this article, the authors examine the multi-faceted nature of dollar hegemony with a focus on its relationship with globalization. They find that the US dollar is the dominant currency in global economic activities ranging from trade to cross-border banking, and this USD strength is reflected in the ability of US Treasuries to diversify local economic shocks. While they find that US Treasuries provide historically better diversification for local equity risk than local sovereign debt, in an ultra-low interest rate world, investors will likely need to consider alternative hedging approaches. As an illustration of a potential alternative, the authors turn to the currency markets and show that a short euro versus long Japanese yen position has had stable to improving performance as a hedging tool. The key message is that alternative hedging strategies merit a robust discussion and consideration in a low-yield environment.},
  creationdate     = {2023-06-24T21:13:23},
  modificationdate = {2023-06-24T21:13:23},
  publisher        = {Pageant Media {US}},
}

@Article{Thiagarajan-et-al-2021,
  author           = {Ramu Thiagarajan and Jiho Han and Aaron Hurd and Hanbin Im and Gaurav Mallik},
  date             = {2021-08},
  journaltitle     = {The Journal of Investing},
  title            = {Financial Globalization and Its Implications for Diversification of Portfolio Risk},
  doi              = {10.3905/joi.2021.1.197},
  number           = {6},
  pages            = {22--33},
  volume           = {30},
  abstract         = {Trade disputes and the impact of the COVID-19 pandemic on global supply chains have drawn much attention to the notion of "deglobalization." The common concern is that the steady trend of globalization and its many benefits may reverse. But the globalization trend is not a monolith. In this article, we show that although trade globalization has stalled since the Global Financial Crisis (GFC), financial globalization has continued to increase. We further show that financial globalization has a much more significant impact on portfolios than trade globalization. The primary mechanism of this impact, US dollar hegemony, impacts portfolios primarily through increased spillover of US monetary policy shocks. The two implications for investors are: (1) global equity markets have become increasingly correlated and are likely to stay that way, and (2) this increased correlation reduces the benefits of portfolio diversification and leads to a more concentrated exposure to US monetary policy shocks.},
  creationdate     = {2023-06-24T21:13:23},
  modificationdate = {2023-06-24T21:13:23},
  publisher        = {Pageant Media {US}},
}

@Article{Thiagarajan-Li-2010,
  author               = {Thiagarajan, Ramu and Li, Yanping},
  date                 = {2010-08},
  journaltitle         = {The Journal of Investing},
  title                = {What's IVol Got to Do With It?},
  doi                  = {10.3905/joi.2010.19.3.020},
  issn                 = {1068-0896},
  number               = {3},
  pages                = {20--32},
  volume               = {19},
  abstract             = {Campbell et al.'s [2001] influential paper on idiosyncratic volatility has received a lot of research attention in recent years. The critical question for portfolio managers is: How does idiosyncratic volatility affect stock selection and asset allocation signals?

This article examines that important question. Using well-accepted metrics of idiosyncratic volatility, the authors find that the effectiveness of both the momentum and the valuation signals critically depends on cross-sectional differences in idiosyncratic volatility. In particular, they find that the mean reversion signal works better for firms with low idiosyncratic volatility, whereas the momentum signal works better for those with high idiosyncratic volatility. This result is robust across different regimes of idiosyncratic volatility.

Finally, the authors test whether a measure of idiosyncratic volatility has an impact on the Fed Model for asset allocation. They find that conditioning the signal in the Fed Model for cross-sectional differences in idiosyncratic volatility significantly improves the mean reversion effect that underlies the Fed Model

. In summary, they note that it is important to condition stock selection and asset allocation models for cross-sectional differences in idiosyncratic volatility.},
  citeulike-article-id = {13970975},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/joi.2010.19.3.020},
  creationdate         = {2023-06-24T21:13:23},
  modificationdate     = {2023-06-24T21:13:23},
  owner                = {cristi},
  posted-at            = {2016-03-07 00:07:47},
}

@Article{Thiagarajan-et-al-2016,
  author               = {Thiagarajan, Ramu and Peebles, Douglas J. and Leki Dorji, Sonam and Han, Jiho and Wilson, Chris},
  date                 = {2016-02},
  journaltitle         = {The Journal of Investing},
  title                = {Factor Approach to Fixed Income Allocation},
  doi                  = {10.3905/joi.2016.25.1.074},
  issn                 = {1068-0896},
  number               = {1},
  pages                = {74--84},
  volume               = {25},
  abstract             = {This article outlines the application of a systematic factor approach to fixed income investment/risk management.

We show that using a parsimonious set of factors explains the returns in fixed income portfolios very well. In turn, this implies that forecasting the returns for these parsimonious factors is an efficient and targeted approach to active management.

We further show that it is possible to create a portfolio with balanced exposure to the identified risk factors that, in turn, provides a framework for evaluating the efficacy of active management.},
  citeulike-article-id = {13967781},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/joi.2016.25.1.074},
  creationdate         = {2023-06-24T21:13:24},
  modificationdate     = {2023-06-24T21:13:24},
  owner                = {cristi},
  posted-at            = {2016-03-05 11:35:37},
}

@Article{Thiagarajan-et-al-2015,
  author               = {Thiagarajan, S. Ramu and Alankar, Ashwin and Shaikhutdinov, Rustem},
  date                 = {2015-05},
  journaltitle         = {The Journal of Investing},
  title                = {Tail Risk: Challenges, Mitigation, and Research Opportunities},
  doi                  = {10.3905/joi.2015.24.2.113},
  issn                 = {1068-0896},
  number               = {2},
  pages                = {113--121},
  volume               = {24},
  abstract             = {This article discusses the multifaceted topic of tail risk. Topics include tail-risk perception, portfolio and enterprise management, and mitigation. We start by summarizing the key insights of the authors who have contributed to this special issue on tail risk and the applications of this work. We then propose a simple rule of thumb to evaluate tail-risk exposure and the viability of hedging strategies in the portfolio context. We add to the menu of these strategies by introducing the idea of whether protection on idiosyncratic risk is a cost-effective hedge against systemic risk. Empirical evidence and the dynamics that unfold during a systemic crisis appear to collectively indicate that the market may misprice the systemization of idiosyncratic risk. The evidence is compelling enough to warrant further research into the basis between idiosyncratic and systemic risk amid stress.},
  citeulike-article-id = {14322500},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/joi.2015.24.2.113},
  creationdate         = {2023-06-24T21:13:24},
  groups               = {Invest_TailRisk},
  modificationdate     = {2023-06-24T21:13:24},
  posted-at            = {2017-03-29 12:33:19},
}

@Article{Thiagarajan-et-al-2012a,
  author               = {Thiagarajan, S. Ramu and Han, Jiho and Okounkova, Inna},
  date                 = {2012},
  journaltitle         = {The Journal of Investing},
  title                = {Expanded Role of Strategic Asset Allocation: Value Added and Implications},
  doi                  = {10.3905/joi.2012.21.3.012},
  number               = {3},
  pages                = {12--23},
  url                  = {https://joi.pm-research.com/content/21/3/12},
  volume               = {21},
  abstract             = {The concept of strategic asset allocation (SAA) has significantly changed over the years. Portfolio managers, consultants, and strategists in charge of SAA use a wider variety of asset classes than just stocks, bonds, and cash (to manage style tilts and control currency exposures) and are dynamic in their choices.

This article expands the traditional SAA framework to accommodate seven different and popular sources of additional exposures. By expanding the concept of asset allocation from traditional assets to accommodate additional factors tilts using a Brinson-type analysis, the authors show that expanded SAA explains nearly 90 percent of the variation in active returns, absorbing some of the value added by active management. Analysis of moderate versus conservatively managed funds shows that the ability to add value from active management is partially dependent on the risk budget provided to the active manager.

Importantly, this article shows that the majority of excess return in active management comes from the risk premium due to exposure to additional risk factors that can frequently be absorbed within the expanded SAA framework. This implies that it is important to view strategic asset allocation under the expanded framework as it is more reflective of practice and allows better assessment of the role of active management.},
  citeulike-article-id = {13971093},
  creationdate         = {2023-06-24T21:13:24},
  groups               = {SAA},
  modificationdate     = {2023-06-24T21:13:24},
  owner                = {cristi},
  posted-at            = {2016-03-07 05:07:48},
}

@Article{Thiagarajan-Schachter-2011,
  author               = {Thiagarajan, S. Ramu and Schachter, Barry},
  date                 = {2011-02},
  journaltitle         = {The Journal of Investing},
  title                = {Risk Parity: Rewards, Risks, and Research Opportunities},
  doi                  = {10.3905/joi.2011.20.1.079},
  issn                 = {1068-0896},
  number               = {1},
  pages                = {79--89},
  volume               = {20},
  abstract             = {Mean variance optimization has recently come under great criticism based on the poor performance experienced by asset managers during the global financial crisis. In response, an alternative approach, called risk parity, which proceeds by equalizing risk contributions, has garnered much interest.

The authors summarize the work of a group of leading researchers on risk parity chosen for this special issue. They survey more generally what is known about this approach. Although risk parity has intuitive appeal and has performed well over some historical time periods, it is premature to claim the superiority of risk parity over other asset allocation approaches.

The authors raise several conceptual and practical questions about risk parity that they think are worthy of additional research.},
  citeulike-article-id = {13970979},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/joi.2011.20.1.079},
  creationdate         = {2023-06-24T21:13:24},
  groups               = {Risk_Budgeting},
  modificationdate     = {2023-06-24T21:13:24},
  owner                = {cristi},
  posted-at            = {2016-03-07 00:14:19},
}

@Article{Thiagarajan-et-al-2012,
  author               = {Thiagarajan, S. Ramu and Schachter, Barry and DePalma, Michael},
  date                 = {2012-05},
  journaltitle         = {The Journal of Investing},
  title                = {LDI: Complexities, Challenges, and Opportunities},
  doi                  = {10.3905/joi.2012.21.2.102},
  issn                 = {1068-0896},
  number               = {2},
  pages                = {102--104},
  volume               = {21},
  abstract             = {This special issue of The Journal of Investing presents a set of articles that addresses some facets of the complex issues around liability-driven investment (LDI). Given the complexity of LDI, the authors recognize that many issues have been left unaddressed in this special issue. However, the researchers in this special issue have considerable experience in managing fixed-income portfolios and in designing custom solutions for client needs in the LDI arena.

The articles address important issues, such as the role of risk factors in LDI solutions, distinguishing between liability-hedging and risk-seeking assets in designing LDI programs, the importance of diversification for LDI, and, importantly, the role of glide paths in dynamic de-risking.},
  citeulike-article-id = {13971090},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/joi.2012.21.2.102},
  creationdate         = {2023-06-24T21:13:24},
  groups               = {LDI},
  journal              = {The Journal of Investing},
  modificationdate     = {2023-06-24T21:13:24},
  owner                = {cristi},
  posted-at            = {2016-03-07 05:04:24},
}

@Article{Thiagarajan-2009,
  author           = {Thiagarajan, S. Ramu},
  date             = {2009-05-31},
  journaltitle     = {The Journal of Investing},
  title            = {Does the Opportunity Justify the Risk? Insights from Quantitative Research},
  doi              = {10.3905/JOI.2009.18.2.018},
  issn             = {1068-0896},
  number           = {2},
  pages            = {18--25},
  url              = {http://joi.iijournals.com/lookup/doi/10.3905/{JOI}.2009.18.2.018},
  urldate          = {2019-07-15},
  volume           = {18},
  creationdate     = {2023-06-24T21:13:24},
  day              = {31},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T21:13:24},
}

@Article{Thiagarajan-et-al-2019,
  author           = {Thiagarajan, Jayaraman J. and Venkatesh, Bindya and Sattigeri, Prasanna and Bremer, Peer-Timo},
  date             = {2019-09-09},
  journaltitle     = {arXiv e-Print},
  title            = {Building Calibrated Deep Models via Uncertainty Matching with Auxiliary Interval Predictors},
  url              = {https://arxiv.org/abs/1909.04079},
  urldate          = {2019-12-29},
  abstract         = {With rapid adoption of deep learning in critical applications, the question of when and how much to trust these models often arises, which drives the need to quantify the inherent uncertainties. While identifying all sources that account for the stochasticity of models is challenging, it is common to augment predictions with confidence intervals to convey the expected variations in a model's behavior. We require prediction intervals to be well-calibrated, reflect the true uncertainties, and to be sharp. However, existing techniques for obtaining prediction intervals are known to produce unsatisfactory results in at least one of these criteria. To address this challenge, we develop a novel approach for building calibrated estimators. More specifically, we use separate models for prediction and interval estimation, and pose a bi-level optimization problem that allows the former to leverage estimates from the latter through an uncertainty matching strategy. Using experiments in regression, time-series forecasting, and object localization, we show that our approach achieves significant improvements over existing uncertainty quantification methods, both in terms of model fidelity and calibration error.},
  creationdate     = {2023-06-24T21:13:25},
  day              = {9},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T21:13:25},
}

@Article{Thrun-Stier-2021,
  author           = {Michael C. Thrun and Quirin Stier},
  date             = {2021-01},
  journaltitle     = {SoftwareX},
  title            = {Fundamental clustering algorithms suite},
  doi              = {10.1016/j.softx.2020.100642},
  pages            = {100642},
  volume           = {13},
  abstract         = {The article presents immediate access to over fifty fundamental clustering algorithms. Additionally, access to clustering benchmark datasets published priorly as "Fundamental Clustering Problems Suite" (FCPS) is provided. The software library is named "FCPS", available in R on CRAN and accessible within Python. The input and output of clustering algorithms are standardized to enable users a swift execution of cluster analysis. By combining mirrored-density plots (MD plots) with statistical testing, FCPS provides a tool to investigate the cluster-tendency quickly before the cluster analysis itself. Common clustering challenges can be generated with an arbitrary sample size. Additionally, FCPS sums up 26 indicators intending to estimate the number of clusters and provides an appropriate implementation of the clustering accuracy for more than two clusters.},
  creationdate     = {2023-06-24T21:14:19},
  modificationdate = {2023-06-24T21:14:19},
  publisher        = {Elsevier {BV}},
}

@Article{Vankwikelberge-et-al-2021,
  author           = {Xander Vankwikelberge and Bo Kang and Edith Heiter and Jefrey Lijffijt},
  date             = {2021-11-04},
  journaltitle     = {arXiv e-Print},
  title            = {ExClus: Explainable Clustering on Low-dimensional Data Representations},
  eprint           = {2111.03168},
  eprintclass      = {cs.LG},
  eprinttype       = {arXiv},
  abstract         = {Dimensionality reduction and clustering techniques are frequently used to analyze complex data sets, but their results are often not easy to interpret. We consider how to support users in interpreting apparent cluster structure on scatter plots where the axes are not directly interpretable, such as when the data is projected onto a two-dimensional space using a dimensionality-reduction method. Specifically, we propose a new method to compute an interpretable clustering automatically, where the explanation is in the original high-dimensional space and the clustering is coherent in the low-dimensional projection. It provides a tunable balance between the complexity and the amount of information provided, through the use of information theory. We study the computational complexity of this problem and introduce restrictions on the search space of solutions to arrive at an efficient, tunable, greedy optimization algorithm. This algorithm is furthermore implemented in an interactive tool called ExClus. Experiments on several data sets highlight that ExClus can provide informative and easy-to-understand patterns, and they expose where the algorithm is efficient and where there is room for improvement considering tunability and scalability.},
  creationdate     = {2023-06-24T21:15:30},
  file             = {:http\://arxiv.org/pdf/2111.03168v1:PDF},
  keywords         = {cs.LG},
  modificationdate = {2023-06-24T21:15:30},
}

@Article{Vyrost-et-al-2019,
  author           = {Tomas Vyrost and Stefan Lyocsa and Eduard Baumohl},
  date             = {2019},
  journaltitle     = {The North American Journal of Economics and Finance},
  title            = {Network-based asset allocation strategies},
  doi              = {10.1016/j.najef.2018.06.008},
  pages            = {516--536},
  volume           = {47},
  abstract         = {In this study, we construct financial networks in which nodes are represented by assets and where edges are based on long-run correlations. We construct four networks (complete graph, a minimum spanning tree, a planar maximally filtered graph, and a threshold significance graph) and use three centrality measures (betweenness, eigenvalue centrality, and the expected force). To improve risk return characteristics of well-known return maximization and risk minimization benchmark portfolios, we propose simple adjustments to portfolio selection strategies that utilize centralization measures from financial networks. From a sample of 45 assets (stock market indices, bond and money market instruments, commodities, and foreign exchange rates) and from data for 1999 to 2015, we show that irrespective of the network and centrality employed, the proposed network-based asset allocation strategies improve key portfolio return characteristics in an out of sample framework, most notably, risk and left tail risk adjusted returns. Resolving portfolio model selection uncertainties further improves risk return characteristics. Improvements made to portfolio strategies based on risk minimization are also robust to transaction costs.},
  creationdate     = {2023-06-24T21:16:49},
  journal          = {The North American Journal of Economics and Finance},
  modificationdate = {2023-06-24T21:16:49},
  publisher        = {Elsevier {BV}},
}

@Article{Wang-Aste-2023,
  author           = {Yuanrong Wang and Tomaso Aste},
  date             = {2023-03},
  journaltitle     = {Expert Systems with Applications},
  title            = {Dynamic portfolio optimization with inverse covariance clustering},
  doi              = {10.1016/j.eswa.2022.118739},
  pages            = {118739},
  volume           = {213},
  abstract         = {Market conditions change continuously. However, in portfolio investment strategies, it is hard to account for this intrinsic non-stationarity. In this paper, we propose to address this issue by using the Inverse Covariance Clustering (ICC) method to identify inherent market states and then integrate such states into a dynamic portfolio optimization process. Extensive experiments across three different markets, NASDAQ, FTSE and HS300, over a period of ten years, demonstrate the advantages of our proposed algorithm, termed Inverse Covariance Clustering-Portfolio Optimization (ICC-PO). The core of the ICC-PO methodology concerns the identification and clustering of market states from the analytics of past data and the forecasting of the future market state. It is therefore agnostic to the specific portfolio optimization method of choice. By applying the same portfolio optimization technique on a ICC temporal cluster, instead of the whole train period, we show that one can generate portfolios with substantially higher Sharpe Ratios, which are statistically more robust and resilient with great reductions in the maximum loss in extreme situations. This is shown to be consistent across markets, periods, optimization methods and selection of portfolio assets.},
  creationdate     = {2023-06-24T21:17:39},
  modificationdate = {2023-06-24T21:17:39},
  publisher        = {Elsevier {BV}},
}

@Article{Wang-Aste-2022a,
  author           = {Yuanrong Wang and Tomaso Aste},
  date             = {2022},
  journaltitle     = {arXiv e-Print},
  title            = {Dynamic Portfolio Optimization with Inverse Covariance Clustering},
  eprint           = {2112.15499},
  eprintclass      = {q-fin.ST},
  eprinttype       = {arXiv},
  abstract         = {Market conditions change continuously. However, in portfolio's investment strategies, it is hard to account for this intrinsic non-stationarity. In this paper, we propose to address this issue by using the Inverse Covariance Clustering (ICC) method to identify inherent market states and then integrate such states into a dynamic portfolio optimization process. Extensive experiments across three different markets, NASDAQ, FTSE and HS300, over a period of ten years, demonstrate the advantages of our proposed algorithm, termed Inverse Covariance Clustering-Portfolio Optimization (ICC-PO). The core of the ICC-PO methodology concerns the identification and clustering of market states from the analytics of past data and the forecasting of the future market state. It is therefore agnostic to the specific portfolio optimization method of choice. By applying the same portfolio optimization technique on a ICC temporal cluster, instead of the whole train period, we show that one can generate portfolios with substantially higher Sharpe Ratios, which are statistically more robust and resilient with great reductions in maximum loss in extreme situations. This is shown to be consistent across markets, periods, optimization methods and selection of portfolio assets.},
  creationdate     = {2023-06-24T21:17:39},
  file             = {:http\://arxiv.org/pdf/2112.15499v1:PDF},
  keywords         = {q-fin.ST, q-fin.PM},
  modificationdate = {2023-06-24T21:17:39},
}

@Article{Wang-Aste-2022,
  author           = {Yuanrong Wang and Tomaso Aste},
  date             = {2022-03-08},
  journaltitle     = {arXiv e-Print},
  title            = {Sparsification and Filtering for Spatial-temporal GNN in Multivariate Time-series},
  eprint           = {2203.03991},
  eprintclass      = {cs.LG},
  eprinttype       = {arXiv},
  abstract         = {We propose an end-to-end architecture for multivariate time-series prediction that integrates a spatial-temporal graph neural network with a matrix filtering module. This module generates filtered (inverse) correlation graphs from multivariate time series before inputting them into a GNN. In contrast with existing sparsification methods adopted in graph neural network, our model explicitly leverage time-series filtering to overcome the low signal-to-noise ratio typical of complex systems data. We present a set of experiments, where we predict future sales from a synthetic time-series sales dataset. The proposed spatial-temporal graph neural network displays superior performances with respect to baseline approaches, with no graphical information, and with fully connected, disconnected graphs and unfiltered graphs.},
  creationdate     = {2023-06-24T21:17:39},
  file             = {:http\://arxiv.org/pdf/2203.03991v1:PDF},
  keywords         = {cs.LG, q-fin.CP},
  modificationdate = {2023-06-24T21:17:39},
}

@Article{Yang-et-al-2019f,
  author           = {Li Yang and Longfeng Zhao and Chao Wang},
  date             = {2019-10},
  journaltitle     = {Physica A: Statistical Mechanics and its Applications},
  title            = {Portfolio optimization based on empirical mode decomposition},
  doi              = {10.1016/j.physa.2019.121813},
  pages            = {121813},
  volume           = {531},
  abstract         = {The investigation about the cross-correlation among financial assets has drawn broad attention recently. Due to the nonlinear and non-stationary identities of the financial time series, e.g., stock return time series, the cross-correlation for different level of fluctuations are quite important for both academia and financial practitioners. Here we use the empirical mode decomposition (EMD) method to analyze the cross-correlation structure among different level of fluctuations for financial assets. The correlation-based networks are then employed to determine the clustering property of stock market. We then propose several portfolio optimization strategies based on the EMD correlation-based networks. Using the topological information of the networks, we can construct some portfolios with high return and low risk. Under two portfolio evaluation frameworks, we prove that these portfolios have consistently good performance.},
  creationdate     = {2023-06-24T21:18:27},
  modificationdate = {2023-06-24T21:18:27},
  publisher        = {Elsevier {BV}},
}

@Article{Zaimovic-et-al-2021,
  author           = {Azra Zaimovic and Adna Omanovic and Almira Arnaut-Berilo},
  date             = {2021-11},
  journaltitle     = {Journal of Risk and Financial Management},
  title            = {How Many Stocks Are Sufficient for Equity Portfolio Diversification? A Review of the Literature},
  doi              = {10.3390/jrfm14110551},
  number           = {11},
  pages            = {551},
  volume           = {14},
  abstract         = {Using extensive and comprehensive databases to select a subset of research papers, we aim to critically analyze previous empirical studies to identify certain patterns in determining the optimal number of stocks in well-diversified portfolios in different markets, and to compare how the optimal number of stocks has changed over different periods and how it has been affected by market turmoil such as the Global Financial Crisis (GFC) and the current COVID-19 pandemic. The main methods used are bibliometric analysis and systematic literature review. Evaluating the number of assets which lead to optimal diversification is not an easy task as it is impacted by a huge number of different factors: the way systematic risk is measured, the investment universe (size, asset classes and features of the asset classes), the investor's characteristics, the change over time of the asset features, the model adopted to measure diversification (i.e., equally weighted versus optimal allocation), the frequency of the data that is being used, together with the time horizon, conditions in the market that the study refers to, etc. Our paper provides additional support for the fact that (1) a generalized optimal number of stocks that constitute a well-diversified portfolio does not exist for whichever market, period or investor. Recent studies further suggest that (2) the size of a well-diversified portfolio is larger today than in the past, (3) this number is lower in emerging markets compared to developed financial markets, (4) the higher the stock correlations with the market, the lower the number of stocks required for a well-diversified portfolio for individual investors, and (5) machine learning methods could potentially improve the investment decision process. Our results could be helpful to private and institutional investors in constructing and managing their portfolios and provide a framework for future research.},
  creationdate     = {2023-06-24T21:19:39},
  modificationdate = {2023-06-24T21:19:39},
  publisher        = {{MDPI} {AG}},
}

@Article{Zhan-et-al-2021,
  author           = {Ni Zhan and Yijia Sun and Aman Jakhar and He Liu},
  date             = {2021-01-22},
  journaltitle     = {arXiv e-Print},
  title            = {Graphical Models for Financial Time Series and Portfolio Selection},
  eprinttype       = {arXiv},
  url              = {https://arxiv.org/abs/2101.09214},
  abstract         = {We examine a variety of graphical models to construct optimal portfolios. Graphical   models such as PCA-KMeans, autoencoders, dynamic clustering, and structural learning can capture the time varying patterns in the covariance matrix and allow the creation of an optimal and robust portfolio. We compared the resulting portfolios from the different models with baseline methods. In many cases our graphical strategies generated steadily increasing returns with low risk and outgrew the S\&P 500 index. This work suggests that graphical models can effectively learn the temporal dependencies in time series data and are proved useful in asset management.},
  creationdate     = {2023-06-24T21:20:21},
  file             = {:http\://arxiv.org/pdf/2101.09214v1:PDF},
  keywords         = {cs.LG, q-fin.CP},
  modificationdate = {2023-06-24T21:20:21},
}

@Article{Zhao-et-al-2018c,
  author               = {Zhao, Longfeng and Wang, Gang-Jin and Wang, Mingang and Bao, Weiqi and Li, Wei and Stanley, H. Eugene},
  date                 = {2018},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Stock market as temporal network},
  doi                  = {10.1016/j.physa.2018.05.039},
  pages                = {1104-1112},
  volume               = {506},
  abstract             = {Financial networks have become extremely useful in characterizing the structure of complex financial systems. Meanwhile, the time evolution property of the stock markets can be described by temporal networks. We utilize the temporal network framework to characterize the time-evolving correlation-based networks of stock markets. The market instability can be detected by the evolution of the topology structure of the financial networks. We employ the temporal centrality as a portfolio selection tool. Those portfolios, which are composed of peripheral stocks with low temporal centrality scores, have consistently better performance under different portfolio optimization schemes, suggesting that the temporal centrality measure can be used as new portfolio optimization and risk management tools. Our results reveal the importance of the temporal attributes of the stock markets, which should be taken serious consideration in real life applications.},
  citeulike-article-id = {14510852},
  citeulike-linkout-0  = {http://arxiv.org/abs/1712.04863},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1712.04863},
  creationdate         = {2023-06-24T21:21:15},
  day                  = {13},
  groups               = {PortfOptim_Network},
  modificationdate     = {2023-06-24T21:21:15},
  posted-at            = {2018-01-02 01:42:36},
}

@Article{Zhao-et-al-2021b,
  author           = {Zhihua Zhao and Fengmin Xu and Donglei Du and Wang Meihua},
  date             = {2021-03},
  journaltitle     = {Quantitative Finance},
  title            = {Robust portfolio rebalancing with cardinality and diversification constraints},
  doi              = {10.1080/14697688.2021.1879392},
  number           = {10},
  pages            = {1707-1721},
  volume           = {21},
  abstract         = {In this paper, we develop a robust conditional value at risk (CVaR) optimal portfolio rebalancing model under various financial constraints to construct sparse and diversified rebalancing portfolios. Our model includes transaction costs and double cardinality constraints in order to capture the trade-off between the limit of investment scale and the diversified industry coverage requirement. We first derive a closed-form solution for the robust CVaR portfolio rebalancing model with only transaction costs. This allows us to conduct an industry risk analysis for sparse portfolio rebalancing in the absence of diversification constraints. Then, we attempt to remedy the hidden industry risk by establishing a new robust portfolio rebalancing model with both sparse and diversified constraints. This is followed by the development of a distributed-version of the Alternating Direction Method of Multipliers (ADMM) algorithm, where each subproblem admits a closed-form solution. Finally, we conduct empirical tests to compare our proposed strategy with the standard sparse rebalancing and no-rebalancing strategies. The computational results demonstrate that our rebalancing approach produces sparse and diversified portfolios with better industry coverage. Additionally, to measure out-of-sample performance, two superiority indices are created based on worst-case CVaR and annualized return, respectively. Our ADMM strategy outperforms the sparse rebalancing and no-rebalancing strategies in terms of these indices.},
  creationdate     = {2023-06-24T21:21:56},
  modificationdate = {2023-06-24T21:21:56},
  publisher        = {Informa {UK} Limited},
}

@Article{Yin-et-al-2021a,
  author           = {Yin, Chenyang and Perchet, Romain and Soupe, Francois},
  date             = {2021},
  journaltitle     = {Quantitative Finance},
  title            = {A practical guide to robust portfolio optimization},
  doi              = {10.1080/14697688.2020.1849780},
  issn             = {1556-5068},
  number           = {6},
  pages            = {911-928},
  urldate          = {2021},
  volume           = {21},
  abstract         = {Robust optimization takes into account the uncertainty in expected returns to address the shortcomings of portfolio mean-variance optimization, namely the sensitivity of the optimal portfolio to inputs. We investigate the mechanisms by which robust optimization achieves its goal and give practical guidance when it comes to the choice of uncertainty in form and level. We explain why the quadratic uncertainty set should be preferred to box uncertainty based on the literature review, we show that a diagonal uncertainty matrix with only variances should be used, and that the level of uncertainty can be chosen as a function of the asset Sharpe ratios. Finally, we use practical examples to show that, with the proposed parametrization, robust optimization does overcome the weaknesses of mean-variance optimization and can be applied in real investment problems such as the management of multi-asset portfolios or in robo-advising.},
  creationdate     = {2023-06-24T21:26:56},
  f1000-projects   = {QuantInvest},
  modificationdate = {2023-06-24T21:26:56},
}

@Article{Yam-et-al-2016,
  author           = {Sheung Chi Phillip Yam and Hailiang Yang and Fei Lung Yuen},
  date             = {2016-06},
  journaltitle     = {European Journal of Operational Research},
  title            = {Optimal asset allocation: Risk and information uncertainty},
  doi              = {10.1016/j.ejor.2015.11.011},
  number           = {2},
  pages            = {554--561},
  volume           = {251},
  abstract         = {In asset allocation problem, the distribution of the assets is usually assumed to be known in order to identify the optimal portfolio. In practice, we need to estimate their distribution. The estimations are not necessarily accurate and it is known as the uncertainty problem. Many researches show that most people are uncertainty aversion and this affects their investment strategy. In this article, we consider risk and information uncertainty under a common asset allocation framework. The effects of risk premium and covariance uncertainty are demonstrated by the worst scenario in a set of measures generated by a relative entropy constraint. The nature of the uncertainty and its impacts on the asset allocation are discussed.},
  creationdate     = {2023-06-24T21:27:40},
  modificationdate = {2023-06-24T21:27:40},
  publisher        = {Elsevier {BV}},
}

@Article{Chopra-Ziemba-1993,
  author           = {Vijay K. Chopra and William T. Ziemba},
  date             = {1993},
  journaltitle     = {The Journal of Portfolio Management},
  title            = {The Effect of Errors in Means, Variances, and Covariances on Optimal Portfolio Choice},
  doi              = {10.3905/jpm.1993.409440},
  number           = {22},
  pages            = {6-11},
  url              = {https://jpm.pm-research.com/content/19/2/6},
  volume           = {19},
  abstract         = {For investors with moderate to high risk tolerance, the cash equivalent loss for errors in means is an order of magnitude greater than that for errors in variances or covariances.},
  creationdate     = {2023-06-24T21:28:25},
  groups           = {Mean_Variance},
  modificationdate = {2023-06-24T21:28:25},
  owner            = {zkgst0c},
  timestamp        = {2019-09-04 01:44},
}

@Article{Jagannathan-Ma-2003,
  author           = {Jagannathan, R. and Ma, T.},
  date             = {2003},
  journaltitle     = {Journal of Finance},
  title            = {Risk Reduction in Large Portfolios: Why Imposing the Wrong Constraints Helps},
  doi              = {10.1111/1540-6261.00580},
  pages            = {1561--1583},
  volume           = {58},
  abstract         = {Green and Hollifield (1992) argue that the presence of a dominant factor would result in extreme negative weights in mean-variance efficient portfolios even in the absence of estimation errors. In that case, imposing no-short-sale constraints should hurt, whereas empirical evidence is often to the contrary.

We reconcile this apparent contradiction. We explain why constraining portfolio weights to be nonnegative can reduce the risk in estimated optimal portfolios even when the constraints are wrong. Surprisingly, with no-short-sale constraints in place, the sample covariance matrix performs as well as covariance matrix estimates based on factor models, shrinkage estimators, and daily data.},
  creationdate     = {2023-06-24T21:29:34},
  groups           = {Mean_Variance},
  modificationdate = {2023-06-24T21:29:34},
  owner            = {zkgst0c},
  timestamp        = {2019-09-04 18:09},
}

@Article{DeMiguel-et-al-2009,
  author           = {DeMiguel, V. and Garlappi, L. and Nogales, F. J. and Uppal, R.},
  date             = {2009},
  journaltitle     = {Management Science},
  title            = {A generalized approach to portfolio optimization: Improving performance by constraining portfolio norms},
  doi              = {10.1287/mnsc.1080.0986},
  number           = {5},
  pages            = {798--812},
  volume           = {55},
  abstract         = {We provide a general framework for finding portfolios that perform well out-of-sample in the presence of estimation error. This framework relies on solving the traditional minimum-variance problem but subject to the additional constraint that the norm of the portfolio-weight vector be smaller than a given threshold. We show that our framework nests as special cases the shrinkage approaches of Jagannathan and Ma (Jagannathan, R., T. Ma. 2003. Risk reduction in large portfolios: Why imposing the wrong constraints helps. J. Finance 58 165-1684) and Ledoit and Wolf (Ledoit, O., M. Wolf. 2003.

Improved estimation of the covariance matrix of stock returns with an application to portfolio selection. J. Empirical Finance 10 603-621, and Ledoit, O., M. Wolf. 2004. A well-conditioned estimator for large-dimensional covariance matrices. J. Multivariate Anal. 88 365-411) and the 1/N portfolio studied in DeMiguel et al. (DeMiguel, V., L. Garlappi, R. Uppal. 2009. Optimal versus naive diversification: How inefficient is the 1/N portfolio strategy? Rev. Financial Stud. 22 191-1953).

We also use our framework to propose several new portfolio strategies. For the proposed portfolios, we provide a moment-shrinkage interpretation and a Bayesian interpretation where the investor has a prior belief on portfolio weights rather than on moments of asset returns. Finally, we compare empirically the out-of-sample performance of the new portfolios we propose to 10 strategies in the literature across five data sets. We find that the norm-constrained portfolios often have a higher Sharpe ratio than the portfolio strategies in Jagannathan and Ma (2003), Ledoit and Wolf (2003, 2004), the 1/N portfolio, and other strategies in the literature, such as factor portfolios.},
  creationdate     = {2023-06-24T21:30:34},
  modificationdate = {2023-06-24T21:30:34},
  owner            = {zkgst0c},
  timestamp        = {2019-09-04 02:49},
}

@Article{Jaeger-et-al-2020,
  author           = {Jaeger, Markus and Krugel, Stephan and Marinelli, Dimitri and Papenbrock, Jochen and Schwendner, Peter},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Understanding machine learning for diversified portfolio construction by explainable AI},
  doi              = {10.2139/ssrn.3528616},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3528616},
  urldate          = {2020-03-06},
  abstract         = {In this paper, we construct a pipeline to investigate heuristic diversification strategies in asset allocation. We use machine learning concepts ("explainable AI") to compare the robustness of different strategies and back out implicit rules for decision making.In a first step, we augment the asset universe (the empirical dataset) with a range of scenarios generated with a block bootstrap from the empirical dataset.Second, we backtest the candidate strategies over a long period of time, checking their performance variability. Third, we use XGBoost as a regression model to connect the difference between the measured performances between two strategies to a pool of statistical features of the portfolio universe tailored to the investigated strategy. Finally, we employ the concept of Shapley values to extract the relationships that the model could identify between the portfolio characteristics and the statistical properties of the asset universe. We test this pipeline for studying risk-parity strategies with a volatility target, and in particular, comparing the machine learning-driven Hierarchical Risk Parity (HRP) to the classical Equal Risk Contribution (ERC) strategy.In the augmented dataset built from a multi-asset investment universe of commodities, equities and fixed income futures, we find that HRP better matches the volatility target, and shows better risk-adjusted performances. Finally, we train XGBoost to learn the difference between the realized Calmar ratios of HRP and ERC and extract explanations.The explanations provide fruitful ex-post indications of the connection between the statistical properties of the universe and the strategy performance in the training set. For example, the model confirms that features addressing the hierarchical properties of the universe are connected to the relative performance of HRP respect to ERC.},
  creationdate     = {2023-06-24T21:31:39},
  f1000-projects   = {QuantInvest},
  groups           = {Scenario_ML},
  modificationdate = {2023-06-24T21:31:39},
  timestamp        = {2020-07-23 13:37},
}

@Comment{jabref-meta: databaseType:biblatex;}
